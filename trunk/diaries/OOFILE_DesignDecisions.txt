OOFILE Design Decisions

see also OOFILE GUI Design Decisions & OOFRep Design Decisions

ABOUT THIS DOCUMENT: My somewhat ad-hoc development method revolves
around 4 documents, plus inumerable scratchings and product-specific
docs.

Design Decisions - formal evaluation of issues, alternatives and why
chosen

Code Changes - as you've seen serialised in changes.txt

Project Diary - client meeting decisions, major events, software
reconfigs...

Thoughts - free-form jottings as they occurred, often cleaned up and
put in Design Decisions. I won't release this - it's too large and
likely to get me in trouble with potentially libellous comments!


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/09/18 Revision of Pattern "Associating fields with tables by
declaration."

Initial design invoked a nameless manager object to handle the
redirection. It is now clear that the major issue is the specific
backend implementation needing to vary what happens at table and field
creation. Therefore creation ("definition" in the c++ sense) of these
objects can have a requirement of order - require a specific connection
to be created and thus be current before creation of the tables. This
lets us hide a lot of logic in dbConnect->AttachTable and
dbTable->AttachField.

I can't think of any other use for the general manager class so I'll
ditch it for now.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/09/18 Renaming OOFILE_LayerBits. 
I want to refer to this class to qualify some enums. As the parent class 
of the public "dbXXX" classes it seemed more sensible to rename it to
dbSomething so I've picked dbClass as a nice root name. Note: as well
as encapsulating some enums this root class is used in the
OOFILE_Dictionary.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/09/25

CLARIFICATION OF RESPONSIBILITIES BETWEEN BACKEND AND FRONTEND DB
CLASSES: 
- although we are not doing so at present, the framework
should not prevent the backend from being changed dynamically. This
means that a dbTable and its parts should contain enough information to
rebuild the schema.

- this implies a field dictionary owned by the dbTable object. However,
in many cases, the backend implementations will need a reference to
this dictionary for their own purposes (eg: building schemas, or
iterating through fields when streaming an entire record to cout).

- there is no assumption on the part of the dbTable that the field
dictionary is mimicked by the backend. The field dictionary is passed
down to the constructor but can be ignored.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/09/25 CONVENTIONS 
Abandoned the convention of public members starting with an uppercase
letter as this violates exepcted iterator syntax such as next, more.

However, retained the "1st letter upper" for user dbTable fields eg
dbChar Lastname.

DICTIONARY OWNERSHIP 
Split the dictionary to a proper reference-counted Handle/Body pair. Can
now pass an actual dictionary object to the backend and copy to any
number of dbTables with references properly updated. (See Coplien)

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/09/26 INDEXING 
Realised that indexing should be clearly a table responsibility  the
decision is simple if you consider compound indexes specified for
performance, and virtual fields (which may or may not be indexed).

OOFILE_Dictionary Opened this out to public use as it is in use by a
lot of classes.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/09/30 RELATIONS 
Left a choice of syntax for the beta testers to comment on but will go
with a syntax for now that makes it a dbConnect responsibility. This
maps onto the ODMG model of providing navigation references from both
ends (members of the dbTables) but modelling relationships as a
separate object. It will also be easier to implement  my other syntax
would have the same implementation but a couple of other methods in
front to forward the info.

theDB.RelateOwnerPart(People.Visits, PatientVisits.Patient,
Visits.PatientNumber);


Realised during this that the easiest way to map onto existing
databases is to have the OID field an explicit field of the dbTable.
This will also help greatly when importing. Thus, need to call
OIDfieldIs in the dbTable constructor.


INDEXING 
Further thoughts when tried implementing. Either I have  a
separate structure in which index info is stored or the index options
are part of the dbField. Generating the index is still a table
responsibility, but putting the info in the field gives me the chance
to simplify declarations. I can now pass indexing options in as another
field constructor parameter.

The mIndexes dictionary can be removed and a runtime check of field
attributes used instead.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/10/05 NUMERIC FIELDS 
Removed the abstract class dbNumericField. I
can't see a situation where it is really needed. At this time there
would seem to be few cases when the numeric classes are used regardless
of type. The numeric operators are overloaded on a per-class basis and
there is no need for an ABC to use these operators.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/10/09 SUPERFILES AND MULTIPLE OPENS 
From a modelling point of view,
it is a lot easier for one dbConnect to map to each "document" opened.
We will thus leave the door open for a later version of the ctree
backend which uses multiple physical files as a logical database.

Thus, one immediate benefit is that the dbConnect_ctree now has a
lifetime the same as a ctree instance. We don't have to worry about
trivia such as maintaining a static count of ctree file numbers used as
the superfile will always use the same numbers (for the current ctree
instance) and the other files will be dynamically allocated.

The constructor and destructor for the dbConnect_ctree can also now
init and cleanup ctree.

STYLISTIC CHANGES 
After part-reading "The Design & Evolution of C++"
decided to use private: instead of protected: unless there is a clear
reason why subclasses would - exist - need access to the data member.

Also,while reformatting classes, decided to isolate all data members
complete with a comment line.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/10/24 USING MACROS 
After long consideration and discussion with Ken
(See Searching Realities below) I realised that I have a need for a
number of minor inline methods per user dbTable subclass, purely to be
able to return a dbUserTable. Accordingly, I will define the macro
CLASS_TABLE which starts the dbuserTable subclass definition with these
fixed classes.

Alternatives considered - move away from value semantics (ie:
dbPeople.Name) to using the heap and thus be able to do some trickery
with downcasting. This is uglier in many ways and use of downcasting on
a regular basis could lead to easily corrupted systems.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/10/25 SEARCHING REALITIES 
The reality of constructors and typing is
such that the original dbPeople sp = People.Name=="Dent"; is very
difficult as it boils down to a dbChar.operator==(char *) passing its
return type to a dbPeople constructor. The core of the problem is that
there is no way at compile time to determine that the dbChar Name
belongs to a parent table dbPeople, as opposed to any other dbTable
subclass.

There is one slightly shonky way as shown below:

dbPeople(dbQueryClause& rhs) { *this = *( (dbPeople
*)rhs.AppliesToTable() ); };

This is extremely dangerous as it involves a cast operation which may
be incorrectly applied to different dbTable.  I have had some ideas
about a safer version of this involving runtime checking to make sure
that the query is against the correct table class, but this is getting
very complex and can wait until later.

So, the syntax for now is that dbPeople sp =
People[People.Name=="Dent"]; is equivalent to dbPeople sp = 
People.Search(People.Name=="Dent");

SUBSEARCH 
Original beta ideas had it that if I took a sub-collection
such as the above sp and applied a second search, that would be
cumulative. I've now decided this is too dangerous (although the
question will be asked in beta3) so a subsearch is now explicit.
sp.SearchSelection(People.Street=="Bermuda Dve");

TRANSIENCE OF SEARCHES 
Searches, by operator[], Search() or SearchSelection() are all transient
- they apply only to the next operation on the dbTable. This is so they
can be used inline in the constructors seen so far, and in expressions
like: cout << "No of Smiths: " <<
People[People.LastName=="Smith"].count();

If you want to make a search affect the permanent collection of records
in a dbTable, you use the CommitSearch() method, eg:
People[People.Name=="Dent"].CommitSearch(); or as two separate lines
People.SearchSelection(People.Age>=50); People.CommitSearch();

While this may appear awkward, there seems little alternative. I did
consider having the operator[] as inline only, and the Search verb as a
committing version, but that mean't I didn't have an inline equivalent
to SearchSelection. I could see this being a real problem: dbPeople sp
= People.SearchSelection(People.Age>=50);  // affects People as well

The other alternative I considered was having EVERY search expression
affect the dbTable to which it was applied, but allow explicit Push and
Pop Selection operations. This seemed unwieldy and prone to error.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/10/25 CLARIFYING USE OF OPERATOR()

Betas 1 & 2 have shown operator() used for field access: People.Name()
= "Dent";

This was partly so I could have "cute" syntax for mass updates, eg:
People.Salary *= 1.05;  // give all people a 5% pay rise

I decided there was too much potential for programmer mistakes leading
to mass updates. Accordingly, these two examples become People.Name =
"Dent"; People.Salary.all() *= 1.05;

Note: this is implemented by the all() operator returning a copy of the
field with a flag set to update all records.

The benefit of using direct member access, instead of the function call
operator, is that it lets me define directly operators and friends for
assigning to and from a dbField. The other approach would have required
a special reference type be returned, which handled the differences
between read and write (see D&E of C++ p87-88).

Note there has been some feedback (28/10) suggesting that
People.all().Name may be a better syntax. This made me realise that
explicitly writing People.Name .all()*= 1.05;  is really what I mean.
It may be possible to do something grotty like People.Name  .all*=
1.05; with a "fake" member on the dbField base type but this feels very
silly and may not be feasible.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/10/28 STYLE CLEANUP 
Decided to go through and mark all parameters
const, except where they are actually changed. Also marked all member
functions const where appropriate and will stick to this in future.

Removed parameter names from header files except where necessary to
document meaning of parameter.

Changed from unsigned char to char throughout.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/10/28 dbRelation Removed totally as I won't be using any explicit
class like this. If I do have a class it will be an internal class.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/10/28 FIELD MAPPING TO RECORDS 
Decided to adopt a flexible scheme and worry about efficiency later
(radical!). Each field access asks the table backend for an address
from which to read and a length, or asks for an address to write a
given length. (Note: may vary later for BLOB and Text fields - some
form of passing reference rather than rewriting?? Consider that c-tree
probably requires copying into a linear buffer *anyway* and so may
others.). This lets me implement very simple logic on both ends but
cuts the dependency. Remember that the fields are generic and we have
not got a backend-specialized version of each field. HOWEVER, in the
interests of efficiency, each field DOES have a copy of the mBackend
pointer! (After debating this point, found I'd already assigned in
dbTable::completeField() - I think I've been here before!).

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/10/28 dbChar mLength rename 
Renamed mLength to mMaxLength to reflect intention. Will also need to
consider if need two reflective operators. 4D Programmers often ask for
a field width as well as current string width.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/10/30 OOFILE_String Operator char * 
Decided, as OOFILE_String is a limited use class, that this operator
should return a pointer to the internal body. This saves us having to
garbage collect later. In particular, this conversion will be called
when passing OOFILE_String as a (char *) parameter, so there will be no
chance to hang onto the new string and collect it later.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/10/31 ASSIGNING C-TREE FILE NO'S 
The auto-assign technique based on using negative file no's in the IFIL
doesn't seem to work. Will use a static variable in the table backend
to communicate between the table and the connect - who have a public
interface with each other through abstract base classes. This is a
little grotty, but is helped by having a static function called to
reset the variable - it is still reasonably clean.
OOFILE_tableBackend_ctree::mNextFileNo

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/10/31 MAPPING FIELD INDEX NUMBERS 
There is a need for a fairly rapid lookup of index numbers for fields,
when a search or sort is performed by the field. In all cases, an
operation is performed by passing a dbField& to the tableBackend.

I can see two alternatives here: 1) a pointer is put in dbField to
point to specific information for the current backend. The backend
casts this pointer and uses it to call an object it knows.

2) extend the current offset map array, that is specific to ctree
backends, to include the index numbers, and later any other
information. The tableBackend is responsible for the maintenance of
this information.

I favour the 2nd approach - it keeps backend-specific information out
of the fields, and gives faster access. At the least, I would not want
to see field offset info gated through the several layers of a
subsidiary object attached to a field - this is a reasonably
high-impact area.

Therefore, there will be an index number added to the current offset
map, with the number set to 0 if no index. NOTE this scheme may require
change if I ever have multiple indexes per field. However, I think that
is more likely to be implemented with some other interface, probably a
derived field and hence visibly attached to such.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/10/31 REMOVING THE IDEA OF NAMES FROM THE DICTIONARY.

I came to the realisation that the current idea (never imp.) of storing
names in the dictionary has a few problems. Mainly, it assumes that all
context which would want a name are accessing the dictionary.

It has since become clear that there are times when you want to pass a
Field into a context where its name may be available.

Thus, I removed the Associations idea from OOFILE_DictRep and added a
field member to contain the name. This was even easier to implement
than expected as the field name can just be passed in through the
constructor, when called from the user table constructor.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/11/03 BLOB & TEXT FIELDS IN CTREE 
The main difference between dbText and dbChar in general is the fixed
length of the latter, which may or may not be implemented in the
database.

Forces: 
- varying length of field 
- possible need to manage multiple blob/text fields/file 
- if >1 blob fields in the same record, need to manage offsets with some
form of blob map (ie: can't just assume the entire variable area is the
blob)
- if separate blob file per master file, could double file numbers 
- if combined blob file, risk of cross-corruption

Context: 
- inside c-tree database, where having variable recs changes
the calls you use to read and write records 
- c-tree provides a number of calls for variable length records to get
the record length etc, but these are useless for multiple field recs

Resolution: 
- implement dbText as subclass of dbBlob 
- use a separate c-tree file to manage ALL blobs, so all other files are fixed 
- use the "record number" (offset) as a link to the blob 
- implies having to update this after writing blobs in case they move. 
- separate implementation in this manner maps well to other databases
which force the separation, so it will be easier to copy this
implementation
- implementation will keep a length field in the parent record, to
simplify some loading logic. (Will speedup reads if store length,
assuming more reads than writes
- single vs many blob files: in superfile the issues of cross-corruption
are there anyway and there seems little point in having multiple files
- blob rewriting can be optimised by keeping a dirty flag independent of
the main record dirty flag.

Side Issues: This raises the issue of how to manage invisible files,
see below.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/11/04 STREAM OPS FOR FIELDS. -> DBCHAR BACKEND CHANGES FOR CTREE
Forces: 
- want direct stream op for field to cut down intermediate conversions
- can't inherit friend operator<< functions 
- such a stream op would commonly be called on a base class dbField*

Resolution: 
- initially implemented with an AsChars() function on dbField
- now realised can have dbField op << call a virtual
OutputOnStream(ostream&)

Side-Issues 
- realised that a lot of this logic assumes the strings returned are
always C strings. The current implementation of the ctree backend fails
to allow extra space to write a terminating null if the field is full.
Therefore it will simplify a lot of things if I also make that change.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/11/04 LINKING THE TABLE AND CONNECTION Up until now it has been
considered that there should be no overt link between the table and
connection. The table definition follows the connection definition and
that sets a temporary link for the building of the table schema.

In the c-tree backend it has become necessary to store some
connection-wide information (a possible blob file no). Thus, I'm going
to use a behind-the-scenes way to make the link. When the connection
creates the backend, it will pass in an additional parameter of 'this".

One benefit of this approach, apart from keeping the link out of the
public classes, is that the member in the tableBackend_ctree can be of
class dbConnect_ctree and so save a lot of casting.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/11/05-06 HOW TO MANAGE INVISIBLE FILES 
The file and field tables for user-specified files are created by the
definition of the variables of type dbConnectXXX, followed by one or
more dbTable subclasses.

The additional information specific to a backend is not created until
the connection is made via a New or OpenConnection call. The details in
OOFILE_tableBackend::BuildSchema are hidden from the connection.

There are times when it may be valid to iterate over the visible
schema, in constructing report-writer and query front-ends. Thus there
is a need for some tables to be flagged as invisible, or managed by a
separate structure.

Let's assume for now that relations are handled in some way such that
the additional tables they need are added to the dictionary during the
constructor. Thus, by the time BuildSchema is called there is no need
to add further tables.

To keep things simple, invisible tables will be managed by the same
dictionary. This means the BuildSchema doesn't need to know if a table
is visible or not, or whether it was declared by the user!

Side-benefit: If I add handling for visible vs invisible members to
OOFILE_DictRep and OOFILE_Dictionary, I get the benefit of this in
handling invisible fields, when we later need to do so with implied OID
fields (and possibly others).

Realisation: This is possibly necessary for later invisible files (eg:
for relations) but totally unnecessary for the Blob file. Reason - this
a Direct file and doesn't require all the management of an ISAM file.
Thus, I'm just going to reserve a fixed file number (as far as the
dbConnect_ctree is concerned) for the Blob file, and create it after
creating all the ISAM files with the mTables loop.

For now, I'll just hard-code the blob file number into the table
backend GetFieldWriteDest and GetFieldReadFrom.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/11/07 LAZY LOADING OF BLOBS 
The supposition is that BLOB's (including Text fields) should defer
loading until the last possible minute. This saves additional database
movement. Thus, we need some kind of dirty flag in blob fields. For
simplicity in implemetation, and the fact that we may want this
facility in nonblob fields later, this should be put in at the dbField
level.

Implementation has shown that, in addition to being marked dirty, the
lazy evaluation of blobs requires a second flag to indicate if they
have yet been loaded. It would be possible to use the pointer to blob
storage as the flag, but there is more to gain by keeping the storage
and flagging it as unusable, than just dumping it and redeclaring it.
(Later could be purgeable?)

So, the virtual dbField::Reset() will be overriden for blobs to also
clear the mLoaded flag. There is a two-way flow between the dbBlob or
dbText operator= and the respective backend. If we assume that most
blobs are going to manage their own buffer, the backend needs to be
able to guarantee the existence of a sufficiently sized buffer. Thus
the backend calls back to the blob field with AllocRoomFor().


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/11/08 BLANKING NEW RECORDS 
Initially I wasn't blanking new recs. Now however I think it is easier
on two grounds: - I have a suspicion I will have trouble searching if I
haven't padded fields (with nulls, at present) - setting the whole
record buffer null saves me having to iterate through the list of blob
fields and set their offset entries and lengths to 0.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/11/09 RENAMING AsChars TO CopyAsChars 
After discussion with Ken it became obvious I hadn't considered what to
do about returning binary values through AsChars.

He asked if he could have a pointer to a mungeable block of chars,
rather than the internals of the field. We agreed this made more sense
for the binaries (otherwise I'd have to worry about when to destroy
it!). The function was renamed to clarify it's return value and the
responsibility of the caller to delete the pointer.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/11/10 ADDING DATABASE DUMP 
While testing (and crashing) the Heritage database, realised it would be
useful to have a database tester that could generate random data and
dump out a set of tables. Decided to add the appropriate stream
function for a connection, and test generators at the field level as a
virtual function.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/11/19 SEARCHING RE-THINK 
While actually implementing the searches I've come to realise that the
CommitSearch ideas are rather clunky from the user's point of view, and
NOBODY has given me feedback on beta3 as yet!!!!!@!

Thus, for now all search clauses take effect immediately. However, to
soften the effect of this, the selection of records will be stored in
an handle/body pair (like OOFILE_Dictionary). This gets around the
potential copying of zillions of record pointers, just because we're
making a temp copy for iteration!

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/11/19 IMPLEMENTING RELATIONS 
Yet another trip around the bloody syntax roundabout! I've been thinking
about the meta-data involved. It would seem there is a need for a
database containing a list of relations. This is the type and actual
objects involved in the relation. With a bit more thought, we may be
able to reduce relations to:

class dbVisits;

CLASS_TABLE(dbPatients) dbChar	Name; dbVisits*	Visits; ... };

dbPatients  Patients; Patients.Visits = Visits.RelatedAsPart();

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/11/19 DEFAULT SEARCH FIELD 
Realised it makes more sense from the user viewpoint if the default
search index (ie: People["Dent"]) is the same as the current sort
order! Thus, don't need separate SetDefaultIndex or anything like that.
To imp. keep the sort field pointer in dbTable, so the backend doesn't
need to know the concept of default keys.

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/11/29 ***** IMPLEMENTING SEARCHES ***** 
LINKING DBFIELD TO PARENT TABLE 
Realised that I need dbField to have a member pointing back to the
table, for resolution of search operators (otherwise NO way of
determining the table). Thus, the only decision was whether to keep
mBackend in dbField or use some form of reference through to the table.
I don't think the overhead is significant in storage, and the
performance and encapsulation advantages seem worth retaining mBackend
at the field level.


WHO RESOLVES LITERALS? 
In an expression such as People.Name=="Fred" the evaluation goes
through: dbChar::operator== ---> create a dbQueryClause I first opted
to have the literal handled at the level of the operator==, and to have
these specified for each field type. This gives me type checking of
valid arguments, but has a lot of work involved.

We basically have two classes of field - numeric and text searchable. I
therefore decided to have a mixin class for searching behaviour for
text fields, but inherit for numerics from a common base dbNumericField
(there are lots of other operators that should be defined for
dbNumericField).


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/12/04 FINAL RELATION LINKING SYNTAX 
Choice of either creating explicit relation objects: dbConnect theDB
dbRelation Supervision("Supervision");

Supervision.LHS(Academics, Academics.Supervises, "Supervises");
Supervision.RHS(Students, Students.Boss, "is Supervised by");

alternative syntax for setting up a dbRelation object, also showing the
join fields (would be 4th parameter to LHS/RHS as shown above)

Supervision.Names("Supervises",        "is Supervised by")
.Tables(Academics,           Students) 
.Links(Academics.Supervises,
Students.Boss) 
.JoinField(Academics.StaffNo, Students.SupervisorNo);


OR the "assignment" approach (implicitly creates a dbRelation)
Academics.Supervises = Students.Inverse(Students.Boss); 
Students.Boss = Academics.Inverse(Academics.Supervises, "is Supervised by"); 
(showing an optional path name)

You can use further qualifiers on the latter, to add all the
information including an explicit join field
Students.Boss->JoinField(Students.SupervisorNo);
Students.Boss->PathNamedAs("is Supervised by");

This forces us to collapse all standard relations into one type, so
that Inverse can create the relation object but it can be modified with
further attributes. If dbJoinRelation was a subtype of dbRelation, we
would have to change the type of the relation created by Inverse, as we
encountered modifiers. This is getting messy.

Further thought - who says we have to use the same navigation
techniques for both sides of the relation? It would be possible to have
some kind of part navigation on one side and yet have simple join from
the part to the parent, as this is infrequent!

The assignment option seemed impossible, as the dbTable* on the lhs is
assigned without any way of telling its containing table. The trick is
that the process occurs in pairs. Thus, after both assignments, we have
specified the pointers to follow, created a dbRelation and created
special "traversal" dbTable objects which specify the tables on both
sides, (optional) their path navigation names and other relation
adjusters.

To make sure relations are specified properly, including the Inverse()
clause, we could invalidate the simple assignment: Academics.Supervises
= &Students; by making the operator& cause a runtime assertion and be
private access, so it will usually cause a compile-time error. However,
in practice this turns out to be a nuisance as it prevents the user
from EVERY having a pointer to a dbTable subclass. This is too
restrictive - people will have to live with it!

In all these cases, when you use the related table, eg: numVisits =
Patients.Visits->count(); you are referring to the fields related to
the current record in Patients. There is never a need to relate to all
Visits for all Patients in current selection, as the circumstances in
which you would need to do this in other databases are catered for by
other mechanisms in OOFILE. eg establishing a dbView cout <<
dbView(Patients) << Patients.PatientNo << Patients.Visits->Treatment;


SIMPLIFYING IDEA OF JOINING ON FIELDS 
Unless convinced to the contrary, I'm going to assume that if you use an
explicit join field, then all relations are dynamic via value. Caching
of the links in a relating table will only occur if using OIDs for the
relations. This has the advantage of simplifying things - the
additional table for managing relations need only contain OID types!

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
RELATION IMPLEMENTATION of RELATIONSHIP CHAINS 
The idea of *just* copying the dbTable to provide a link is fallacious -
it doesn't handle chains of relations. (You would have to be cloning a
table already marked as being a related clone, not an original, and
this wouldn't handle circular relations).

So, we need a smart pointer type dbPeopleRelPtr that returns a pointer
to dbPeople but also sets a transitory navigation class variable. This
is actually appended, to cope with chains of relations.

Try generating the smart pointer with an inherited MakeRelPtr(this)
which returns the correct class (ie: can be macro-ized).

All field access operators should ask the table to check if they have
the correct record loaded. This allows for transparent checking of the
static relation chain as well as the case where the current record may
have been moved by other actions.

The static relation chain must be reset by the terminal action in each
case, be it reading data, storing data or performing a search.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/12/27 SELECTIONS 
After discussion with Ken, it becomes apparent that we are likely to
build chains of selections at some point in evaluating relations. Also,
the current selection state and current record variables maintained in
OOF_tableBackend_ctree are rather messy. Will move these into selection
and make that clearly a class for just ctree selections.

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/12/28 RELATION IMPLEMENTATION 
linking the FIELD to the RELATION CHAIN The work so far on relation
chains is fundamentally OK, except the nodes along the chain may in
future be cached selections on the database.

The BIG problem identified to date has been the delay between building
a relation chain (in some static container) and evaluating that chain
at an uncontrollable later point.

The rules now are, that every relation chain ends with a field with
parens: People.Visits->Date() so that operator() will be invoked on the
field to IMMEDIATELY match up the relation chain.

See "OOF Debug Notes" for the future implementation of a debugging
technique to catch instances where users forget these parens.

The operator() in this case returns a copy of the field, thus it is
fractionally more efficient to avoid the parens in most cases.

Fields are extended by having a pointer to a dbRelationChain. If this
is not 0, the field is responsible for evaluating the chain. Later, we
will introduce caching of chains so multiple chains to the same table
will be evaluated once.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
94/12/29 NAMING CONVENTIONS 
There has been a conflict so far between the common names for some
functions being lowercase, with my desire to start public methods with
uppercase and private/protected with lowercase.

I've now decided to reverse the case issue, so all public methods are
lowercase and private/protected methods start with uppercase


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/01/01 DEPENDENCIES 
I'm going to implement a simple way of encapsulating checks prior to
closing records - a virtual method dbTable::canSaveRecord. Note: it is
up to a GUI to call this - this is not a constraint on every db op.

Later we will have more flexible dependencies, but this lets us
encapsulate validation and other data manipulation. Remember that
derived fields let us perform calcs on loading record, so between the
two we may be right for now.


CONTROL OVER WHEN TO SAVE 
Saving is handled as a generic thing for all dbTables - allowing -
requireExplicit - saveAutomatically - requireExplicitAndBuffer (this is
set on all part tables in Owner-Part rel)

As we have already got ContextChange in place, it can handle these
decisions!



-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/01/19 RELATIONS and CLONING FIELDS 
In the past I've shied away from cloning all the fields on a table, just
to setup a related context. Horror scenarios of updating two related
fields in a 200-field table haunted me.

However, it has become obvious that something like this needs to happen
- there are some operations (eg: newRecord) in which the backend
iterates over all fields. If the related backend is left pointing at
the prototypical fields then it will reset the wrong fields!

So, the blunt approach for now will be to clone all fields on the
related table, regardless. A related field expression will then return
a reference to one of these related fields, instead of cloning an
individual field as it does now. We are moving from a retail (clone
field on demand) to wholesale (clone all and supply on demand)
approach.

The corollary to this decision is that there is now very little
difference between cloning a table to be at the end of a relation
chain, and the programmer copying a table to give a different iterator
over the database.

The only item which varies is the current database selection - there is
no need to copy it for a clone table in a relation.

Related things to do urgently: 
- turn OOFILE_String into a
copy-on-write, to cut this overhead.

Later sophistications 
- only clone fields on tables at end of chain.
Tables cloned for intermediate nodes in a multi-link chain don't need
fields. However, there will have to be some work on the backend (clone
join fields only maybe).

- clone an empty dictionary and attach fields as related fields are
referenced. This is halfway back toward the current ondemand strategy
but would leave the backend pointing at all active fields, where it
current points at all prototypical fields and doesn't know about the
active related fields.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/01/22 WHEN TO UNLOAD RECORDS 
Until now, the emphasis has been on delaying actions. However, in trying
to debug the saving of related tables, it has become obvious that
flushing the cache of saved records is a complex issue.

Specifically, look at ooftest2.cpp in AddVisits. When newRecord is
called, there needs to be some form of logic which decides if the
current record cache should be unloaded (say if belonged to a previous
Patient) or if we are calling newRecord to add a second visit for the
current Patient. This can be determined by the state of the relation
chain - is it valid or not. Thus, the only two places in which a
relation chain calls unloadRecord are in newRecord and validateContext,
both of which are conditional on  (!mContextValid).

Note: this allows interfaces which allow the Save command to be used
but retain the information on-screen, another reason why delayed
unloading will be required in the long run.

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/01/28 USING VIEWS WITH BLOCK EDITORS & MORE FLEXIBLE DATA VIEWS 
In considering adding table editors to the LGA Places program, it became
obvious that something needed is a dbView that can be passed to an
editor, able to edit data in a related table. The dbSSBrowseTable
already handles display of fields from the current table but the main
problem is handling the relation.

Note: the reason why you might create a dbView on People.Visits, rather
than People, is the context. A dbView on the related table will iterate
over the current related collection, for the loaded People record. A
dbView on People will iterate over all the People.

Thus, if you are displaying a table of Visits for a person, you just
want the related table in the view.

Possibilities arise: 
1) abstract dbTable & dbRelRefBase to a common dbDataSource class
2) use different classes of view to handle the different sources
3) have a way of returning a related table from a dbRelRefBase
4) define dbDataSource as a helper class that we use from dbView, with
subclasses of dbDataSource managing dbTable, dbRelRefBase etc.

1) To handle related tables the same as direct tables, we can abstract
the basic iterator, loadRecord, newRecord etc. up to a common
dbDataSource class. From this would descend dbTable, dbRelRefBase and
(in future) other data sources, eg a direct file map.

A dbView then becomes a manager of a data source and a list of
accessors (fields, functors etc.).


2) Using a different class of view implies the view hides the
difference between table & related table (dbRelRefBase). This is
possibly the easiest solution but has the minor hassle that we have to
explicitly create different dbView types.


3) For a dbRelRefBase to return a table implies that the table is then
able to manage its own relational context. This further implies that
each generic table wears the overhead of checking if context should be
updated etc. on each operation (as in dbRelChain). It is also
table-centric, not offering much flexibility in future to model other
data sources.

4) A helper source for dbView has a few advantages: - dbView can retain
its iterators for iterating across members, with myView.source->next()
used (eg:) for vertical iteration

- the source can be subclassed as we like

- sources can be used elsewhere (if necessary)

- we aren't stuffing around with the class hierarchy of dbTable
(instead of inserting an abstract layer, we're adding a parallel
abstraction)

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/01/28 A NICER SPECIFICATION FOR JOIN RELATIONS 
It has bothered me for a while that join relations are redundantly
specified (the table is implied by the join fields) as well as having
too many parameters. I just realised that the lhs/rhs form would look
very neat if the first param were EITHER a table, or a join field.

This also implies that you don't have to specify the .tables call if
using the "vertical" form (dbRelation.names.tables.joinFields etc.)

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/02/05 VIEWS AND SAVING RELATED RECORDS FROM Spread/VBX EMBEDDED EDITORS 
There are several models for saving records from embedded
part-relationships, all assuming that the multiple change is batched to
be saved with the main record.

- create a set of transactions or control objects to apply the changes
- directly modify records, relying on the buffering mechanism in the
relation to store the changes until necessary 
- treat the embedded table as a block and save the changes by writing
the entire block back to the database, overwriting, deleting and adding
records as necessary.

Without arguing the relative benefits of each, the implementation of a
block update now implies developing several capabilities: 
- parsing a block of text into records
- scanning a current selection and deciding if records match
- updating 
- deleting records

As import capabilities, driven by a dbView, will be required anyway
this represents a code-reuse opportunity.

One item in the favour of the block-update approach for now is that
implementing deletion within the user-interface of an embedded table
requires only row-deletion from the table. The other two approaches
require the delete button to create some form of control object or make
a more complex call to the background.

However, in the interests of speed, it might be faster for now to just
get the Add and Modify modes going with the current table editors
(propagating the add message through to the related dbTable).

The block-update approach also involves writing import logic for a
replace-model of updates. A simpler model which would be acceptable
within the Heritage project is an add and reject model, which logs
rejected duplicates.

MODEL FOR "FIELD-WISE" UPDATE 
It seems from testing that operating the sheet in a non-row mode means
there is no concept of the current row?

The "field wise" refers to the approach being treated by the database
engine as the same as assigning to fields. Dirty & deleted records will
be stacked.

Updates to cells will be assigned to their backing field on exit from
the cell.

FLAGGING RECORDS 
- an additional mDeleted flag should be added to the context. I
initially considered changing the mNew flag to be a tri-state but it is
possible for the user to add and then delete a record within a single
edit, so it never exists. If we are to later add an Undo capability for
this then we need to preserve the (deleted) new record until saving.



-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/02/08 SPECIFYING A DEFAULT FONT FOR EDITING FIELDS 
There is no way under the Windows dialog manager to have a different
font applied to the fields on a dialog from the static text. Thus, we
need to explicitly invoke zControl::setFont for each control.

The slightly shonky way to add this in a hurry is to have a static font
in the dbEditMgr (which can be read by all) and to change the editing
classes dbEditLine etc. to set the font.

Some control is given by: 
1) have the font spec a member of dbEditMgr, so it can be overriden by
preferences later
2) only create the font with an explicit enabling call from each dialog,
so we can choose to turn on the behaviour dialog-by-dialog

ALTERNATIVES REJECTED 
- explicitly create and set the font in each dialog - just messy


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/02/11 ADD SERIAL NUMBERS 
There is an urgent need for some means of generating unique sequence
numbers for records. This is provided by ctree in the GetSerialNbr
call.

ISSUES: 
- m/u synchronisation 
- the race condition if two users are saving together
- how to call

For now, assume that the call is performed just before a SaveRecord and
we won't worry about synchronisation for m/u. The problem is one of
locking - we need the serial number guaranteed prior to the actual
writing of the data. For m/u a solution may be required which uses a
separate file and locks it to guarantee a number.

Providing a virtually straight-through interface to the ctree call will
do for now but the synch demands on using it immediately before the
save mean the programmer will have to avoid writing code that uses the
sequence number. It is one matter for us to propagae the sequence
number to related join fields but, say, getting the sequence number at
the start of a gui editing sequence with possible duration of minutes
would be much more dangerous.

CLEANER SOLUTIONS A fancier approach for specifying the number would be
to have a collaboration mechanism for "pre-save" events and have
sequence number generation a ready-coded example of such.

A guaranteed sequence number allocator would probably require using the
OOFILE soup file to store a sequence object.

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/02/14 ADDING NEW RECORDS TO SELECTIONS 
In programming the embedded tables for related records, it became
obvious that new records should be added to selections (using the model
that every row in the table corresponds directly to an entry in the
current selection).

The current model fails to do this.

The suggested change is basically that newRecord appends an entry to
the selection, similarly the add button appends a row to the table.

Creation of the new records will nearly work with the current stacking
of dirty records as each stacked context has its mNew flag checked when
being written out, and we have validated that writing several new
records is OK (ooftest2).

However, there is a problem with returning to these contexts if the
user clicks on a different row. The current
OOF_tableBackend_ctree::LoadRecordAtOffset uses the record offset to
identify records in the cache.

Thus, this offset will have to vary for new records.

The slightly hacky solution proposed for now is two-fold: - track the
number of new records in the current selection - assign numbers
starting at 1 for the new record number - there will be no chance of
confusion for a considerable number of new records given the header
space in the superfile.

The only danger I can see with this approach is that the fake, small
record numbers eventually overlap real ones, thus the scanning method
of identifying records in the cache becomes unreliable. This does not
seem likely given the situations in which stacked, new records are
likely to be used.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/03/13 SIMPLE LOCKING 
The currently implemented approach of setLockingStyle which sets a
global Read or Write approach doesn't work - it's too broad and there's
no feedback from the application as to when locking is turned on and
off.

For now, it makes more sense to pull automatic locking out of the
backend and expose it to the application - requiring the user to call
readLock, writeLock and unlock on the entire connection (which is how
ctree handles things).


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/04/07 CONNECTIONS, BLOB FILE NUMBERING & ALLOWING MULTIPLE BLOB FILES 
Until now, I've used a constant for the blob file number. This
works fine as long as nobody wants a blob file specific to their table,
and as long as we only have one connection open at once.

As soon as multiple connections are opened, the 2nd and others are
based on a different starting point for the file number.

If we want the flexibility to allow future versions to have different
blob files, eg: on different disks or just to avoid mixing data, then
the constant has to go!

Note: the rational behind the current mixed, single blob
file/connection is to achieve better reuse by mixing fields. It is
hoped that in many cases, a lot of smaller blobs will appear and fill
in the gaps as others expand.

Thus, we are back to a Factory situation - when dbConnect_ctree
manufactures backends it must allocate a blob number and set it for
each class.

If we allow for multiple connections, then there are two situations: -
multiple simultaneous - multiple over lifetime of program.

There probably needs to be some file-number reuse logic for multiple
simultaneous - you can't guarantee that one connection uses only as
many file numbers as the next one.

For now, stay with resetting the file number on opening a connection -
it doesn't allow multiple simultaneous but causes no problems with
sequential opens.

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/04/08 BLOBS CROSS-PLATFORM 
There's no problem with the data within a blob - that's up to the
application, and plain text is OK. However, we are storing two longints
representing the length and record offset of the blob. These need to be
explicitly declared as numbers for the ctree flipping to work.

The problem is, if we map one blob field to two DODA fields, the
remaining fields are offset.

Solutions: 
1) map to two fields, and use a redirection for all field numbers->DODA
numbers
2) map to one field, inline with the rest, which provides the offset of
the real BLOB fields (a pair) at the end of the record:

fld1  fld2  blobOffset  fld3  fld4...fldN   (blob1RecLen blob1RecNo)
\________________________________^

Issues: 
1) Two DODA fields inline: 
- adds overhead of mDODAnum to each field entry in mFieldBufMap
- complicates CompleteIIDX & BuildDODA BUT
we've already broken the 1-1 correspondence between field number and
DODA number because of the leading 2 bytes


2) Offset field inline pointing to real Blob pair at end of rec 
- adds
overhead to each record of 4 bytes/blob field 
- complicates CompleteIIDX & BuildDODA even more
- complicates other routines that retrieve the BLOB info, requiring a
double indirection.

DECISION Go with option 1).

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/05/29 FILTERED AND METHOD INDEXES 
The whole issue of indexing and searching by method can be split into a
"virtual field" approach, vs a method semantics.

If we define virtual fields as fields which are calculated rather than
stored, then they have the same search semantics as any other field.
Further, their indexing is orthogonal to their existence - we don't say
"add a calculated index" so much as "add a calculated field, and make
it indexed".

Orthogonal to THAT idea is the concept of a field participating in
multiple indexes.

Let's say we have a dbChar  CompanyName which is indexed, keyword
indexed and the keyword index is also phonetically indexed.

This field contributes keyword and phonetic keyword values which can be
used to search the field:
Companies.search(CompanyName.hasWord("Software"))
Companies.search(CompanyName.hasWordSounding("Softwear"))

or we can search the table regardless of the
Companies.searchByWord("Software")

It is tempting to say build an abstract structure which allows for a
mixture of field and table level searching like the above. However,
whilst the indexing side can be neatly abstracted, the searching
semantics would appear to be closely tied to the semantics of the
index.

Thus, I conclude we can get by with specific semantics for keywords and
phonetic keyword searching as shown above. All other calculated indexes
can be handled by virtual fields.

However, the indexing algorithms used can be encapsulated in a Strategy
that is runtime replaceable. Thus a European user can fine-tune the
phonetic algorithm.


NON-ISAM INDEXES 
Having decided on strategies for defining & searching these indexes,
exactly how should they be stored. So for it's been easy to use the
ISAM indexes. We now have indexes based on values that are NOT in the
data record (or handled by a standard c-tree transform, like
uppercasing).

Choices: 
1) use one extra index file for ALL such indexes, using a
sufffix to distinguish the virtual index

2) use an extra index file for EACH virtual index.

To implement 1), we need to be able to do table-wide searches, thus
can't use a prefix. However, in using a suffix, we run into problems
with comparative searches - the Next value may be a value with a
different suffix.

DECISION For compound indexes, use the native c-tree construction which
uses ISAM techniques and consumes an index slot.

For keyword indexes and phonetic keyword indexes, use the pooled
technique.

For other calculated indexes - offer either.

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
95/09/15 C-TREE STATE 
Up until now, c-tree context state has been somewhat sloppily
represented by the combination of bool mNew and the
mCurrLoadedRecOffset, where a value of 0 indicated an unloaded record.

To fix the bug in all record selections, where we relied on our ISAM
context not moving, we need to preserve mCurrLoadedRecOffset even after
unloading a record (so we know if we need to reset the ISAM position
before calling NextRecord, for example).

Thus, mNew needs changing to a tri-state variable
eNew/eLoaded/eUnloaded.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
IMPLEMENTING COMPOUND INDEXES and VIRTUAL FIELDS 
95/09/21 
Even thought it is a minor waste of space, I think items like the field
buffer map should include a slot for these fields.

The basic tradeoff seems to be: 
- virtual fields, compound indexes etc are real fields, with a field
number or
- they are totally fake and don't descend from dbField.

The latter seems too kludgy to bother with and breaks the fundamental
model.

I think that having dbField descendants with no matching entries for
their field number would lead to a lot of backend code that knew too
much about the field types.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
SURVIVING MULTI-USER DELETIONS 
95/10/13 
The problem arose in the PhoneControl multi-user directory - an
overnight update destroys and replaces all records. There's room for
conflict if another screen is open throughout the process.

If our selection is allRecords, there's no problem as reloads are
straight from the database. If however we have a selection of one or
someRecords, we have stored record offsets and use LoadRecordAtOffset.

The issues: 
1) how often and how should we check if stored recno's are invalid?
2) how to react?


Detection Alternatives: 
1) an error 160 is issued if a deleted record is attempted to be read
via ReReadRecord. This doesn't cover re-reading records which have been
replaced coincidentally at the same address, but that is a lesser
problem.

2) We could store an OID or incremental number with each record and
check on re-read. This could be stored in the leading 2 padding bytes
(with the proviso that it must be reset before reaching 0xFE). However,
this doubles the overhead of selections.


Reaction Alternatives: 
1) Fail and abort unless the user provides a "policy object" as an
attachment to the table, that can handle this event.

2) Automatically repeat the last search action, requiring the last
search to be attached to the table.

3) Give in and start using c++ exceptions (I strongly suspect this is
not workable across the current range of compilers).

4) Pass failure status all the way back up to the application, without
exceptions.

5) Fail cleanly - clear the buffer and present an effectively
read-only, empty set of records.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
CONTEXTS VS CACHING 
95/12/23 
At present, a c-tree context contains enough information to identify a
record, plus the record buffer and associated blob buffers.

This is too much - we are overloading the concepts of containing
buffers with the idea of a lightweight context that lets us go back to
an object.

Currently we have OOF_tableBackend_ctree descending from and also
containing a cache of OOF_Context_ctree.

We will now have

OOF_tableBackend_ctree descends from OOF_Context_ctree &
OOF_Buffer_ctree. The mRecordCache member contains a list of
OOF_Buffer_ctree's.

The blob is managed for now by a list of pointers. The MAIN point is
that the current object does not have such a list, as its fields manage
their own storage. The only time we populate the list is when we cache
a record. At that time we transfer the pointers to the list and the
current blobs are all reset.

The cache needs to contain both contexts and buffers and is indexed by
the contexts. When we store something, we will create both. When we
load a context from the cache, we will only retrieve the body (we
already have the context).

Eventual Realisation: 
1) Contexts don't need buffers, but buffers need contexts. Hence, don't
multiply inherit from both, but have a tree: OOF_ctreeContext ->
OOF_ctreeBufferedContext -> OOF_ctreeTableBackend.

2) Caches are sophisticated enough to be worth a lightweight subclass
of OOF_ExpandableLongArray which evaluates a contains() member.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
SORTING AND DEPENDENCIES 
95/12/26

Sorting by an index is a virtual no-cost issue.

However, any time we have a sub-selection of records, due to a search
or traversing a relationship, we can't use indexed sorts.

(Note: future heuristic may use an index to optimise sorting a
selection that represents a large percentage of the total file.)

However, we don't want to penalise the user with sorts until the very
last minute (but also give programmer option to force a sort).

Thus, we are led to the conclusion that *when* to sort is a dependency.

It is very simple to go from having to call a sort method on certain
state changes, to abstracting these into a common set of changes, and
hence to a generic dependency mechanism.

Thus, setting a sort order simply adds a dependency on that table.

Potentially, we need to re-sort whenever the selection changes, or a
field in the selection changes on which our sort depends.

Triggers for these: - search - add record (field changes) - delete
record - save record (field changes) - selectAll - relational join


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
IMPLEMENTING INHERITANCE 
96/01/07 
GOALS 
1) Compatible with existing file structure, with 2byte null pad at front
of records. (Don't rule out extra options, but offer compatability).

2) Fast, able to use record offsets (but later allow these 32 bit
quantities to be replaced by 32bit OIDs).

3) Allow subclasses that add no data, just methods.


APPROACHES 
1) Use existing two bytes as data for subtyping. Remember we can't put
high binary values in the first byte, so can't treat as simple integer,
as that's used as a flag by c-tree with values like FF = Deleted, FE =
Resource.

First byte = Subclass State 0 = this is a base class object 1 = is a
subclass object that adds no data 2 = is a subclass object that adds
data, so we have other files Note: by "adds data" we mean that at least
one subclass in the chain down to the leaf adds data. eg: we may have
an immediate subclass that adds a field, then 3 subclasses down that
are just subtypes with no extra fields. The main purpose of the
indicator is to say that some extra processing MUST occur to bring in
some

Second byte = which Subclass. This allows up to 255 subclasses of a
given class.


POINTING TO RELATED RECORDS 
It's easy to add pointers in the subclasses. They are either a true
record offset or an OID. Navigation from subclass (partial) record to
parent record is directly via this pointer. The pointer value is also
indexed, so navigation from parent to subclass record is by an indexed
search of the subclass for this pointer value.


PARSING INHERITANCE OUT OF A DECLARATION 
Looking at the list of constructors executed, an inheritance situation
has a single dbTable, followed by a series of dbField and dbRelRef
constructors.

There's no inherent way to tell which dbField's are part of a parent
class and which are in the subclass.

Two methods come to mind, to put a break point in the list of dbFields
so we know that the remaining fields are part of a subclass: 1) you
have to call a function in each classes constructor. 2) the
OOFILE_METHODS macro creates an extra member which has a constructor
with the same effect as 1).

This gives a constructor sequence (B inherits from A): 
dbTable, dbStartFields, dbField....  A dbStartFields, dbField....  B

Another method appears possible - post-processing to work out which are
tables due to the pattern of adding fields. This falls apart if you
have multi-level inheritance and no instances of just the base classes.

With Multiple Inheritance (C inherits both A & B), we have a sequence:

dbTable, dbStartFields, dbField....  A dbTable, dbStartFields,
dbField....  B dbStartFields, dbField...   C

This is impossible to distinguish from a class A followed by an
inherited B->C separate class.

The easiest way around this, and to avoid other issues, is to define a
dbMixin base class for B.

Alternatively we can require people to virtually inherit from dbTable.
Have to check the implications of this but I don't think there are any
apart from an extra redirection step in referring to members of
dbTable. (AFAIR the only significant limitation of virtual inheritance
is not being able to downcast and we won't be downcasting from dbTable
to a user subclass.)

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
IMPLEMENTING GENERAL RELATIONSHIPS 
96/01/08

POINTING TO RELATED RECORDS 
This is basically the same as in inheritance:
1:1 Both records have a pointer to the others

1:N The N records have pointers to the parent, and are searched
similarly to inheritance. The parent's pointer slot is a count of
related records. (Unless we have evolution from 1:N to 1:1 records,
this is a safe re-use).

N:M The value in both sides is a unique OID of the relationship itself,
so traversal in either direction is by indexed search to get a result
set.


PARSING A RELATIONSHIP OUT OF A DECLARATION 
Now that we have only a single extent of each type, we can have a
default situation. If there is just one relationship between classes A
& B then the traversal members for each can work out their inverse
automatically (it's the only pointer in B pointing to an A and vice
versa).

If there are multiple relationships between classes, then you have to
create an explicit dbRelationship for each, as done at present, or use
some other means of assignment.


AVOIDING CONFUSION 
If using join relationships, the form used at present, then a
dbJoinRelationship must be used. There's too much danger in the
alternative, which assumes a pointer relationship unless you specify
the join fields.


RELATIONSHIP OIDS 
With the n:m relationship we have to generate an OID to be placed in the
traversal member on both sides. This may be generated by an indexed
search on one side of the relationship.

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
DECLARING GENERAL RELATIONSHIPS 
96/01/21 
Naive idea, which appears workable at first:

Instead of the smart pointers currently in use, use simple pointers to
persistent classes.

At the time of class declaration, these can just be forward declared.

At the class definition, the pointers will of course be indeterminate.

By assignment, the sides of the relationship will be filled in,
pointing to a prototypical table.


96/01/21 RULING OUT SIMPLE POINTERS ONCE AND FOR ALL 
The core things that must happen with the traversal member: 
- it can be instantiated (creating any necessary cloned tables) by any traversal
expression. 
- when the base table is cloned, the traversal member must
be reset. Note that both the above points would work with a smart
pointer which is lazily instantiated and reset, or a dbTable* which is
set at initial table definition, and reset to point to a clone at
cloning of the dbTable.

If we have a raw pointer, it must always point to a valid table. This
eliminates raw pointers as a possibility. At first it seems like we
could get away with pointing to a "I'm Related" table and relying on
the field to instantiate the relationship. However, if we navigate to a
field by raw pointers, there is no logging of the relationship
traversal to that field. Thus, the only way it could work would be to
always instantiate each related dbTable before it is used. Taking this
principle to a logical extension, it makes circular relationships
impossible. It would also impose a significant load if we had a base
table that was commonly cloned, with a high fan-out to related tables.
We would be frequently cloning related tables unnecessarily.

ADAPTING CURRENT RELATIONSHIP TRAVERSAL 
From the above, it is obvious that we will need to keep the current
smart pointers to manage traversal paths. The changes will be: 
1) operator-> will return a clone, or clone the next table in the chain
immediately, rather than just putting an entry on a list. This has the
consequences:
- we can drop the parens on the destination field
- all expressions are table expressions, eg: People.Visits.count()
becomes People.Visits->Count();
- the entire relation chain management logic can be removed. Tables are
either at the start of a rel

2) Relationships will be established by simple assignment. This
requires an operator= on the xxxRef classes.

3) On cloning a table, all its relationships are reset so any
instantiated links will be rebuilt.

4) fields will become ignorant of whether they are on a related table
or not. They will always call their table to validateContext() which
will mask the relationship traversal. This cuts the complexity of
cloning a table (doesn't need to pass on the relation chain to the
fields) and lowers overhead of fields which will no longer multiply
inherit from OOF_mixRelChainEndPoint. Pushing this responsibility back
to the table will also make context management easier for inherited
situations.

DIFFERENCE BETWEEN OWNER-PART AND GENERAL RELATIONSHIPS 
In both cases, you traverse a relationship to a cloned table that
manages the related selection. The Owner-Part manages referential
integrity: 
- if you delete the owner, all parts are deleted 

The general relationship manages referential integrity to a lesser extent: 
- if one side is deleted, all links are deleted on the other side
- if you modify the rhs of a relationship, those changes are saved when
the lhs is saved
- if there is a join field, the join value is propagated to new part
records.

96/02/03
From the above, it would make more sense to characterise the difference as a "propagates deletes" which we can call in the constructor, as we now call joinField().

This allows us to later extend this - we might temporarily propagate deletes.

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
DEPENDENCIES AND RELATIONSHIP MANAGEMENT 
As documented with sorts on 95/12/26, we have a similar situation with
Relationship management. There used to be a complex framework of
classes being up invalidated as dbTable operations occurred. This
framework has now been simplified and dbTables no longer have an
explicit list of their relationships.

To replace the invalidation of relationship lists, we need to add our
relationships as another dependency, triggered with any change of the
context of the base table.

This forces us to confront the kinds of state changes and the question
of who should be dependent on the base table - the traversal member
(dbRelRefBase) or the cloned table managing the related selection?

The answer lies in the inheritance required, and is *neither*.

Using a helper class, that reacts by changing the dbTable state, keeps
the dbTable methods cleaner and is a principle we can extend into other
areas (like sorts).


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
IMPLEMENTING RELATIONSHIP TRAVERSAL & OIDS 
96/01/26-27 
Who knows what?

In a join relationship, the dbRelRefBase that performs the relationship
needs to know the join fields on both sides, and which dbTable is being
affected.

In a pointer traversal, the mechanism varies according to the number of
members on each side: - if the rhs is 1-ary, take the OID from the lhs
rel'n and directly load the rhs - if the rhs is N, do a search on the
OID stored in the traversal member space.

This implies that at any time, a rel ref must supply an OID for the
other side to use as either a search value or direct reference. The OID
that is stored is supplied by the other side.

We will abstract out OIDS. They will always be an unsigned long and a
given table will determine whether this is a record offset or a
generated unique OID. This will be hidden behind the currentOID() and
gotoOID() methods.

Rel Kind	A stores		B stores 1:1			OID of B		OID of A

1:N			nothing			OID of A


N:N			OID of rel		OID of rel


inherit		nothing			OID of A

Note: inheritance chain treated as a set of 1:N relationships, eg: if C
inherits B inherits A A contains nothing B contains OID of A C contains
OID of B & A

There are two situations when it comes to saving data. By careful
sequencing of saves, most situations are fairly simple. 1:1 If both
sides are new records, one must save twice, first to get an OID for the
other side to point back to it, then again after the other side has
been saved.

1:N save 1-side first, then broadcast (most common example will be
owner-part)

N:N get OID of rel, then save order is immaterial

inherit save from base class first


As we are updating pointers on the rhs, even if it is not a part
relationship, this implies that saves should be propagated. It also
raises the question of loading a single copy of an object, rather than
(as present) allowing a single program to load multiple copies. This
may be deferred for now but can probably be done as a simple extension
of the current cache. The only issue with that is identifying when to
write the cached items, and keeping the current object in the cache
(making it current removes it from the cache).


HOW TO GET OIDS AND SAVE THEM!!!!! 
From the above discussion, it can be seen that we have a case out of
many possibles, where extra saving action is required.

The easiest way to manage this is by making these dependents. Happily,
this has already occurred. The inverses of these traversal members, if
needing to be saved, are already dependents.

Thus, dbRelRefBase dependents that are 1-ary sides need to set
their opposite number's OID. This shows that the dbRelRefBase itself
needs to be the dependent, NOT a separate class as originally tried.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/01/30
CLONING RELATIONSHIP FIELDS
There's a subtle gotcha in cloning relationship fields, as occurs when we
have a join relationship and the table is cloned (eg: for the rhs of a
relationship).

Assume, as in ooftst02, that the relationship field is the first field in
the class. At the time of this field being copied, the other fields will
not yet have been copied and so won't have linked themselves. This means
the join field will not have been constructed and so won't yet be
registered with the cloned database.

There are a few ways around this:
1) After the cloning procedure has created a new userTable, it calls
something like userTable->postCloneCleanup().

2) Post a dependency that will have the join field updated before the
next action.

3) Have a special value in the join field and check for it before using,
so it is setup properly.

ISSUES
1) Adds an extra processing step which lengthens clone time.
2) Relies on the dependency having a message broadcast before the join
field is needed.
3) Contaminates a lot of code.

CHOICE
1) Although a nuisance is by far the cleanest and gives us a frame in
which other dbFields could be extended for post-clone cleanups.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
SAVING OIDS IN MULTIPLE CACHED RELATED RECORDS
96/02/01
Say we have a typical Owner-part relationship where the rhs is being
cached. eg: a listbox on a GUI dialog, where invoice line items are
being entered.

When the Owner record is saved, the save propagates to
relatedTable->saveRecord() which then saves all the cached records.
However, as we have delayed setting the OIDs of the relationship
fields, we need some way to set these OIDS for *each* record in the
cache, not just the current one.

ALTERNATIVES
1) For each cached record, have a dependency for each relationship field
that needs completing. (This is one/record by the nature of the part
records - at the rhs of a relationship

2) Have a "pre-save" pass performed on each cached record.

3) The relationship field, when it receives the saveRecord, does a loop
at dbTable level, pulling in records from the cache and setting its 
value in all of them.


ADDITIONAL COMPLICATION
In examining these problems, it becomes obvious that there's a flaw in our current split of responsibilities:
dbTable - broadcasts status changes & calls backend
backend - caches multiple records.

When we save, the broadcast happens at dbTable level. However, a two-level relationship causes problems:

current Patient ->  Visit 96/02/01 -> Drug blah
               \->  Visit 95/12/20 -> Drug blah
                                  \-> Drug blah2
                                  
Assume Visit 96/02/01 is the current Visit and 95/12/20 is in the backend cache. If we just broadcast a save at Visit dbTable level, when the cached 95/12/20 is saved it won't broadcast to its dependents. Thus it seems that the backend loop to save cached records must now call dbTable::saveRecord, not its own saveContext.

NICE WAY TO GRAB OIDS
Initially I thought all relationships had to update their OIDs when you saved a related record. This is of course false - only the relationship managing the link to the lhs need save. The dbTable points to this relationship field with mRelated. So, it is actually trivial to call an updateLink method on that field, from dbTable::saveRecord. This lets us remove the use of dbFieldCopier for join fields, as well as giving us a point at which to save OIDS for other relationships.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
EVALUATING IF() EXPRESSIONS FROM dbQuery OBJECTS
96/02/01
There are two ways I can think of. The most obvious is that a cast operator on dbQuery does the same sort of parsing that occurs in the search backend. This is painful and would be rather slow.

A more optimal solution would be for dbQuery to cache a boolean result, and quickly calculate it in the field functions that construct the dbQuery object (eg: dbChar::operator==). This has a lot of very lightweight bits of code and is easily extended as we add new dbQuery builder functions.

LATER NOTE - due mainly to comments in "More Effective C++" casting dbQuery has been ruled out as it breaks short-circuit evaluation.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
MAKING VIEWS AWARE OF MULTIPLE FIELDS
96/02/02
Views are our primary means of coupling the database to input/output. At present they allow:
- output on a stream
- driving a report
- linking to a browser.

When combining related fields on a view (or future multi-value fields) we have the problem of different numbers of values for each row. The obvious way to solve this is some form of counting pass, which totals up the values in related (and MV) fields. This can get complicated when extended to multi-value relationships and later some heuristics can be introduced (eg: keep a list of possibly MV fields). For now we'll just blindly call count() across the whole lot.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
GUARANTEEING NON-INDEXED SORTING
96/02/19

A minor bug has been discovered with the dependency-driven sorting, in that some combination of steps fails to trigger the sort, eg:
	People.setSortOrder(People.LongUnsigned);
	People.selectAll();
	cout << People;

is safe, but if the selectAll precedes the setSortOrder() then no triggering event happens before the dbTable::extract() calls start().

It would be desirable if the sort order could be specified independent of how the selection is generated.

Possibilities:
1) Have dbTable::validateContext() check with the sorter if it's ever been fired & fire it.

2) as for 1) but only call from start()

3) Use dbSorter::receiveMessage()
- Have setSortOrder() perform an unloadRecord, thus guaranteeing that start() 
  will call ContextChange() and then mBackend->start().

- dbSorters flag if they've ever been triggered. On receiving OOF_ChangeContext, 
  if a sorter has never been triggered it will perform the sort.

Decision:
Use 3) as it's a lot cleaner and more generic - other ContextChanges may need to trigger sorts.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
SORTING SELECTIONS BY COMPOUND FIELDS
96/02/19

OK, I blew it with the idea of the compound indices, in that they are
useless if you are using a search then sorting by them. 

The current scheme for sorting selections has nothing to do with the
indexes!!!!! 

There are also radically different cases, depending on whether you are
searching by the leading item in the compound index or some other
indexed field (including the trailing items). 

ie: for the compound index DirFile << Directory << Filename it is a far
simpler case to be searching by Directory. 

Possibilities
1) Add a sortSelection function for compound fields. In the specific
case of sorting by DirFile, I could just do a blind copy of the dbChar
sortSelection for now but it wouldn't be savvy enough to check for
reversed segments. 

2) Add a sortSelection function for compound fields that uses the index,
just looping through and comparing record numbers (this is woefully
inefficient). 

3) Provide an xBase-style filtering mechanism for use when you are
searching by the leading item in the compound index. eg:
start(FileName.startsWith("...")). 

4) Provide a generic sort by N fields, and call that from
SortSelectionNow(compoundField) 

NOTE: If you are searching by the leading field, try using startsWith()
on the compound. This should work. 


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
HOW TO BUILD AD-HOC QUERIES
96/02/28
Liam supplied the following (broken) example.

dbQueryClause *theQuery, *theTerm, *theTemp;
if (isOr) theQuery = new dbQueryBinary(ItemType == itemType_Dummy);
else      theQuery = new dbQueryBinary(ItemType != itemType_Dummy);
while (stuff) {
        theTerm = new dbQueryBinaryCombo(
                          FileName  == ::SPtr(inText, thePos)
                   || Directory == ::SPtr(inText, thePos));
 
        if (isOr) theTemp = new dbQueryBinaryCombo(*theQuery || *theTerm);
        else      theTemp = new dbQueryBinaryCombo(*theQuery && *theTerm);
        delete theQuery;
        delete theTerm;
        theQuery = theTemp;
}
search(*theQuery);


The problem is the deletion of the terms, before they are processed by search(). Thus to solve this at least we need a dbQueryBinaryCombo that owns its parameters, and deletes them on deletion.

A cute way to do this is to overload the dbQueryClause operator| etc. to create a dbQueryBinaryOwningCombo when given pointers to queries, instead of references.

Fundamentally, we also need an easier way to accumulate searches.

{
   dbBuiltQuery theQuery;
   while (stuff) {
      theTerm = new dbQueryBinaryCombo(
         FileName  == ::SPtr(inText, thePos)
         || Directory == ::SPtr(inText, thePos)
      );
 
      if (isOr)
         theQuery |= theTerm;
      else
         theQuery &= theTerm;
   }
   blah.search(theQuery);
}

The above still runs into problems with temporaries - had to add a dbQuery class which handles built queries, and allows you to |= or &= terms.

Will later go back to Corneill Du Plessis' suggestion of stream-based builders.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
API FOR MULTIPLE FIELD SORTS
96/03/04
We need a clean way to specify a list of fields on which to sort.

Driving Forces:
- follow current idiom for lists of items (use << operator)
- allow user to build sort objects and hang on to them
- sort objects should be persistent, or easily made so.

Suggestion, allow a dbSorter to be constructed then passed in.

dbSorter phoneDirectorOrder;
phoneDirOrder << People.LastName << People.Salary.reverse();
People.setSortOrder(phoneDirectorOrder);

or, using a temporary
People.setSortOrder(dbSorter() << People.LastName << People.Salary.reverse());

Maybe even possible to abbreviate to
People.setSortOrder(People.LastName << People.Salary.reverse());

// implies a temporary dbSorter() which has a dbField constructor

Liam's suggestion was:
People.sortOrder << People.Lastname << People.OtherNames;
People.sortOrder.descendingSegment(1);


The main problem here is not knowing when you have finished appending fields, as opposed to setting a new sort order. An object needs to be constructed, and passed in as a parameter.

Note: my suggestion requires a reverse() operator adding to dbField. 

An alternative is to use the rightshift operator as well:
phoneDirOrder << People.LastName >> People.Salary;
If people don't like this shortcut, they can simply post-process:
phoneDirOrder.segment(1).reverse();

This latter syntax can be used for compound fields, and probably
factored out into a base class of OOF_SegmentOwners.

RELATED THOUGHTS
The current dbSorter needs to become a concrete class, for ease of
construction (as above). However, it may still be worth having the
abstract hierarchy. dbFieldSorter could be a useful simple subclass for
the common case. 

There's an obvious symmetry with the searches by compound fields - it
should be possible to set a sort order by a compound field or compound
expression. This way people can use the compound expressions most of the
time and just add a compound field if it is used for an allRecs()
situation. 

The sorter evaluation has to work out when an index can be used - it
makes more sense to embed that knowledge here than in the c-tree backend
at sort time. 

Thus, the sorter, when first triggered, will ask the backend for a
compatible index, and cache the resulting number.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
REFINING SEARCH EVALUATION
96/03/09
The current backend search parsing has the character-mode search
equal/not-equal evaluate the string for wildcards. 

We need to perform this check in the new dbQueryBinary::isSimpleIndexed
method in order to decide if the index can be used or not. 

It is a fair overhead to doubly scan the key for wildcards, and the
current level at which this occurs is counter to the other dispatching
logic. 

searchEqual is a bit of a special case, being called directly from
outside as well as within the backend search() despatcher. 

it seems a good time to provide a public searchEqual and a private
SearchEqual which is just the low-level search. The wildcard evaluation
can be moved out to where it is appropriate (we don't use wildcards when
searchEqual is used for a join). 

Deciding if a string contains a wildcard can become part of the string
literal object, rather than the backend, and so can be used and cached
if isSimpleIndexed is called.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
FIXING COPY-ON-WRITE SELECTIONS
96/03/24
Final testing for v1.1.1 (thanks to Liam's QM) revealed inconsistencies
in cloned selections. 

The cloning of a selection uses a standard copy-on-write
OOF_ctreeSelectionRep. 

The copy-on-write semantics are NOT implemented anywhere at present -
they occur for searches as a by-product of the way searches allocate
return values, and of the implementation of sub-searches as building a
new selection.

There are two cases of update-in-place for selections which are not currently handled:
1) add or delete records
2) sorting. (later realised current sort algo's don't sort in place)

ALTERNATIVES
1) for each update-in-place candidate, have a call to an explicit
function aboutToDirtySelection(). 

2) for every gateway into the selection rep, have a check to see if it's
multiply referred to and if so duplicate.

Alternative 2) has some huge performance implications, adding extra code
deep inside sorts for example. 

Alternative 1) has a danger that we will forget the call in some
location. That would be easily fixed, if noticed. It is certainly far
more efficient than 2) and easier to code. So be it.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
CALLING POST-CLONE CLEANUP
96/03/24
We have a clean structure for the user_table::clone() call which calls
postCloneCleanup(). This fixes things like getting the relationship
fields to point to the right table. 

However, if the user calls a simple copy ctor, this is not called.
Furthermore, we can't call it from inside the copy ctor as the fields
don't yet exist.

WRONG - SILLY SILLY ASSUMPTION
I was extending the rule for Base Classes (don't call derived members)
into assuming that members aren't complete at the time the body of the
constructor is executed. This is incorrect - postCloneCleanup can safely
be called from inside user copy ctors. *1 

COMPLICATIONS
Things are not this easy when it comes to actually generating the copy
ctor. We don't know our base class if we allow inheritance such as in
ooftst19. 

If we have a copy ctor that doesn't specify the base class with a
parameter then the default base class ctor will be called instead of its
copy ctor. 

The problem (for now, ignoring multiple inheritance) is how to specify
the base class name, without knowing it. We are trying HARD not to force
the user to specify it. 

Ref Design & Evolution of C++ p 292 it may be possible to use a typedef.

Thus, dbTable will
typedef dbTable directParent;

and then the macro ABSTRACT_BASE_METHODS will override by
typedef tb directParent;


*1
WHOOPS AGAIN!
Yes, it's OK to call postCloneCleanup from inside user ctors, but to create such a ctor we have to specify an initialization list of all fields, otherwise the field copy ctors aren't called.

Back to the alternatives - we *have* to use the default copy ctors for the user, so the fields are all copied. Thus we have to make it possible to use these in a public idiom - we can't declare them as private for the same reason.

It may be slightly inefficient, but a dbCloneCleanup dependant sounds the simplest way to go now. The inefficiency can be slightly bypassed by using a special dbTable copy flag sCloning.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
IMPLEMENTING M:N RELATIONSHIPS
96/05/15
Assume we have Q -> DQ -> D in a 1:N N:1 situation.

It may appear we can just use this triplet of files instead of a true M:N relationship. However, the second relationship traversal is just for the first DQ record (for a given related selection).

Fixing this for the general case may cause other surprises for users that expect to be able to iterate over DQ->D.

Thus, it makes more sense to implement true M:N relationships such as Q->D.

Because of the DECLARE_REF etc. macros it would be awkward to use a different class to implement these relationships, and the ODMG standard implies an M:N relationship just comes from two SET sides.


NEEDS
- maintain a middle cloned table as well as the rhs clone
- propagate relationship field updates through the middle table also
- perform a "full join" or relateSelection to go from the selection in the middle table to ALL its related rhs records

If we use a helper class to manage the extra info, then dbRelRefBase can maintain a pointer to the helper. The pointer being 0 indicates that this is NOT an MN relationship.

Eventually, the middle table will be auto-generated.
However, for now we can use a bit of a hack - the middle DQ table must be declared. This is also the approach that would still be used in databases like Kidmap where the middle table has attributes.

SUMMARY OF CHANGES

dbRelRefBase::BuildRelatedTable()
will be called on the lhs of the rel and needs to now clone the middle table as well as the rhs table. It also differs in that we have TWO dbRelRefBases on the same cloned table - the "fake" one used for the direct traversal and the other from the middle table.

There's some awkward timing here. The MN linker is responsible for building the related tables. However, in the process, it acquires some "real" pointers (as well as the references to prototypical fields). This info needs to be put in another MN linker which is passed onto the cloned rhs table.

The solution is to have a 

dbRelRefBase::relateFromRecord
for MN relationships, calls relateMN on the middle table

dbRelRefBase::relateSelection
needs fixing to relate all current selection to create single rhs sel.

dbRelRefBase::receiveMsg
doesn't need to propagate pointer info as the traversal path is never traversed to relate records.

Note: we don't need to do anything fancy to get the traversal path propagation working through the intermediate table as that's managed by the normal behaviour of the two traversal path members in that table.

In addition, the middle table is managed by a new helper
OOF_RelMN::relateMN
new method will call relateFromRecord for the lhs and then relateSelection on the rhs link.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
EXTERNAL HANDLING OF SELECTIONS
96/05/19
In relating a selection, and in some other circumstances, we need to accumulate a series of selections. This has in the past been handled by cloning a dbTable, but it makes more sense to have a lightweight operation which just allows us to clone the selection. (Note: Ken may have some backend stuff which will benefit from this.)

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
FIX PROPAGATION FOR MN RELATIONSHIPS
96/05/28
Initial implementation of MN relationships had a direct relationship from A->B as well as one through the intermediate link file L (ie: A->L->B).

The cloned table B' uses the A->B traversal path to monitor the status of the relationship. Internally, that traversal path then uses the A->L->B traversal path.

The remaining area which is not working is propagation of changes to the parent selection. Currently, this is propagated A->L and then L->B however as we are not directly acting on L-> the second propagation doesn't occur.

Thus, it seems that the A->B direct relationship will be the one which subscribes to A as a dependent, to affect the clone selection in B'.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
IMPLEMENTING A RAM-BASED TABLE
96/06/27

We have a distinct need for a RAM-based dbTable (ie: a RAM backend) in order to pass complex calculated results into a graph or report. 

Another need is to make the searches search the dirty cache of records, particularly so people can defer saving single records, for the c-tree backend.

There's a synergy here - if the current c-tree cache is made a little more abstract, and searchable, it will work as a RAM backend. It may be we can use the same cache for other backend types, reducing the work in writing new backends.

Things that need to happen
- abstract out the storage for records in the cache
- extend search to apply non-indexed searches to the cache
- make the non-indexed searches part of a common base class, pulling them out of OOF_ctreeTableBackend.
- write a RAM backend that essentially keeps all records in the dirty record cache.

DECLARING THE RAM-BASED TABLE
Three mechanisms make sense:
- mimic the oofField standalones using an oofTable base class which passes a ctor parameter to indicate the table is RAM-based

- a factory function for user-defined tables to enable a counterpart to be created in RAM

- a dbConnect_RAM class that can be used to wrap declarations

LATER REVIEW (96/07/11)
The oofTable descendent is not of huge benefit to users and has some implementation problems - it lacks a point at which we can close the table definition and build the backend.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
RAM-BASED BLOB STORAGE
96/07/10

In the c-tree backend, we use the OOF_recordBufferedContext::mBlobBodies to receive the blob bodies from each dbBlob individually, when putting entries in the cache (and overriding the current record with a cached version).

The pos field, in mBuffer, is used to track an on-disk position.

With the RAM backend we of course have no on-disk position to track. Thus, the pos field can be used to point to the actual blob storage. Each dbBlob has its own storage, so saving and loading will copy between these blocks in RAM.

mBlobBodies is therefore totally ignored.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
SEARCHES BY RELATED FIELDS
96/07/27
The core issue is how we translate a search by a related field, yielding a selection in that table, to one on the current table. ie: 
- we redirect the table being searched to be the one containing the field
- we relate back from the related table's resulting selection, to the table being searched.

A complicating factor is how we handle multiple terms. Imagine Patients->Visits->Drugs as a 3-level relationship:

Patients.search(
	Patients->Visits->Drugs->Name == "Blat" &&
	Patients->Visits->Date > "1/1/96"
);

Intuitively, this transforms to:

Patients->Visits->search(
	Drugs->Name == "Blat" &&
	Date > "1/1/96"
);
Patients.relateSelection(Patients->Visits);


which further transforms to:

Patients->Visits->Drugs->search(Name == "Blat");
Patients->Visits->relateSelection(Patients->Visits->Drugs);
Patients->Visits->searchSelection(Date > "1/1/96");
Patients.relateSelection(Patients->Visits);


This recursive transformation model suggests the latest possible evaluation would be desirable, and that evaluation be essentially transparent.

Alternatives that come to mind:

1) the operator== etc. members create a different dbQueryBinary type, which incorporates knowledge of the related file, if the field is on a related table. This has a certain elegance - the idea of using subclasses to vector the later handling is attractive (eg: dbQueryRelated::operator&&(dbQueryRelated)) however I have a deep suspicion that using inheritance here is not enough - the combinatorial logic depends on the exact relationship chains involved, not just the types of the tables on each side.

2) the dbTable::search() checks the field on the lhs of the expression(s) to see if it is related, and builds a subsearch expression (postprocessing). This could be very complicated - it amounts to double-parsing the search expressions. It also feels inappropriate - knowledge of deconstructing search expressions should be embedded in the backend.

3) the backend search() evaluator checks for related fields. (Preferred option from encapsulation reasons). 

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
IMPLEMENTING RELATED SEARCHES
96/07/30
Within the Query parser, ie: OOF_ctreeBackend::search(), we need to
- establish if the field is on a different table to that being searched
- if they ARE different, is there a relationship between them (is our search 
  table part of a traversal chain)
  - if not a relationship currently, are the tables related, so we can build a 
    traversal chain
    
With the reuse of the traversal chain, in the opposite direction, we save the overhead of cloning the related tables, but run into another problem.

The cloned rhs table, after the search, is unloaded. Iterative operations and count() check the unloaded status to decide if a related table should be traversed to (ie: search from the lhs). We need a mechanism to suppress this.

For possibly other reasons, but primarily to have a clear channel of controlling related table behaviour, it is desirable to explicitly mark tables as valid. Validity is thus independent of the table being loaded or not. Invalid tables can be used to provide more user feedback on database initialisation (ie: if they have forgotten to issue a newConnection).


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
IMPLEMENTING CLONING VIEWS
96/08/06
The desirable option is for views to (by default) clone the selection, as a shared iterator over that selection.

The semantics of shared selections allow two dbTables to iterate independently over the data. The actual selection is shared, being the same OOF_RecordSelection as present except that the iterator must (obviously) be pulled out into the OOF_simpleRecordBackend class.

(This may not be necessary. As OOF_RecordSelection is implemented as a handle-body class, the current semantics should work, allowing us to share selections and have the iterator stay in the handle as at present.)

A shared selection should NOT mean one table is dependent on the other. Nor, although it's a little counter-intuitive, should changes to the selection automatically invalidate a table. The assumption is that application logic will be responsible for these changes, in most cases via some GUI. The GUI classes are responsible for the linking of the different iterators - the dependencies in the GUI will cause refreshes and hence re-reads as appropriate. If we have a dependency from the shared selection to the iterators, a double-update occurs. 

In a non-GUI context, there is no way for the dbTable to know the destination of its data - if fields have been copied into local variables etc. Thus, forcing the dbTable to be updated in some manner to match the shared selection update has no meaning.

Using a second param on the dbView ctor (...cloneSelection=true) allows us to preserve the current semantics of views but give the user explicit non-cloning if desired for performance reasons.

With regard to the members of the view, there are two possibilities:
1) fields are appended as field numbers alone and translated into actual fields at runtime.

2) when fields are appended, if cloning the source, each field is asked to supply its counterpart based on the cloned source. This allows us to hide navigation to related fields.

For performance reasons, 1) takes a large hit when using the members of the view, and there seems little reason to delay translation.




-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
SHARED SELECTIONS
96/08/07

There is a minor complication when sharing selections - we already have copy-on-write behaviour and want to keep that. This COW only applies to the selection body that keeps a set of record numbers. Thus, the current handle-body pair of classes will become a handle-handle2-body where the 2nd layer is sometimes shared and independently ref counted of the body.

An initial idea was to simply set an mShared flag in this handle2 so that every clone after would share the selection. 

Further thought shows this is invalid - a possible scenario:

dbTable search gives selection of some recs - have body of rec offsets
dbView created (clones sharing selection)
dbTable cloned for other use - should be copy on write sharing of body

Thus, we have to explicitly pass the copying behaviour down as a tri-state instead of the bool selectionNotCopied.

The default semantics will be copy on write, only if explicitly instructed does copying a selection share it. This preserves the use of dbSelection to temporarily save a selection and then restore it.

It also means there is no need for an OOF_recordSelection to know if it is shared or not. Sharing is a by-product of copying and once the handle-handle2-body is established the behaviour is the same regardless.



-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
SPECIFYING dBase REAL FORMATS
96/08/20

The default for the dBase backend is to specify dBase numeric formats to match the maximum size of the relevant OOFILE numeric field. In the case of dbReal this is very large.

Thus, for people who want only a subset of the numeric range, we need a mechanism to specify dBase number formats.

It would be preferable if this mechanism was NOT in any way specific to dBase backends, but used something generic at the dbField level, without having to extend the abstract dbField interface.

An elegant solution is to use the output filters. If the user attaches a formatting mask to a numeric field BEFORE the database is constructed, the format (eg: "###,##9.99") will be used to work out the size and decimals of the dBase field.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
KEYWORD INDEXING ISSUES
96/09/15

(see also 95/05/29 FILTERED AND METHOD INDEXES )

There are two models under consideration for key storage - using a
related file and using low-level c-tree keys (bars us from UNIFRMAT
files). 

For now, assume we'll use a separate related ISAM file and use
relateSelection to go from the found keys to the destination. (This
works and optimisation with pointer relationships will make it
reasonably fast.) 

Previously discussed is the issue of pools of keys, so a pool can be
searched by keyword or an individual field. For now, we'll make do with
field-level indexing and possibly searching at a file level. 

Possible encoding in ISAM keyword files:
1) each word entry is composed with a separator character and field
number attached. Thus, a startsWith(word+separator) is a search on the
entire file, regardless of field. 

2) word and field number are separate fields. A compound index (and
search) is used when searching at field level, and will be reused for
table-level matches 


Option 2) also allows us to searchSelection on the field number alone
(which might be separately indexed) to find all words belonging to a
field, once we have the words related to a table. 

Using a parent-child relationship, the keywords file will automatically
be cleaned when we delete parent records.


-----

Attaching keywords to dbTables, and building indexes:
How does the backend know that it should build an extra index (or table)
for keyworded files? How does the dbTable know to calculate keywords
when saving?

There are a few possibilities. The initial decider - does the dbTable
generate extra dbTables or does the c-tree backend do it? 

Backend doing it: lets us hide the fact that low-level keys are being
used or ISAM file. 

dbTable doing it: can maybe fake-out by just adding tables and retaining
high-level relationships. 

Another argument for hiding wholely in the backend - that's the level
that parses the searches. We can implement in the c-tree backend for
now, and use more abstract logic up in the record-backend later. 

At buildSchema time, the backend can construct a keyword index object,
that contains a list of the field numbers it covers. This can then be
invoked for indexed searches, saves and deletes. The code it uses may be
a copy of the relationship traversal logic but there's not a convincing
reason for it to be visible at the table level as a related table.


-----

The remaining issue to consider is what happens when the field is edited.

Possible models:
1) all words are loaded into memory, with some being deleted and others added

2) all records are deleted then rewritten


Model 2 is easiest to do for now. However this will incur a significant
performance hit. At the least, for records with many keyworded fields,
we should consider tracking dirty status field-by-field.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/09/19
ADD CALCULATED FIELDS

Initial reasoning was that calculated fields are handled solely at the
leaf field level, eg: dbLong. This allows use of a specific field type. 

However, this has led to a massive amount of duplicated work as the
field variable must be updated in every constructor, destructed etc. 

An alternative worth considering is having dbField contain mCalculator
and use downcasting in the leaf fields. If mCalculator can only be set
by leaf field calculateWith() then this is safe - the initial release
will use this approach.

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/11/02
AVOIDING RE-KWEYWORDING

When you have 1:1 relationships with pointer relationships you need to
save one side of a new record twice.

A - saved, now has address on disk
B - copies A's address, saved, now has address on disk, updates A with same
A - saved again so it saves B's address

We need to avoid the second save of A which is currently deleting all
the keywords just added.

The possibilities are some kind of state indicator, or an intelligent
handling of field updates. 

Due to the high-level (mark table as dirty) approach to indicate that a
relationship pointer has been written to A, a state indicator would
violate a lot of encapsulation. 

Using intelligent handling of field updates, we can check if keyword
fields have been updated and avoid reindexing them under normal
circumstances. The field dirty approach can also be used with
relationship pointers, so if we modify a record but don't update the
pointer there's no unnecessary second save of A at all. 


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/11/02
STATUS AFTER SEARCHES
An interesting bug has been found if you do a search then the only
activity is on a related table. As the search does not actually load a
record, the related table has nothing from which to relate. 

The major question which needs deeper analysis is should searches load a
record? So far, we have lazy evaluation which only loads records when
fields are accessed. It seems reasonable to extend this to
relationships, thus relateFromRecord() should call ensureRecordLoaded().

With closer examination, what is going wrong is that OID() is
effectively a field read but unlike most fails to call
ensureRecordLoaded. A general search for similar cases found a few other
minor bugs.

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/11/10
APPENDING TO dbText

Liam reported append() not working on dbText - turns out operator+=
compensates for text string, but dbBLOB::append() not designed to be
used on dbText and ignores trailing null.

Possibilities:
- keep overloading dbBLOB routines with dbText versions that compensate
- enhance dbBLOB routines to preserve a trailing null regardless
- use protected inheritance to avoid dbText users mistakenly using dbBLOB methods
- add explicit appendChars for dbText use

Decision:
- for now, just add virtual version of append



-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/11/12
MULTI-KEYWORD SEARCHES

Parsing of the top level of these searches is easiest implemented as an
extension of the current approach:
- add members to OOF_MixKeywordableField to build the queries
- add query operation types for the new ops
- add a query literal type for the rhs, a multi-value string

Actually performing the search should be optimised to minimise the
amount of scanning over the keyword file. 

These searches won't be implemented for files lacking keyword indices,
although they could be a variation of searchSelContainsAllDelimited. 

The behaviour depends strongly on the All or Any nature. 

The Any case is simple - keep searching the keyword file for each
keyword, then relate the entire built selection. This is just an
expansion of the normal keyword logic. A non-indexed version will do a
single pass, like searchSelContainsAllDelimited. 

The only straightforward way for the All case is to perform a set of
keyword searches, and intersect the results. We are intersecting the
related target file, not the keyword selections. A later version might
optimise this by choosing keywords to search in order of their
frequency, and using a searchSelection once the target set had fallen
below a certain amount.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/11/13
RETAINING FILE PATHS

There are two bugs with the current software:
1) if you open a database in a subdirectory, that directory becomes the
current dir and the next open is relative to that location.

2) database rebuilds assume the database is in the current folder.

The fixes are:
a) after opening a database, restore the original path
b) when rebuilding, set the path to the database location and restore after

This implies:
a) we retain the opening location, OR
   we can always return to the current location (allows for change)
b) we retain the database location


For portability reasons it is best to abstract the database location -
the need to save and restore current directories is not a cross-platform
activity and the values are not persistent, we just need an abstract
interface that can be implemented differently per-platform. For the Mac,
an FSSpec is best, where an absolute path is probably the most common
solution. 

(Check if Unix inodes can be used directly and if NTFS has an equivalent
to the Mac FSSpec). 

An oofDirectory would be used by the dbConnection at a fairly high level
then, to record the database directory at the time of connection or
opening. A stack class can be used to wrap all operations which traverse
database paths. 

Mac note - research (MoreFilesExtras) shows that it *is* safe to use
HGetVol and HSetVol provided we only do so temporarily. Now that we are
returning to the normal directory after opens, new's and rebuilds, we
can use these simpler calls instead of Working Directory numbers. 

LATER - clarified the above assumptions. c-tree does NOT use ANSI file
calls and so requires us to use Working Directory numbers. We will have
to create these in the situation where we open the database from a
pathname. 

Furthermore, for a new database, the logic is to create folders as
relevant (Set_Path function).

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/11/26
PERSISTENT RAM STORAGE - STREAM IN AND OUT

Normally,the dbConnect class acts as a factory for backends for the
dbTables, and all storage I/O is handled in the OOF_tableBackend
classes. 

In this case we have two levels of I/O. There is the mock I/O that takes
place in the normal RAM backend, where saveRecord, newRecord etc.
actually interact with the record cache. The second level is the
persistence of the entire database to a non-portable file on disk. 

This second level of persistence is a dbConnect-level activity so it
makes sense that dbConnect_ramp would handle it. However, the mechanism
of actually transferring the record caches to and from disk should be at
the backend level. 

The first level of I/O uses up the dbTable's abstract save/new interface
so it seems easiest to have the dbConnect_ramp have a direct list of
OOF_rampBackend classes to do the initial load and save.

HOWEVER
Realised there's a bug in Ken's dbase backend - he's not closing the
files. Thus, we do seem to need a general close() that's visible through
to dbTable level and can use that for storing data. The current
openTableInConnection can be used to load a table.

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/11/29
USING VIEWS WITHOUT TABLES
Particularly in reports, it would be convenient to put together a dbView
from a number of odd fields. This only needs to iterate once. 

Passing in 0 as a dbTable* could be an indicator of this, and the
internal behaviour changed to iterate once for mTable==0. 

The biggest danger is people using dbView::source() or table() to get at
the table, to attempt iteration. These accessors need an assertion check
at least to cope with the null table pointer.


=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/12/11
CLOSING FILES

The standard way of declaring a database, which satisfies the attachment
of dbTables to dbConnect_xxx is to create a dbConnect first then the
dbTable variables. 

This results in a destruction order of the opposite - the tables are
destroyed first. 

Thus, if dbConnect_ramp's dtor closes the database, it is too late as
the tables are already gone. 

Possible solutions:
1) have the 'prototypical' backends owned by the dbConnect_ramp and
marked in some manner so they are not deleted by the dbTable dtor.
dbConnect_ramp::close() can then iterate over the owned backends. This
requires a COW type ref count in the backends, as the dbTable logic to
delete its backend should not be affected by the needs of a single
connection. 

2) in the dbTable dtor, check to work out if it's the prototypical
dbTable, and call mConnection->close(). There's a minor redundancy, but
given the point at which this occurs it seems worth it.

DECISION:
- implement 2)

LATER
- realised problem with Ram backend which allows connection to be
deleted before tables. Will work around with special case for this one
and enforce connection deleted last for all others.


=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/12/13
NESTED SORT EFFECTS AND CLONING
When we sort a table, there's an unfortunate propagation of the sorter
to the temporary table used on the rhs. 

We use cloneWithoutSelection to create a temporary table for comparison
purposes, which of course has a cloned sorter. As sorting is regarded as
a setting, rather than command, it should normally propagate across this
kind of activity. However, in this case it is redundant and imposes an
unnecessary overhead. 

Copying of the sorter is accomplished through dbSorter::clone(). It
occurs that we can make this intelligent, without making the upper
levels aware of what's going on. The ONLY time a sorter will be cloned,
and mIsSorting is true, is if the sorter is being cloned for a table
clone. Thus, clone can return 0 in this circumstance.

=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/12/16
BLOB I/O WITH PERSISTENT RAM BACKEND
We want to ultimately have some kind of lazy I/O for blobs, so they are
not read unless necessary. Loading a persistent RAM database should not
always impose a huge RAM requirement. 

However, any delayed loading of blobs massively complicates the writing
of the database as we can't just start writing from the beginning of the
file and stream out all tables.

For now, the approach is to just store the BLOBS sequentially after the
record. We will read them in just using the lengths from the record
buffer.


=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/12/17
FIX RAM BACKENDS TO COPE WITH CACHING DIRTY RECORDS

The original idea behind the RAM backend was a simple backend that would
tend to be used for a single table. With its expansion into the
persistent RAM backend, this is not the case. 

The quick decision to re-use the dirty record cache as a holder for all
records therefore doesn't work - we want the normal
OOF_simpleRecordBackend behaviour that expects the dirty cache, eg: for
related records (see ooftst02). 

Thus, whilst the cache class can be used, we need a separate cache for
the RAM backend to contain all its records. This also frees us up to
refine or replace this cache class later, eg: to provide compression of
the records. 



=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/12/18
FIX SORT BUG WITH SHARED SELECTIONS

The bug is exhibited in a simple case by the ooftst07 test routines,
where a dbView is created then the sort order on the original table
changed. 

The dbView is created without a false 2nd parameter, so is using a
cloned, shared selection to iterate the dbTable without disturbing its
current record. 

HOWEVER the dbSorter attached to that shared selection is listening to
messages from only one dbTable - the original. It is therefore not
triggered by the start() issued on the cloned dbTable. 

The problem also manifests in GUI listboxes such as the KIDMAP
application which make heavy use of dbViews in this manner. 

CONSTRAINTS
We can't just clone the sorter when we clone the shared selection, as
the sorter will then replace the original in the selection. There must
be in some manner multiple sources of messages, with a single reactor. 

SOLUTIONS
1) Some forwarding mechanism, so when a dbTable sends messages, the
shared selection retriggers them in the cloned, shared dbTables 

2) Specifically, for dbSorters, when the sorter on the selection is
changed, make that sorter subscribe to multiple dbTables.


	
=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/12/28
FIX BUG WITH SINGLE SEARCHES RETURNING WRONG (PREVIOUSLY LOADED) RECORD

There is some inconsistency in the behaviour of the ctree backend in 2 cases:
1) searchEqual on an indexed key with no duplicates - GetRecord is used
which loads the record. 

2) searches resulting in a selection of one record - LoadRecordAtOffset
is used, resulting again in the single record being loaded. 

This goes against the principle of avoiding loading records until
needed, and works directly against searches as part of the relationship
traversal, which often only need to get the record offset (pointer
relationships). 

Decision - remove these inconsistencies.


	
=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
96/12/29
FIX BUG WITH SORT TRIGGERING & RESTORING SELECTIONS
There are two problems with sorting:
1) sorts are triggered by state changes in the dbTable to which they
listen, despite the sorter actually being owned by the relevant
OOF_recordSelection. If we clone a table with a shared selection, sorts
aren't triggered by the cloned table as the sorter is only listening to
the original. 

2) When restoring a selection with a stSaveSelection dtor or other call
to dbTable::setSelection(), an OOF_ChangeSelection message is broadcast
which triggers a resort. This is often unneeded as the selection may
currently be sorted. (A deeper problem may be failure to recognise a
currently sorted condition.) In fact, the selection has NOT changed, but
*which* selection is being used HAS changed. This affects users of the
table, eg: GUI listboxes, but shouldn't trigger a resort.

IDEAS
a) Make sorters listen to the selection itself. Thus the dual master
problem in 1) vanishes and the resorting problem will not be triggered
as the selection won't have changed.

b) fix the dual-master problem directly and fix the sort triggering problem by 
suspending sorting so the sorter never hears the message to say that the
selection has changed.


NOTE:
Sort order should be copied with a selection, so a sorter could be
copied with a selection, and restored with it.

97/02/02
Refinement - sorters no longer need to be listeners if they are wholly
owned by a selection, copied with the selection etc.

So, under what conditions is a resort triggered?
- selection wholly changed (relationship traversal or search)
- add records to selection
- save records (regardless of whether the selection is in use)

Note: that a setSelection does not count if the selection being set is
already sorted!!!!

97/03/05
Ken pointed out there's a weakness here if we delete records that are in
the selection. He also queried if we want a "dirty records" map to avoid
resorts if we haven't dirtied records in the selection. Probably not as
the effort in checking each time might be outweighed by the ease of
sorting - we have to scan the selection to see if it contains the dirty
records. Maybe later it could be a lazy evaluation thing.
	
=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/01/04
HOW TO DECIDE TO PRINT HEADINGS

At first blush, this appears to be a report-writer issue - we want to
specify that a given view band prints headings or not. Indeed, a global
report-writer setting is a good idea. 

However, for individual view bands, there seems a synergy that because
the headings are set on a view, so should their showing. This lends
itself to the wider view that view headings could appear on a listbox. 

We want to mimic the behaviour of the report inherited settings, so a
given view can override the report setting to show or hide headings.



	
=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/02/09
RICK'S DATE-TIME CLASSES
(written by junior programmer, hence the lack of discussion of for and
against, Grrr!)

dbTime doesn't allow addition and subtraction.

dbDateTime allows floating point addition and subtraction in values of days.

dbDate and dbFullDateTime allow addition/subtraction of dbTime

dbTime contains days.

dbTime is in numbers of milliseconds (signed) making ~ 24 days in each direction.

dbTIme wraps around the time by zeroing the number of days.
ie  24 days 3pm + 1 day = 0 days 3pm.

dbDateTime is an unsigned 32 bit long indicating a total number of seconds since
Jan 1, 1970 00:00:00. (LATER - make the start date user-configurable)

dbTime ++ / -- / += / -= are all in milliseconds.

dbFullDateTime will include a timezone number in half hour segments from
GMT. It was decided that dbTime and dbDateTime didn't have the available
bit space to include sure information, and that there wasn't the demand
for it in the usual case.

	
=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/02/13
HANDLING DUPLICATES
We've had an urgent need to handle duplicate records here by simply
ignoring them, as a part of an import process. This arose in KIDMAP
where a complex navigation down the possibly cross-linked curriculum can
result in adding a bottom level item several times.

Ignoring duplicates seems a good option to include as part of adding exceptions,
really as a simple alternative to throwing an exception. 

- Duplicate record handling is triggered by having one or more fields with an
index type such as kIndexNoDups. It is currently only available with the
c-tree Plus backend.

- If dealing with a single object that has only fixed-length fields there is
no difference with duplicate handling.

If the object has dbBLOB fields (includes dbText) then the main object may
bewritten twice. The logic is:
	if table has blobs that need saving
	   if table has noDups fields that need saving
	      write main record
	write blobs
	write main record

The reasoning behind the double write is to avoid orphan BLOB records.
If we update the blobs first (to get new blob direct addresses) then
fail to write the main record we have inconsistent blobs to remove. 

	
=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/02/14
ADDING RELATED RECORDS VIA RELATED SELECTIONS
In GUI's such as KIDMAP and the Contacts2 example, we have a "linking"
interface with an MN relationship that of course uses an intermediate
table. eg:
People -> AssociationWith -> Organisation.

In the GUI interface, we append records to the selection of Organisation
(from a pool of all organisations) to show how people are associated. 

The set of related Organisations for an individual People object is
displayed, and we want to append our new set (usually 1) of
Organisations. This implies adding link records. 

There is another scenario where the target selection is a subselection
directly on an existing table. eg: a "working area" used to display a
subset, not implying any linking to the main table.

DECIDING WHEN WE'RE ADDING LINKS
One issue is to decide if we can *always* decide if the appendSelection
will be effectively adding link records, or if it is just adding oids to
a selection. If the decision is transparent, we must have a guaranteed
way to determine when adding/deleting links vs updating a selection. 

A further complication is the level of isolation - the user code adding
the selection, or deleting records, is dealing with GUI integration
classes rather than a method at dbTable level. 

Using the principle of separating interface from model, it would be nice
if interface code remains the same, ie: using appendSelection. Maybe we
can have a state change on the dbView's table, something like:
theView->table()->changeLinksToMatchSelection(); 

Later - it suddenly struck me that this can be handled through the
dbTable as broadcaster model. The table need not know that someone is
attached however it does require a couple of new messages -
appendSelection and removeSelection.


DETECTING CHANGES IN THE SELECTION
Assuming we know if we're adding links, we have two choices:
1) compare selections at save time, adding and deleting as necessary
2) at the time of changing the GUI, record the additions and deletions. 

Option 1) could be a lot of work if we have a very large selection with
only minor change. It also implies a performance hit on every single
selection (alleviated with a single dirty flag). 

Option 2) can take advantage of the fact that we have to establish the
differences in the selections anyway to avoid adding an oid that already
exists. 



97/02/18
Further thinking on the way to handle broadcasts, how to set them etc.

If we have a relationship listening to the rhs of a selection this may
be an overhead the user doesn't want all the time. Therefore it should
be easy to set close to the specific GUI interface. 

We don't want this tied to the GUI. The GUI classes should call a
dbTable or dbView method to do most of the work. Similarly, the setup of
the feature should not require the GUI classes. 

We need to separate out the current setSelection and selectNone et al
methods from ones which broadcast updates. It is ok to change the
meaning of appendSelection slightly to include broadcasts. I think we
need to add removeSelection(const dbSelection&) and clearSelection
broadcasting methods so there are a set matching the GUI actions. 

There is slight potential for confusion between clearSelection and
selectNone but the confusion is safe - if people use the former they are
incurring the slight overhead of a message broadcast, no other
side-effect.


Creating the link records: If this is made the job of the OOF_RelMN (the
actual MN relationship traverser), it knows all the relevant objects.
This makes more sense than the link maintainer having to perform the
work. The link maintainer should be a forwarder of a message. This
implies that the link maintainer (dbRelMaintainer) knows the MN
relationship.

How to get join value or pointers from rhs:
- OOF_RelMN knows the rhs relationship which is actually used for the
traversal. Therefore it can copy the value or OID.

- it will do a stSaveSelection on the rhs and iterate it with passed in
selection that is being linked.

- deletions imply we are finding which of the intermediate table's
records correspond to the rhs selection we're deleting. It is most
likely that the rhs selection will be smaller than the intermediate
selection (we are deleting a portion of the relationship) therefore for
now easiest to iterate the intermediate table and find matches. With a
pointer relationship, the intermediate table will contain the OID values
from the rhs set and so the process is very efficient. With a join
relationship, we will have to join from the intermediate record to the
rhs to check that the rhs oid matches that being unlinked.


LATER ISSUES
- how to broadcast removal of selection - before or after we actually remove?
- a clonable listener that can be set at schema definition time.
- what if records being related are new and so have synthetic oids or
lack a join value? Do we just disallow this option?


=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/03/08
DEFAULT CALCULATORS
We want an easy way to attach a calculator to a field, eg: for a derived
ID, that will only be used as long as the field is empty - a user
editing the value should make the field behave as normal. 

A naive implementation is to specify an additional flag (eg:
mCalculateOnlyDefaults) for dbField, with suitable accessors. 

We need some finer rules for when the calculator is used. 

Use of the calculator should be transparent to users of the field. They
can determine if a calculator is in use, but the normal getters should
decide to use the calculator without external influence. ie:
dbChar::operator const char* may return a calculated value or stored
value. 

If we view the calculator as used to supply initial defaults, and allow
the user to edit a saved record to nil, we are only concerned about
using the calculator whilst the field is new and empty. However the
process of checking if empty can lead into endless recursion - we end up
testing the calculator to see if we should use the calculator... 

If the rule is, use the default calculator whilst the table is new and
field not dirty, that can be tested safely and quickly.

97/03/09 - saving records
To optimise the checking whilst saving records, we want to avoid looping
all fields to see if any are default calculators, for tables that lack
any. Whilst thinking about this issue it becomes obvious you can attach
a default calculator at any time, unlike a normal calculator. 

Thus, we can't rely on a build-time setting of a flag, but maybe
calculateWith can set a flag in the backend to say it has default
calculators. We could extend this to a bool set in the field map
indicating that the field maybe using a default calculator, but probably
not worth the optimisation. 

Given a flag in the backend, we then need to iterate all fields asking
them to save any default values. This is starting to feel like a LOT of
work, and internal coupling of the backend & field classes.

There's also a weakness in that the field dirtying logic has not been
triggered if no other fields are set. This pushes even further to the
idea that maybe the field default calculator should be wholly
transparent to the backend - the default calculator when invoked sets
the field's internal value and dirties the field. 

This leaves us with only the problem of determining how to keep on
invoking the default calculator. Somehow it needs to know that whilst
the field is dirty, it has not been set by other than the default
calculator. 

Note: we can't rely on comparison - the original default calculated
value may differ from the value that would be calculated at a later
date.

The states now appear to be:
- field not dirty 
- field has been dirtied by calculator
- field has been dirtied by other source

To outside observers, the latter two states appear the same. It is only
the calculator that knows the difference. 

At present field dirty status is tracked in the backend but could easily
become a method of dbField. The isDirty() method could then include a
usingDefaultCalculator() which takes care of the latter two states. 

The final question is, if a default calculator is never read from, what
is the value of the field?

There may be two subcases:
- a new record is created and no field values set. Until the user
dirties some field, the record is not dirty. Therefore it can't be
dirtied by just the presence of a default calculator.
- The record is dirtied, but a default calculator is never read. At the
field level, should a default calculator be regarded as ALWAYS setting
its value?
One way to answer this is to consider alternatives - such an algorithm
could be specified in an overloaded saveRecord. 

I think this leads us to the semantics that default calculators exist to
supply values - once read they will set a value, but until the record
has been dirtied by another field, the calculators won't dirty it
themselves. 

This brings us back to the earlier approach - a backend knows if there
are default calculators and invokes a special loop through the fields to
use them when saving. 



-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/04/06
TEMPORARY FIX FOR SHARED SELECTION INTERACTION

The core of the problem is that OOFILE at a low level is mimicking the
behaviour of an OODBMS - there is not one copy of an object in memory,
but one for each dbTable iterator, if you end up with the same current
object. 

This will be fixed soon for other reasons, but a temporary fix is
urgently needed to cope with GUI behaviour in KIDMAP, the Contacts
samples and other GUI users. The problem stems from adding a new record
- the dbTable used by the editHelper has the new record and the shared
selection has that new record's synthetic OID. However, the cloned
dbTable used by any dbViews being used by browsers doesn't have access
to the new record. The selection has a synthetic OID that isn't
available to that dbTable and so it tries to load a (crap) database
record. As c-tree allows you to load a database record from any
location, loading from a disk address like 1 will get you a largely null
record from the header portion of the database. 

The proposed fix is to avoid adding the synthetic OID to the selection,
until we save the record. Thus, the shared selection won't grow and the
browsers won't try to display the new record. The corollary is that the
browsers should avoid having ANY record highlighted whilst editing a new
record, so the visual feedback doesn't confuse the user.



-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/04/09
REFINE TEMPORARY FIX FOR SHARED SELECTIONS (DON'T ADD NEW RECORDS TO SELECTION)

The above fix burnt us badly when using related data that gets iterated.
For example, a Student's Notes in KIDMAP, or Patient Visits in ooftst02.

If we create a new related record, the related table is of course in a
requireExplicitAndBuffer save mode. The new record is retained in our
dirty record cache but as it is not in the selection it is skipped when
we iterate over the selection, saving all dirty records. 

There are several approaches which could be taken to solve this:
- ultimately, put new records into selections and have them globally
accessible, as referred to above as the preferred solution 

2) for now, have the new records added to the selection if the table is
in a requireExplicitAndBuffer mode. This has complications - how does
the backend know that we are in this state? 

3) some kind of gross hack which has the effect of 2) without requiring
the backend to ask its table for the save mode. 

There is a single entry point in OOF_simpleRecordBackend where we change
contexts for the buffered mode. On first inspection it looked like we
could use this to add new records to the selection, however there is not
a counterpart at save time that is unique to the mode. We would then
have the problem at save time of some new records being in the selection
and some not!

The cleanest short-term solution seems to be some variant of 2) - we
need to know we are in the mode so the new record can be added. The
selection-level appendNewRecord etc.have no knowledge of the table, thus
they will need a flag passing in. The backend classes that call them
*do* have a pointer back to the table, so can check the state. For now,
the performance hit (possibly slight) on repeated calls back to the
table will be accepted.



-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/04/10
UNLOADING RECORDS AND ITERATION THROUGH SELECTIONS
In the context of related records, there is an apparent weakness in the
way that we handle iteration through a selection. 

Consider a classic for (start(); more(); next()) loop, or a for (i=0;
i<numRecs; i++,next()). 

The final next() will be executed after processing the last record in
the selection. 

This should be side-effect free. Whilst we don't necessarily expect a
sensible record to be loaded after iterating to the end of a selection,
there should be no harm to the records. 

Unfortunately, the current backend calls unloadRecord when we iterate
off the end of the selection. We also call unloadRecord in a number of
other contexts, eg: after a search. 

UnloadRecord is currently very harmful if we are in a mode where we
buffer dirty records. It deletes any unsaved new records and destroys
the dirty cache. 

There is a situation where this destructive behaviour is desired - when
the parent record has moved off its current record and so the related
selections are now invalid.

There is arguably a similar requirement for an external call to
unloadRecord - if the user wants to force an unload. The dbEditHelper's
handling of OOFmsg_RevertRecord is an example of this. 

Can the existing unloadRecord be made smart enough to guarantee it only
behaves destructively in the right circumstances, or do we need a flag
or other function? 

For now, lets try the side-effects of not unloadingRecord after
iteration. 

94/04/28 Found one side effect - we end up adding the last record to the
dirty record cache repeatedly, if it was dirty, on a related selection.
Decided to reinstate the unloadRecord behaviour and have explicit call
for related records to unload the entire selection.



-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/04/18
DEBUGGING DISPLAY OF RELATED RECORDS
KIDMAP - Strands screen has problem with adding related Outcomes, which
is by an appendSelection on an MN-related selection. 

The bug is due to having multiple browsers (on different panes) on the
same related selection. Each browser has created a dbRelMaintainer
listening to the related table, and so it attempts to add an MN link
record twice. 

Possible solutions:
1) simplistically, change the broadcast of OOFmsg_AppendSelection to an
askAllStoppingAtFirst style of broadcast. This leaves the extra listener
in place but ignores it. 

2) Don't create the extra listener in the first place - work out that we
have two views on the same related table. 

Similarly, on the Student Info screen we get an error when saving new or
deleting if we have any Essential Outcomes (dbPDE* PQD). This is not due
to the above multiple browsers, but due to a bug in the way we create
dbRelMaintainer - there is nothing deleting them until the relationship
disappears at the end of the application. 

A solution to this would be use solution 2) above - have some kind of
factory function on the relationship that creates the maintainer. 

		
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/05/07
LOCKING
We need some easy way for applications to lock at record level. This
needs to be coupled with throwing exceptions for lock failures. 

Issues: 
- stack class to lock a record - default mode where dbEditHelper
auto-locks as you enter records (later make it only start pessimistic
locking when attempt edits)

- how to handle the enterWriteLocking used in
OOF_ctreeBackend::SaveContext - needs to avoid doing this.

- (#ifdef'd?) remove the call to resetLocks in unloadRecord 

- make WITH_LOCK_WAIT compile out to throw an exception if fail to get record

Looking at the usage patterns in KIDMAP, we need to make it very easy
for an application to iterate a database, within an editing context, eg:
running a report whilst in an Edit screen. 

One model that seems viable is to leave locking/unlocking behaviour out
of the core database as much as possible - let the application decide.
This in particular works well if the framework interface classes
automate locking decisions. Thus, we need to lock a base record when
editing commences, and rely on that lock to protect all the related
records. IF the application plays fast and loose by directly editing
intermediary data (eg: create link records in an MN relationship) then
it needs to manage the locking of these records and react to lock
collisions. 

This gives us a very low-overhead model for the initial core database
locking, with the responsibility of making sure that the automatic locks
that C-tree places on an AddRecord are freed where necessary. 

The biggest problem with automatic locking/unlocking in the core
database is probably inadvertently removing a lock!

OTHER MODELS FOR FUTURE CONSIDERATION
Client-server using transactions for rollbacks, optimistic catching
record locked exceptions. 

Traditional optimistic model - tries to get all locks before writing,
just iterate over all nested records and acquire locks - classic 2-phase
logic. 

Traditional pessimistic would use ISAM locking mode. Assume application
responsibility to suspend locks, eg: if run report whilst editing. How
know the termination point at which we release the lock? 

SRB to the rescue - we know that we have finished with the record and so
can unload it.

	
		
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/05/14
DELETING LINK RECORDS

Taking the specific example of KIDMAP - say we've removed a couple of
Strands from the rhs of a Student's Enrolled Strands.

We want an efficient way to get the subset of link records.

1) Search and Intersect
   - relate selection back from the rhs - gives us a link set of all links to
     the rhs Strands
   
   - search from lhs - gives all link records for Student
   
   - intersect the two, could easily be intersecting a thousand recs or more
     against an even larger set



2) Optimised Search
(may use heuristic - check database config to decide to do this)
  - given a link table with compound index on join fields to both sides
  - search on unique join combinations, one search per selected Strand


3) Search related selection
   - search from lhs - gives all link records for Student
   - subsearch for the join fields from the rhs set. Would result in iterating
     through a thousand records!!!!
     
     
Where possible, 2) is obviously faster and much simpler than the
alternatives. 1) is a better fallback, particularly in multi-user
environments, to avoid loading too many records. For now we'll just try
1) given the hassle of working out which combination of fields to use
etc. for the compound key. 


	
		
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/05/15
SORTING SELECTION WITH COMPOUND FIELDS

Initially thought a simple approach would be to treat as a character
sort. However, the character retrieval in compound fields is very
inefficient if called repeatedly, using extract() and streams. 

Second rather elegant idea is to make use of the common parent
OOF_FieldSegOwner for both dbSorter and dbCompoundField. A sorter can be
constructed very easily and a recursive call to sort used.
	
		
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
- 97/05/20 STRUCTURED BLOBS 
This would be a good time to implement variable-length records, however
if we use a Structured Blob approach we can use the current external
Blobs and move them inline later for
performance.

Ideally, through the backend it is a nested object:
- we have a SB which has X number of fields setup much like a dbView.
- a given SB has a field map up the front
- the normal class declaration has nothing to do with it - it's a runtime attachment.

How does GetFieldWriteOffset etc. map through to the SB? 
- I'm leaning toward using functions GetData, StoreData rather than getting addresses.
- can be intercepted more neatly by SB 
  - actual backend pointed to is an SB which then forwards (feels awkward) 
  - OOF_TableBackend descends from common storage class, which SB also descends 
    from, so SB maps this onto some kind of allocation within the blob. (This 
    leads us onto being able to do streamed blobs in future.) 
- would be nice to get Liam's stuff rolled in but doubt it is going to be possible 
within the timeframe

For now make the assumption that we don't need to do indexed searches on those fields.

Will need to flag the endian-ness of the SB, to convert on the fly as we
read on other platforms.
	
		
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/06/06
INVOKING USER METHODS IN ITERATIVE SAVES

An interesting problem arose with the dbTable::saveRecord loop.

The method loops, calling the backend save, in case of the record being
dirtied or being new. This can happen for two reasons:
- a relationship updated the record after saving (ie:lhs of a pointer relationship - auto-saved rhs which updated the pointer in the lhs)
- we are saving a cached set of records, so a cache copier has actually replaced the "current" record with one from the cache.

The problem with looping calling the backend save in the latter case is
that we don't call a virtually overriden saveRecord for each record. 

We need some kind of recursion, or a flip-flop recursion only from the
first saveRecord. We also need to specifically identify the case where
the current record has changed.

		
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/07/27
FIXING THE C-TREE DODA FOR ODBC
Geoff Hitchmough's experiments with Access showed that the main problem
is the null, repeated fieldnames used as fillers. 

The problem with specifying strings for these fields is that c-tree only
stores a char* and so requires a constant string somewhere at which to
point. We can't therefore construct an OOF_String on the fly. 

Note that "normal" c-tree use is by defining your structures with an
initialiser. 

A reasonable solution is to take note that all tables can point to the
same set of dummy names. A class can be defined which has static storage
and just returns a pointer to the nextDummy() name.

		
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/07/27-28
SEARCH LOGIC AND COMPLETING recordNumberMatching

The problem is that in some circumstances we want to repeat a large
amount of the search parsing logic in recordNumberMatching, but the way
in which the search is performed is simplified. 

Note non-indexed refers to an inability to use an index, eg: if there is
a wildcard in a string.

Given a set of all records and a suitable index as the sort order, RNM
is trivial and efficient:
- perform a search to get the first matching key
- call OOF_ctreeBackend::RecordNumberOfKey.

WARNING This case only applies when the current sort order is the same
as the index - if we are sorted by a different index from the search key
then the first record found is not necessarily the correct one.

Given a set of all records and a suitable index as the sort order, RNM
is trivial and efficient:
- perform a search to get the first matching key
- call OOF_ctreeBackend::RecordNumberOfKey.

For any other circumstance in which we have a set of records, RNM becomes:
- search to get an OID
- search the set of records for that OID

However, if the search amounts to a linear search through the database,
the above (for non-indexed cases) is much more efficient as 
- search selection until we encounter a match

This case is virtually the same as the set of all records with a non-indexed search.

A crude implementation is possible by copying a lot of the existing
search logic into RNM, and living with the duplication. 

Looking at the search code, there is a LOT of code repetition. If we
pull the comparison mechanism out of the loops it will be much more
efficient and vastly smaller.

Doing this for the RNM completion gives a basis for recoding the
non-indexed searches later. Searches could also be improved by use of
indexes when we have a set of records, and doing intersections rather
than using non-indexed searches. 

Assuming the comparisons are going to be callable by a single loop,
there are a few alternatives. The choice comes down to a lot of code
with simple code being executed for any given path, vs a virtual
function. 

1) write a set of virtual dbField::fieldComparison functions and pass
the appropriate one into the loop 

2) have a case which contains several copies of the loop and use logic
similar to search, which reduces the data types to their simplest (2 & 4
byte integers & reals) 

3) Create a "comparison object" which knows the exact field type and
comparison type. Use the "zig-zag" pattern to create, where we use
compile-time resolution to do half the work, eg call
dbQueryClause::makeComparison(dbShort&,...) with:
dbShort::createComparison(compareEnum) 
{ return dbQueryClause::makeComparison(*this, compareEnum); }

The main goal for efficiency's sake is to avoid virtual function calls
or a lot of evaluation within the loop. In a worst-case scenario a
non-indexed search loop may traverse thousands of records, even millions
(we hope people don't code too many non-indexed searches of this size!) 

The current structure of search looks fairly appalling at first glance,
with a very large amount of repetition of similar code of the form:
	switch (field type)
		loop through selection
			if (field oper value)

The runtime efficiency of the above code is very high, apart from the
single issue of the loops using start/more/next. 

However, for the RNM I'll try option 1) above as discussion with Brad at
Mercator indicates he'd appreciate these functions anyway (as I
suspected) instead of some of the situation-specific casting used in
their app. 


** LATER **
Recode to separate out parsing of the situation from the actual search mechanism.????


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/09/02
CLONING AND DELETING RECORDS

A very nasty bug surfaced recently in Mercator's KIDMAP where the following sequence occurred.
- clone a record which has related info
- before saving, delete the clone
- examine the original - the related records are deleted!


The essence of the problems is that the clone process leaves the
original relationship fields intact. In this case they are all pointer
relationships. 

A naive fix is to iterate all join fields, changing them to blanks.
However, if the user has allowed another record to be saved with a blank
ID, the clone may still be pointing to someone else's related records! 

There may be an argument for some applications to preserve the join
fields in a cloned record. To allow for people to customise the
behaviour, we can use a virtual function on dbTable, so they could
override and not automatically cleanup.

The nature of the cleanup must:
- be efficient
- guarantee that no other records will be inadvertently related



-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/11/12
SORTING BUG FIXES - SORT COMPOUNDS FOR NON-INDEXED SELECTIONS

We lack non-indexed sorts for fixed binary fields, and also reverse
non-indexed sorts for compounds (the initial reason for this fix). 

A naive fix is to treat the field as a binary set of data, and for fixed
binary fields we can do so with a copy of the dbChar sorters. 

For compounds, we don't store the compounded value. We *could* use
copyString on each field but that's not very efficient as it uses
extract() on each segment. 

Looking at how we handle normal non-indexed sorts for compounds, we just
convert the compound into a dbSorter and recurse. 

This can therefore be applied to reverse sorts - we just toggle the
reversed flag for each segment in the dbSorter made from the compound. 


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
97/11/26
CHANGE FIELD WIDTHS TO MATCH OPENED FILE

In particular with using dBase for import/export, there's a migration
issue with field widths and the database schema. 

Ideally you want to be able to open a slightly-modified database with a
new schema, and have fields adapt themselves. 

The dBase backend already validates all fields types against the header
on open. It first eemed we just need to check the field lengths and
modify them, and make sure any internal record lengths are also updated.

However, if the alignment of fields is changed by a move from an even to
an uneven field width then we effectively have to recalculate the entire
internal schema of the record. Similarly, we may move subsequent fields
from a 2 to a 4 byte boundary, and no longer need a padding field. 

At the same time, there is no need to keep buildSchema visible at the
dbTable level and we can cleanup the identical buildSchema methods in
the backend. 



-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/04/16
REMOVE (INCORRECT) AMBIGUITIES UNDER MSVC

MSVC has always had very bad parsing of the "logical distance" in
conversions when trying to find possible matches for a function. It
therefore reports compilation ambiguities that most (if not all) other
compilers accept.

Two examples: OOF_String::operator[](unsigned long)

As OOF_String also has a cast to a const char*, VC happily considers the
following equivalent: 1) convert the argument from int to unsigned long
2) apply the user-defined operator const char* to the lhs object, thus
using the standard C array subscript.

A similar problem occurs with the query clauses in our search() calls.

eg:, where an operator<=(long) exists in the parent dbNumericField
dbUshort x;
....x <= 25;

The choices are between
1) convert the rhs int to a long
2) invoke the user-defined cast operator to convert the lhs dbUshort to an unsigned short.

Again, it is only VC which sees this as ambiguous.


The solution we will try is basically overkill - define each offending
operator for each likely integer rhs argument. This has been tried in
the past and leads to ambiguities on other platforms, mainly depending
on the default data type of numeric constants. Thus, for prudence, these
changes will be for MSVC only.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/05/04
Y2K COMPATABILITY

The only problem that can be blamed on OOFILE at present is the
behaviour of 2-digit dates where we simplistically just add the current
century. This has come in for criticism. 

A similar flaw exists with our dbDateTime class if the application
resets dbDateTime::sBaseYear to use 1904 (a popular base). In this case
entering 03 would get 2003 but 04 gets 1904. 

The simplest way out seems to be a cutover date which the application
can adjust. This can be central to the date-time classes, say a public
member dbDate::sNextCenturyThreshold which defaults to zero. 

If the application defines this year, eg: as 70, then any 2-digit date
earlier than that will have 100 added. 

To maintain consistency we will use a 2-digit value in this variable and
provide accessor functions. Thus it will be equally valid to call
dbDate::nextCenturyThreshold(1970), dbDate::nextCenturyThreshold(2010)
or dbDate::nextCenturyThreshold(70). This is a purist's point - I don't
*really* expect OOFILE to survive a century but we may as well lead by
example :-) 

Whilst investigating this issue I noticed there is a lot of duplicated
code in functions like dbDateTime::istream2ymdhms where two variants
exists, solely so that one can have character separators passed in as
parameters. It would be much easier for code maintenance to make on a
variant which calls the other, passing appropriate defaults:
dbTable::kFieldSep & dbTable::kRecSep 

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/05/07
CROSS-PLATFORM LINE ENDINGS

This is an issue we will restrict to dbText fields only, the assumption being
that a dbChar fixed-length field has a single line.

Given the overhead, I feel very strongly that this should be an optional
feature, and something that is runtime switched rather than being in 
a database build. Later, it could therefore be switched based on a resource
in a file saying on which platform it was written.

A static member and appropriate setters/getters in dbText should do for
the conversion, with a default of false.

All loading of data into dbText fields is gated through dbBLOB::LoadField. 
There seem two reasonable alternatives for affecting the loading process:
1) make LoadField virtual and override with a version in dbText that 
  postprocesses the parent method by converting the string (if necessary)
  
2) have a flag in dbBLOB to say that it is a text field. This may be useful in
  other circumstances and avoids a simple function call being changed into
  a virtual method. Adding a bool member is minor overhead.
  
Choice 2 offers the most extensible architecture & least overhead of speed.

Later also realised there is an issue with the user code called from 
dbText::operator const char* in a calculator. After discussion with users 
agreed that it makes the most sense for this to be post-processed in the same
manner as we process on loading from the database. However this is not possible
with the current calculator architecture as we return a const char* from
the calculator, and so don't have local storage with which to post-process.

98/05/31
Our initial implementation had a simplifying assumption that made it a lot
more efficient - that the style of the first line break encountered defined
them for the entire string. This was promptly found to be broken by 
KIDMAP as the user had a string with linefeeds to which they appended a
field from the database (which had the current platforms's DOS CRLF
line endings).

Decided this kind of circumstance is going to occur often enough that it was
worth making all the algorithms cope with a mixture of line endings all the time.

For the CRLF line endings we can simplify things by, as we count the
number of single line endings that need expanding, we convert them all to
the same character (ie: don't leave a mix of CR and LF). 

Alternatives:
1) For the conversion to CR or LF line endings, a common routine can be used
that is parameterised by the destination type?

2) whilst the conversion to CR and LF are very similar in pattern, in fact
they are different in that the CRLF conversion has a different character of
the pair removed. It would be more efficient to copy the algorithm.

Decision:
as this is a very critical performance routine go with 2)


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/06/15
USING FIRST FIELD IN COMPOUND INDICES

Background:
OOFILE was originally intended to transparently re-use the first field in 
a compound index as an index without having to separately index that
first field.

This didn't work very well, and the feature to set this up in BuildIFIL() 
was disabled.


Issues:
A number of places assume that the uniquifying suffix is a longint (the
standard c-tree Plus method of appending the record offset). We need to
store a separate value for a key to indicate the length of
the suffix (which defaults to the sizeof(unsigned long) ) and pass this
into routines like BuildKey.

There don't appear to be any trade-offs in doing so, or competing
implementations.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/06/18
CLEANUP SELECTION SYNTAX

There are annoying inconsistencies in the key interfaces for set
manipulation. Although dbTable is best, it has a bunch of operators like
+= that fail to take the dbTable* form of table that is most common in
GUI code, forcing the user to deref the dbTable* pointer all the time.

dbSelection is even more annoying to use. It lacks most of the
additional interfaces declared by dbTable and also you can't create a
dbSelection variable without copying from a table.

The original intent of dbSelection was to preserve table's current
selections, however the coding style that is cleaner often involves
declaring the dbSelection vars ahead of their assignment.

Thus, I think it is highly overdue that 
a) the interfaces of dbTable and dbSelection be made identical for
   set operations, including taking dbTable* arguments

b) we be able to create standalone dbSelection variables.

Issues:
1) implementing b) requires some runtime checks to ensure mSelection is
   non-nil before using it. However, in a lot of cases it is still valid
   to perform the operation, if we consider a dbSelection with no
   internal selection to be semantically equivalent to an empty set, ie:
   intersection has no effect nor does difference, and union equates to
   just assigning the rhs to the uninitialised lhs.

 2) if the rhs is an oidT then we don't have enough information 
   to initialise the dbSelection (no table from which to copy).

Possible solutions to issue 2):
2a) just assert if the dbSelection has not been init, and rhs is oidT
    (only an issue with union_with)

2b) have a factory function on the abstract base OOF_Selection which can
    return a default selection.

There doesn't seem to be a lot of point making this into a big deal.
It's basically a coding error if someone appends an oidT to a non-inited
dbSelection so I'm happy to just assert for now.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/06/18
LOSING CHANGES TO RELATED SELECTIONS

Background:
Given a related expression like Patients->Visits->currentSelection() we
are operating on the set of Visits related to the current Patients
record.

However, there are times when it is apparently natural to programmers
(including me, dammit!) to change the rhs selection, eg:
Patients->Visits->searchSelection( Patients->Visits->Date > "12th March" )

The intuitive behaviour has been agreed, that you would expect this
refined selection to persist until the next change of current record or
similar operation on Patients.

The bug at present is that the next operation on the related selection
finds it is invalid (and so resets it as the related selection 
for the current Patient) - WHY????

Issues:
Are there any places which rely on this buggy behaviour?
- none in KIDMAP
- relateMN uses relateSelection which is affected by this bug. This may
  mean a benefit of side-effect fix to relateMN which is possibly broken at
  present!

Later:
with relateSelection (now selectAllRelated) it seems there was a
simple bug in that the related table was not being marked as valid. This
may not overlap with the main bug where generic behaviours on the RHS
table cause it to be marked invalid. We need at least to test
searchSelection on the RHS table and maybe arbitrary search as well.

Note: I suspect very strongly it is the broadcasting of a change of the
RHS selection which triggers the update.

98/06/19
Deeper investigation shows that after traversing a relationship, the RHS
is not shown as valid. It appears that in many cases we are traversing a
relationship, only to traverse it again driven by MakeTableValid.

Checking of the fan-out from the various relating methods
(relateRecordFrom, relateRecord and relateSelection) shows that all
ultimate callers either
- are unused in KIDMAP and our samples
- call MakeTableValid.

Discussions with Mercator programmers and looking at some comments in
KIDMAP indicates that some methods (eg: countAllRelatedIn) may have 
been ignored because of such bugs.

Decision: scary though it may be, make the lowest level
(relateRecordFrom) set the RHS table as valid. This seems OK.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/06/25
PROPAGATE SORT SUSPENSION

Context:
in KIDMAP's complex export, found that they occasionally had a hang when
iterating a related table, where sorting was being triggered on the related
table because they were updating it, and so their iteration of the related table
was stuck in the loop (continually restarting). This was despite them suspending
sorting on the entire database.

Obviously suspension was not being conveyed to the related tables.

Alternatives:
1) suspendSorting should broadcast to all existing tables

2) suspendSorting should propagate to all related tables
   a) by calling a virtual suspendSorting on all ields
   b) by broadcasting a message and having relationship fields react
   
Arguments:
1) has an obvious weakness - how do we exclude tables? There's an obvious
hierarchy down from a database connection and it would be hard to tell if a
temporary table was supposed to be dependent in this manner. I suppose a
registration mechanism could be used, so we iterate all tables derived from this
connection rather than just all those declared in it, but this seems weak and
prone to side-effects.

2a) has an appealing simplicity. However it means suspendSorting would acquire
up to hundreds of extra virtual function calls. This is a large overhead for a
routine called inside core search and sort loops.

2b) is clean and generic - if anything other than a field needs to know if a
table has suspended sorting, then it too can subscribe to the table. The
disadvantage is that some GUI classes will receive extra (ignored)
notifications. I had some initial concerns that there would be richocheting
broadcasts back to the LHS from the cloned tables, but the dbRelRefBase objects
that manage the links only listen to leftmost tables.

Decision: choose 2b. It is clearly superior to the others in genericity and
performance, despite the GUI issues.

Later note: an implication of 2b is that dbTable::suspendSorting and
resumeSorting need to be non-const as they invoke the non-const
dbBroadcaster::broadcast().

This could be worked around - we could have a dbBroadcaster point to another
structure and make broadcast() const. However, suspendSorting etc. are
conceptually affecting the state of the table so I'm happy to just make them
non-const.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
INDEXING CALCULATED FIELDS
98/07/13

The simple issue is that we need to store data for the ISAM engine to index it
therefore a calculated field that is indexed will still have data storage.

The complicating issue is which behaviours do we want to affect.

As far as the user is concerned, the behaviour of the field should not change,
therefore it should be regarded as still being a virtual field. It is only a
single backend (c-tree) which changes its behaviour therefore it seems
reasonable that only in that backend do we decide to allocate room for the
field.

However, we do need to know when to update the data buffer before saving the
record and it seems this may want to be handled at the same level as we handle
default calculated fields (OOF_SimpleRecordBackend::saveRecord).

This introduces a performance/side-effect dilemna - when should we be invoking
calculators?

The simple approach is that the field value is copied from the calculated value
into the buffer when we save the record. This is independent of whether the
calculator has been invoked previously.

There is an attractive optimisation to avoid invoking calculators that says we
can use the record to cache the calculated value. However at present we don't
record enough about dependencies to know when this cached value will have been
invalidated. (If later calculator implementations allow dependency tracking we
can overcome this problem.)

Decision:
- always copy calculated values to storage (with field determining if actual copy takes place)
- name methods assuming we may later have other reasons for storing calculated values - don't refer to indexing alone as the reason


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
DIRECTORY ITERATION
98/07/16
The usual iteration logic for deep iteration of a directory is recursive.

We have a choice of either
- allow an iterator to receive an operation as a Visitor object, or
- implement iteration which returns "current file" as a means of traversing the
tree in a single external loop

The latter is appealing for the immediate need to iterate all files in a nested
fashion but initial programming investigation shows that it presents some
difficulties. Essentially, we need to retain some kind of stack of context to
mimic the effect of recursion. 

In general, the OOFILE model has been to provide simple iteration built into
basic objects. 

If we either provide explicit iteration in the oofDirectory or in a separate
oofDirectoryIterator then we have the issue of flattening to cope with and the
need to stack contexts.

On the other hand, building a Visitor model into oofDirectory has some
attractions. We can have a base oofDirectoryVisitor which just outputs to a
stream. This allows us to easily follow the recursive model of directory
iteration and is a good way to introduce Visitor concepts to OOFILE users.

We need a simple semantic on what the visit() interface passes. This will be
platform specific (unfortunately) and should comprise the minimal information
required to open or get info on a file. This may be a Unix filid, full pathname
or Mac dirID and filename. To keep the interfaces clean, it's time for a class
to encapsulate this reference. The big benefit of using a class here is that a
user can decide to only implement certain platforms within their visitor. If the
interfaces were class specific they would have to write #ifdef'd code at a wider
level to make it compilable.

Decision - use Visitor model.

The only remaining complication is how to react to an alias not being
traversable (Mac) or possibly the same thing with Windows shortcuts or Unix
links. (It suddenly occurred to me that all 3 OS have similar features with
potential for the other end to be lost.)

How about if a Visitor handles the error, it can do so in another method
(linkFailed) and the default method returns false, causing an exception to be
thrown?

DEFERRED:
We will not therefore be using exception handling for a Visitor that tries to
find broken aliases (an early example) and so can make the LinkNotResolved
exception cross-platform. This implies it will need to pass back an oofDirectory
or file reference identifying the problem link. This can be implemented later.

REVISON OF VISITOR MODEL - 98/07/19	
Implementation of the above shows one weakness, knowing when we leave a
directory.

I think it would be clearer if the the visitor model had 3 visit methods - with
enterDir and leaveDir as separate calls so the visitor can adjust any state it
likes. This is better than making assumptions that a visitor only needs to know
if a file reference is a directory.

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
DIRECTORY ITERATION - FILE REFERENCES
98/07/19

Should a file reference know if it is a directory?

Our oofDirectoryVisitor needs to know if it has been given a file ref to a file
or a directory.

One complicating factor - we want visiting to be a very fast operation. In our
platform-specific visitFile implementations we will have a platform-specific
internal state (FSSPec record, string etc.) to which we could pass a reference
rather than copying it.

If the oofFileRef has extra data such as isDirectory then we can't directly
substitute an FSSPec record. This immediately causes significant copying
operations.

For efficiency, it makes sense that isDirectory is a separate flag to
visitFiles.

However, in future implementations, it may be more convenient for an oofFileRef
to be able to know if it is a directory. We can always add methods that do a
runtime evaluation to resolve this query.

Decision: keep separate flag and use minimal structures in oofFileRef.



-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
IMPROVE CALCULATORS & THEIR CLONING WHEN CREATING NEW TABLE
98/07/21
With the dbCharStripperCalculator we need to keep a pointer to another field in
the same table.

Traditionally, accessing other fields in the table has been done via a somewhat
awkward casting in the calc() method. This means we are using dbTable::field() a
lot of the time.

We could cache the dbField* returned from field() if we know to perform the
lookup once at clone() time. This would be a lot more efficient for calculators
that are heavily used.

We therefore need a way to know when a calculator is being cloned, the
destination table. We can use the same downcasting idiom within the clone method
that's been used in calc() in the past. This would let us get a pointer to a
field in the new table.

To retain backward compatability, the existing clone() method should remain
unchanged. However, we can add another virtual clone(dbField*) which will not be
a pure virtual, but defaults to calling the older clone(). For future
calculators, this allows the choice of working out fields at calc() or clone()
time.

PROBLEM
One flaw which shows up with this is when the calculated field precedes the
field it is mapped to. We seem to need some deferred initialisation here.



-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
STRING PORTION HANDLING AND COMPARISON
98/07/24
We want an easy way to extract and replace portions of strings. This goes with a
findString feature.

Thus, a replaceString takes two parameters and uses findString to locate the
area to remove.

A more useful high level routine for replacing portions of text files, like web
pages, is replaceBetween which takes a start/endString pair of bracketing
strings. This requires the ability to find a string from a given starting point,
as the terminating string is not guaranteed to be unique (ie: instances of it
may occur before the starting string - we are only interested in the first
instance AFTER the starting string).


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
DIRECTORY ITERATION - CONTROLLING DEPTH
98/07/28

The initial design of the directoryVisitor and oofDirectory::visitFiles had a
boolean param to control deep iteration.

With more experience in applying this model, there are times when we want to
control the depth of the iteration, ie: no more than 3 directories.

The proposed fix is to have an unsigned integer be the depth to which we
iterate. This is something that has to be handled in the visitFiles loop - all a
visitor could do would be to quickly leave a directory without processing, it
cannot prevent further recursion (is this a weakness in our model?).

This also fixes the problem of endless recursion. If our default number of
nested directories is something relatively sane, like 1024 then at least we will
not recurse forever. Also, passing false into a directory iterator will map to a
depth of zero so giving the same effect.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
READING TEXT FILES
98/07/29
It is useful to be able to rapidly read an entire text file, of presumably
moderate size, into an oofString. For example, using the string portion features
recently added to update or extract parts of a web page.

Using iostreams is a fair overhead. It would be better to have a
platform-specific way in which a single disk read could bring a file into
memory. There is not any way in which this seems justified being part of
oofString itself, as an oofString should be manufactured by such an operation.

However, now that we have oofFileRef to point to an individual file, it seems
reasonable to use that to provide "utility" features such as reading a text
file. This also lets us take advantage of the Mac-specific file details in
oofFileRef.
												
		
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/08/02
FILLING IN STRING MASKS, EG: WEB PAGES

We want an easy way to specify a mask and then to populate it with a bunch of
strings. For now, a simply replacement syntax is being considered, rather than
something with more sophisticated field references etc. ie: we have a series of
static chunks intersposed with replacement points. 

For more general use, a syntax for specifying the replaceable parts could be
specified (eg: based on FM Pro tags) which may read field names from the mask
and go directly to a specified database.

For simple replacements right now, we assume we need
- a way to specify the text of each replaceable item in the mask
- an ordered set of strings.

There is also the question of how we store the chunks around the replacement
points.

For optimal speed of output, it would be best to store a set of (offset, len)
pairs. These can be used to retrieve data from a single string containing the
mask and output it in combination with the input fields. Having the sets of
lengths stored means we can quickly calculate the total length of an output
string by summing those lengths with the replacement fields.

However, after a bit of experimenting, it occurred to me that if we store all
our chunks as discrete strings, we don't need to load the entire web page from a
single file. We could instead create a page by appending individual chunks. This
makes much more sense for programmmatic construction of pages.

For a simple approach, even if we don't allow for it now, we should have the
capability to replace multiple locations in future.

Note: for future flexibility, it would be nice if the assumption of
chunk|replacement point|chunk... was more flexible.


										
		
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/08/04
CLEANUP STRING SEMANTICS
The main motivation for changes to the oofString interface is providing more
consistent use, and recognising that a lot of people use oofString in their code
and we want it to be a class findable by browsers.

Thus, we will swap the actual class and the typedef OOF_String and oofString
around.

For a long time, I've wanted to avoid having a char* cast in oofString. This
causes problems with the c-tree Plus and similar C interfaces. However we can
have a large and ugly named function (to discourage abuse) for these highly
wrapped interfaces.

To avoid using casts to get a const char* in places where implicit casting
doesn't occur, add a function. 

Finally, encouraging people to use const oofString& instead of const char* in
their interfaces, we need a way for field types such as dbChar and dbText to be
implicitly converted. It would be very clean if all fields behaved the same way
and we've added copyString as a virtual function in the past to achieve this, so
an operator oofString that invokes copyString gives a nice interface.

		  
		  
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
OOFILE 1.3b4d18 public release
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/08/25
DELETING FILES WHILE ITERATING A DIRECTORY

In the Printforce web site rebuilder, a weakness was shown in
oofDirectory::visitFiles if a visitor deleted the file being visited. As the
Mac version just indexes the current directory, the entry after the current
one was being skipped because of the deletion.

There are three possible approaches to fix this:
1) force the application code to handle the problem (probably by constructing
a list of items to delete). This ignores the fact that other users may well
make the same mistake in future.

2) Provide a callback to notify the oofDirectory that the current visitor
either dirtied the directory or deleted the file, so the index should not be
incremented.

3) When creating the list of files to visit, preprocess the directory to
create a list in a single pass, before calling visit.

Alternative 1) seems to likely to cause other error. Of the latter two,
both have minor flaws but 3) seems somewhat cleaner in minimising the
chance of side-effect. It also doesn't depend on special behaviour in
the visitor, which is more convincing as a reason. 

Note: for future use - preprocessing the list of files would allow us to
add sorting capabilities regardless of the runtime OS sorting, if any. 
DECISION - 3).


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/10/21
QUICK WAY TO SET KEY LENGTH

Since making it possible for calculated fields to be indexed, we have a
problem with some clients who have declared a calculated field as
indexed in the past. Whilst the index will not previously have been used
for searching, there will still have been a c-tree index of 0 keylen
created (probably with the 4-byte suffix for duplicates allowed).

Simply marking that field as non-indexed now causes a c-tree error on
opening due to the indexes in order having the wrong field length.

Other clients have long wanted the ability to specify an index on only
part of a field length.

Both needs can be met by being able to specify a fixed key length, which
must be less than or equal to the data field length. Users with
previously indexed calculated fields can specify a length of zero.

An alternative which copes with only the indexed calculated field issue
is to add an index type specifically for these, that creates the index
as defined in the old manner. This is a hack for backwards compatability
and doesn't yield any other benefits.

DECISION - allow keylen specification.


Other Issues - Deferred:
Specifying key length as a dbField attribute is another item which
should be in a central dictionary. Time pressures force it to be
deferred, but VERY SOON I want to have attributes which are 'static' to
a field to be in a central definition so that copying dbFields is lower
overhead.

This feature does not provide a way to construct compound keys from
parts of other fields.


Side Effects:
If an index no longer indexes the entire width of a field then some
searches will not work properly. I think for the first cut at this we
will leave this as a user-expected behaviour, but ask the mailing list
for feedback on expected behaviour. It might be appropriate to use a
partial key for startsWith searches, sorting and creating a selection to
be iterated to complete a search.

Note: for many users, if not the majority, using key compression would
be more appropriate than partial keys, if they just want to conserve
index space.


=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/10/30-11/02
ELIMINATE REPORT MEMORY & RESOURCE LEAKS

One problem identified with memory leaks shows in the RAM backend,
where blob bodies are not being deleted. The RAM backend uses the normal
cache for all its storage but overloads the use of the record position
pointers slightly to be owners of the blob bodies.

This causes two leaks.

OOF_ramBackend::SaveContext leaks when saving old records as it doesn't
delete the old blob contents.

Deleting the cache invokes the 
only deletes the mBuffer, not the bodies owned by
positions in mBuffer
		  

=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/11/17
OOF29 - FIX BUG WITH DATES IN NEXT CENTURY

With the date threshold set to 1940, and a current year of 2003, dates of year 11
were being calculated as 2111 which caused wraparound in the dbDateTime 136 year
range to about 1975.

The problem was our decision to store the threshold as a 2digit year,
failing to consider that the threshold might be set in a previous
century to the current one.

Including the century in the threshold makes the current logic
safe regardless of the current year. Note that it will still be 
possible to cause errors with dbDateTime's limited range if a 
year is entered outside the range.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
98/12/27
VC5 COMPATABILITY - OOFSTRINGS AND DBCHARS
Compiling KIDMAP with VC5 - still problems with the dbChar as source for
oofString ctor.

Just adding back the dbField::operator oofString causes assignment
ambiguities, although solves occasions when we pass a dbChar into an
oofString param.

Tried just dbChar::operator oofString without dbField version. This is
still ambiguous with dbChar::operator const char* for assignment.

Tried a few silly things
- params as const oofString& instead of oofstring - no change
- change oofString ctor to oofString(const char* const) - no change

The problem is twofold.

According to the ANSI standard, we can't just pass an object of class dbChar
into an oofString param unless there is either:
- an operator oofString for the dbChar, or
- a ctor oofString(dbChar)

The dbField::operator oofString that was added on 98/08/02 works
fine in this circumstance.

However, a new bug that has appeared with VC5 (and I think is a real
MS bug) is that copying a dbChar to an oofString is now considered
ambiguous.

When choosing between operator=(const char*) and 
operator=(const oofString&) VC5 considers the dbChar::operator const char*
to be an equal contender with dbField::operator oofString.

Since the operator oofString is a relatively recent addition, I think we 
can survive its removal without aggravating too many users. This means
a copyString() method will have to be called (or an explicit oofString 
cast used) whenever passing a dbChar argument to an oofString param.
No alternative seems possible (after several hours stuffing around!).


Further insight & issues 99/04/28
Making the oofString(const oofString&) explicit removes some ambiguities when 
passing dbChars, as blind creation of an oofString is not allowed and thus the
dbChar::operator const char* is invoked.

It doesn't fix the problem trying to pass a dbChar in explicitly to an oofString
ctor however, being ambiguous choosing between dbChar::operator const char* and
dbField::operator oofString.

As Mercator are compiling this code with VC6 it appears to be only a VC5
problem but the original decision stands - remove the dbField::operator oofString
and let users call copyString() if necessary.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/04/27
VC5 COMPATABILITY - AMBIGUITIES WITH CALLS TO FIELD

dbTable::field() ambiguities reported with all kinds of integer args 
(even when you passed in a fieldNumT it said ambiguous)! 
I'd added an oofString(char) ctor which was being invoked as an int conversion.
ie:
int -> char -> oofString(char)
vs
int

AAAGGGG!

As this ctor was added recently for ease of defining strings quickly, we could
either remove it or remove the ambiguity.

Making the ctor take a 2nd param of multiple char count makes it more useful 
as it can now be used to create a string of N chars and will still be a quick
way to create a string of one char.
		

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/04/28
EDITABLE VIEWS

We want to be able to 'overlay' the fields in a view with a set of rewritten
fields (to support editing in the report-writer preview).

A subclass of dbView could be used that hangs onto a list of standalone fields
indexed by record number and field number.


		  
		  
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/05/06
ALLOW DELETING OF TEMPORARY DATABASES & IMPROVED CLEANUP

We want a dbConnect to be more easily used to manage a temporary database.
Also, it's become a nuisance to users to have to delete dbTables before the
dbConnect.

There are a several different user models for how tables and connections
are used.
1) a user data class inherits from a dbConnect class and includes dbTables
   as concrete members
   
2) dbTables and dbConnect objects are members (heap or concrete) of a user
   data class.
   
3) dbConnect_ram is used just to create a dbTable in RAM which can be 
   retained AFTER deletion of the dbConnect_ram.      


Alternatives for indicating a database is temporary - pass a default flag into
the ctor or have a member to set it. There seems little point in having a
separate method which just expands the interface.


Operational Patterns
--------------------
If you delete the tables before the connection, they call close. Thus if they
are members in 1) or deleted before the dbConnect in 2) above this works.

If the connection is still open when deleted, we could assume it is responsible for 
deleting its tables.

Deleting storage is a virtual operation for a temporary (persistent) database
as we have one or many files depending on the actual connection subclass.

We have to allow for the possibility that we may want to reopen a database
so can't have close() do the deletion of files.

Remember that the deletion has to be via a virtual function so can't be
in the dbConnect dtor. We therefore must have a virtual function called from
each connection subclass dtor.

Who deletes storage?
If the dbConnection manages all file storage, it can delete files.
If the individual backends (eg: OOF_dbaseBackend) manage files then it
needs to call a special method on them. We can defer this implementation for now.

		  
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/05/20
PROBLEM WITH RAM DATABASE DELETION USING BLOBS AND RELATED RECORDS

Finally I think we're getting to the core of this.

RAM databases store their records in a OOF_ramContext (subclass of
dbContext->OOF_recordBufferedContext) which has a pointer to the backend which
created the record, used to delete the blob bodies according to the dictionary in 
the RAM backend.

The problem comes with adding related records.

Scenario:
1) Students and Classes are original tables

2) Students->Classes is used to add related records. These are of course central
   so can be accessed directly through Classes (probably after selectAll)

3) Deletion order follows table declaration order:
   - Students
     - Students->Classes (cascading cleanup of related clone)
   - Classes
   
By the time we delete the main Classes, the OOF_ramBackend belonging to 
Students->Classes is deleted. However, the records added in 2) have pointers
to that backend and try to use it.

Alternatives:
1) add reference counting to the backend so it isn't necessarily deleted until
   all the users have finished with it
   
2) as the backend is only used by the OOF_ramContext for this cleanup, store a 
   pointer to the 'prototypical' backend (used by the prototypical table)
   in the OOF_ramContext.
   
3) store enough information in the OOF_ramContext that it doesn't need to 
   use a backend to delete the records
   
4) in the OOF_ramBackend dtor, before we delete the cache, make all the 
   cached records point at the deleting backend.
   
Alternative 4) is the only one which doesn't add extra data to objects,
particularly the OOF_ramContext objects of which we have one per record. It also
has by far the simplest amount of code changes and doesn't make any assumptions
about the order of deletion. It is possible that a prototypical RAM backend
might even be deleted before the cloned one so 2) could have its own 
order-dependent bugs.

However, once I started looking more deeply into 4) I realised that the sole
remaining reason for OOF_ramContext to be a subclass was for this callback.

DECISION
We can therefore short-circuit and simplify this whole process even further
by having the RAM backend call its own DeleteBlobBodies on the buffer from the context!

		  
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/05/26
SERIALIZATION SUPPORT

Where to put support?
---------------------
In serializing custom code associated with reports (to an XML file) we actually
have a default serialization mechanism that can be subclassed. Given that it is
not really part of reporting, although used there, it should be available in the
free OOFILE core folder.

So, where should it go?

The serialization doesn't know about anything other than strings - should it go
in oofXML.cpp (association by use), oofstring.cpp or in its own files?

Adding files to OOFILE complicates builds, version control and shipment and is a
last resort.

DECISION - it may not be optimal but serialization will
definitely be used with XML, although usable elsewhere.


How to lookup?
--------------
Options
1) lookup by fixed size signatures in an array
2) lookup by strings in an array
3) implement lookup by a method call to factory

Option 1) is efficient for lookups of lots of potential objects, as used
in PowerPlant, but the 4-character signatures become tiresome.

Using strings as in 2) (and potentially 3) allows for users to have much
more expressive signatures so is appealing.

The lookup is slowed slightly by a virtual function call as in 3) but
allows for default string matching with the ability for the user to
add their own qualifiers in the signature for ultimate flexibility.

Decision - 3) with virtual call to Factory.



Dictionary Location
-------------------
Storing oofSerializableFactory::sFactories with the method implementations
in oofxml.cpp leads to a nasty race condition with static init order, due to
the oofRepDefaultSettings object trying to register factories at static init
time and (unpredictably) being called before statics in oofxml are processed.

More research on classic Singleton patterns is needed before I'm aware of
all the options, those that I've thought of being:
1) pragmatically, as reports are the only users of sFactories, move it into
   the same file as the oofRepDefaultSettings static causing the problem
   
2) lazy init of report standard factories - check each time a report is
   created
   
DECISION - for now use 1) as it is much easier


Factory objects vs Functions?
-----------------------------
PowerPlant uses static factory functions to reload from disk but then it has
many more classes to stream in.

The disadvantage of using factory objects is that it doubles the number of classes,
being a unique factory class for each custom code class. It is rare that the user
will need the features of a factory object. 

The advantage is that it allows the factory to retain state. If the user custom
code classes should react differently when reading data, based on some
application state change, then having a factory object is much better
encapsulated than having the factory function have to query something else in the
application.

Using the Builder pattern, we can satisfy both needs.

A default Builder is used to handle a serializable object that registers a 
factory function with a lookup key. It knows how to match a lookup and call
the factory function.

Alternatively, the user can add their own Builder object (oofSerializableFactory)
with all the flexibility of a separate object. When retrieving objects, they 
can lookup the Builder itself (to use a downcast possibly to pass in different params)
or can just lookup and invoke the factory in one call.

DECISION - use Builder to provide a factory method approach with optional override.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/05/31
Y2K MINOR FIX - MORE REASONABLE 2-DIGIT YEAR HANDLING

USER REPORT:
	When the System Date is greater or equal to 2000
	using 2 digit dates gives wrong result
	Change system date to any day month in year 2000, DD/MM/2000
	Go to any info window
	in the DOB filed, enter the date 1 4 92
	the date that you get is some strange figuure.... 

ANALYSIS
	Their application sets sNextCenturyThreshold to 1940. The simple 
	algorithm in OOFILE at present (compliant?) adds the current
	century and tests against the threshold date to see if earlier.
	Thus, 92 becomes 2092. The reason they saw strange dates is their
	use of dbDateTime with 136 year range, instead of dbDate, and their
	setting the range to 1940->2076.
	
	Obviously a user entering 92 for a historical date in 2001 will 
	expect 1992, and it's highly unlikely they would want 2092.
	
	We thus want to be aware of a reasonable upper bound for dates - 
	sNextCenturyThreshold of 1940 could imply a sPrevCenturyThreshold
	of 2040 (but the user able to adjust it independently). Any 2-digit
	dates OUTSIDE the two thresholds are adjusted up OR down. 
	
	

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/06/04
CACHED RELATED RECORDS BUG

We've had a bug in OOFILE for a long time now that shows up in test 2 with the
RAM backend (I've had David doing a LOT of regression testing lately) which is
the main thing delaying our free release and the forthcoming 1.3. (There have
been a few parallel activities sorting out bugs in the new version).

It's really only a problem creating related records, unless you've set your main
database table setSaveOption(dbTable::requireExplicitAndBuffer).

When we create cached new records (eg: related) we allocate 'synthetic' record
numbers which are expected to fall below the range of valid record addresses.

This assumption is based largely on c-tree Plus behaviour where the first 1024
bytes (at least) are header. It has potential for breaking any time we allocate
more cached records (usually related records) in an application run than the
number of bytes skipped at the start of a file. Thus it will cause problems much
faster under dBase as well.

With the ram backend, record 'addresses' are indexes into an in-memory array and
start at 1 so the problem is hit immediately.

Thus, a new record in the dirty cache can have the same 'address' as a record we
are seeking to load from storage, and the new record is pulled out of the cache
and overwritten instead of pulling the existing record from the main store.

Options: 1) offset the synthetic numbers way up to the top of the unsigned long
range - has potential for conflict with huge c-tree Plus databases.

2) implement more sophisticated cache checking so it's legal to have synthetic
numbers overlap real record numbers (that makes me very nervous)

3) offset the RAM record addresses to leave a reasonable 'gap' at the bottom of
the unsigned long range for the synthetic records.


LATER CORRECTION
Synthetic record numbers are reset as soon as you change the selection on 
their table. Effectively they exist only whilst you are operating on a given
parent record and are reset to 0 when you move onto a new record, so there is
much less chance of them growing to overlap.
(Checking the code change diary, this was fixed on 96/06/07).

However, apart from the RAM conflict the current scheme can still conflict 
with dBase with as few as 30 related records being added in one hit (a small
dBase file with few fields might have that small a header) and with a c-tree
database with a few thousand related records being added at once.

This rules out option 3) which is RAM-specific.

Option 2) has potential for very subtle bugs.

Option 1) is the simplest to implement and most straightforward to validate.
It is also easy to make a user definable range by having the threshold
only defined if the user has failed to define it advance, and losing a mere
2kb off the top of the possible 4Gb range.

DECISION - option 1) with user-definable starting point.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
OOFILE 1.3b4d20 public release
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/06/06
BACKING STORE FOR MODIFIYING REPORTS

We need a way to store edited fields sparsely scattered across a database to
support editing the report contents in the preview window.

The changes can't go back to the original report database and that may well be
too big to copy in entirety into RAM.

We apply the constraint that there must be enough free RAM to store temporarily
edited data.

options
1) use a dbView to manage edited fields
- requires explicit call to trigger mutating
- requires explicit call to save a field 
- doesn't prevent user code updating real fields

2) clone the tables with a backend that 'forwards' calls to the original backend
unless a particular record encountered (use the cache)
- requires explicit call to trigger mutating
- avoids anything bypassing the view and accessing the table
  (but clients keeping pointer to original table or field is dangerous)
- not view-dependent which might be useful
- if user calls view or band for table still OK  
- builds on current caching scheme so much more transparent once triggered


3) enhance current caching scheme so no change to tables, just a change
   in caching mode for current tables
- requires explicit call to trigger mutating
- clash if user code has same tables used in parallel for updates, we've 
  change the caching mode (in this case they should use cloned tables in reports)
- requires a pass through all the tables to restore their caching status   

DECISION - 3) because it is more central and avoids subtle bugs from user code
acquiring pointers to tables or fields that the report later clones.

				
-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/06/10
NEW BUG - ASSERT IF ADD MORE THAN 2048 RECORDS BEFORE SEARCHING

The logic for OOF_recordSelection::appendNewRecord always increments the
mNextFakeNewRecOffset although often it is called when we are not appending 
records to the selection and so there is no need for incrementing.

Now that we have an assertion to guard the upper end of the range, this
results in a crash if more than 2048 records are added in a row (with the
default definition of OOF_FIRST_SYNTHETIC_RECORD_OFFSET.
				

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/06/15
PATH CONVERSION
Path conversion - is it just a string issue?

Simple relative path conversion is strings but anything involving volume
lookup has to be on a destination platform.

More thoughts on path conversion - we should learn from the lessons
with files. Should be able to append a relative path in a different
format (probably Unix) either with a += to oofDirectory or
the input path conversion functions coping.

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/06/21
TEMPORARY DATABASE CLEANUP

When does it occur?
-------------------

It is possible for users to close a database before destruction
of the dbConnect object.

Should the disk storage of a temporary database be destroyed at close()
or dtor time?

There's not much to decide between these. However, the intuitive model
of c++ says that resources have a lifetime of the owning object.

DECISION
- given no counter-argument, destroy temporary storage in dtor


How much should we delete?
--------------------------

The OOFILE model for multiple file databases is to treat the directory
containing the database as a single container.

It seems reasonable that a temporary database should also clean up its
directory. In particular, a user creating temporary databases which have
their own (temp) directories would be very annoyed to find them cluttering
up their disk with a trail of empty directories.

We have to guard against inadvertently deleting directories not created
by OOFILE, however, even if they end up empty.

DECISION
- make OOFILE able to know it has created a directory for a database
- temporary databases cleanup their directories if using multiple files


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/07/04 Sun
LAST MODIFIED DATES OF FILES

We want a cross-platform way to handle the last modified date of
an oofFileRef. However, the contrary force is that ooffiles.cpp has
been carefully designed so you can use it in a minimal context:
- ooffiles, oofstr & oofarray.cpp are all you need. (Plus more 
headers, but this cuts down radically on link overhead).

As soon as we have a method of oofFileRef returning an oofDateTime
we have to link in all the field and table handling of oof1-5.cpp
and possibly more (depending on your linker's ability to strip
unused functions, particularly if they are virtual).

An alternative approach is to compromise the cross-platform nature,
seeking a platform-dependent return value that can be used as input
to an oofDateTime ctor.

Thus we could still write:
   oofDateTime lastMod = theFileRef.lastModified(); 
   
This yields a secondary bonus - if we want to use other date classes
like oofDate or the (future) oofFullDateTime provided they still 
accept this param to a ctor then we are OK.

A quick look at the dbDateTime setters shows that it already supports
the unix-compatability structure tm which is available on all platforms.
Thus, if lastModified() returns a tm structure we have achieved our
uncoupling and have a return type which is immediately usable by people
who'd rather not use our oof types.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/07/04 Sun
WHEN TO USE EXCEPTIONS IN FILE PROCESSING

In the past it has proven useful to throw exceptions when failing 
to write or read text files.

I want to have a clear philosophy about when a status is returned or
an exception thrown.

If we can characterize a method as querying a status, eg: scanVolsUntilFound
then it is reasonable to return a bool.

There are two reasons why we might throw an exception:
- different amounts of error info available on different platforms, and
  exceptions allow subclasses to express that (eg: oofE_MacOSErr)

- the exception is unexpected failure due to an external error.

For now, it seems reasonable and probably useful to users for file
manipulation (delete, rename, read/write) to consistently throw exceptions.

One point which is arguable - should it be an exceptional case if we fail
to goto a directory?

On some OS it is legal, although inadvisable, to have an invalid current dir.

For now, I'm going to err on the side of caution and OOFILE will consider
this an error. However, attempting to goto an empty path is legal, 
considered as the trivial case of going to the current dir.



-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/07/23 
IMPROVE SELECTIONS TO COPE WITH SELECT ALL STATE FOR ALL OPERATIONS

There are a few problems with selections that stem from their original intent.

Selections were originally meant to be just a mechanism for copying around the
selection of a dbTable, not to be operated on by themselves.

Accordingly they implemented the state model which optimises a selection of 
all records and couldn't implement a count() or invert() without the assistance
of a table.

Over time client apps have made stronger use of the selection public 
interface and run into major problems as a result, so it's time that dbSelection
and its internal representations could handle the full range of selection
operations, particularly returning consistent results from count().

We thus need a selection to have access to a table to perform two operations:
- countAll()
- invert().

Within OOFILE there is the unemphasized concept of a prototypical table:
the dbTables that you instantiate to build your dbConnection serve as prototypes
for cloning later, and can be expected to persist the life of the connection.

Whilst it is possible that a client app could have selections that persist
beyond the life of the dbConnection, this would be very rare. Selections contain
internal record pointers and REALLY don't make sense outside of a connection.

If the dbSelection implementations point to the prototypical table against which
they should be used, this gives us a table to solve the above needs as well as
making them more robust - assertions can now guard against taking a selection
from one table and wrongly applying it to another.

There doesn't seem to be an alternative approach which can resolve these forces
as both countAll and invert require database access.

Debugging safety-net:
We could have OOF_recordSelection be a listener for the prototypical table and
if the prototypical table goes away, will clear its internal pointer to that
table. It's not an error for the selection to persist beyond the table, 
provided it is not used. (We must allow for variable scope to cause deletes
out of order!)

Minor side issue:

Tables cloned in RAM (or other backends) must become the new prototypes for
themselves. It's as if you created a table from scratch in a new backend,
although the creation process is actually copying the definition. The key point
is that if we want to call operations on the prototype we must be able to call a
compatible backend. We don't want a countAll from disk after cloning that table
in RAM!


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/07/25
PROBLEMS WITH RELATED DATA 'DEATH SPIRAL' SINCE ADDING CanUseIndex

As part of the BACKING STORE FOR MODIFIYING REPORTS changes OOFILE
became aware of cached data when searching a relationship again.

This has led to a problem with the following client code
(cut down later when I've identified the problem area) going into
an endless spiral. The problem appears related to a non-indexed
search (traversing a relationship) which is causing an unloadRecord:

"OOF_simpleRecordBackend::SearchEqualNonIndexed() calls 
unloadRecord() duing start more next loop thus forcing the relationship to 
be re evaluated, which  means that 
OOF_simpleRecordBackend::SearchEqualNonIndexed() gets called again...
"

				mData->P->PDE->start();
				long countPDE = mData->P->PDE->count();
				for (long counter = 0;counter < countPDE;counter++)					
				{
					if (mData->P->PDE->Attributes.AllowedToDelete(false)) 
						mData->P->PDE->deleteRecord();
					else
					{
						mData->P->PDE->next();
						showMessage = true;
					}
				}
				mData->P->Enrolled_Strands.relateRecord();// refresh relationship -- relateRecord is a method of the relationship
				mMyStrandsHelper->refreshBrowser();


On analysis of the problem, the user report doesn't look at the cause of
the non-indexed traversal.

There IS an outstanding OOFILE bug to do with traversing non-indexed join
relationships, which of course affects the RAM and dBase backends.

However, close tracing revealed the problem was due to records being left
in the dirty record cache rather than due to the above bug. The records in
the cache were TRIGGERING the non-indexed join bug, but they shouldn't have
been there in the first place!

Desk checking revealed a couple of subtle bugs in removing records from
the dirty record cache, however fixing these bugs didn't fix the main one.

Further tracing showed that the problem was that the calling loop calls
deleteRecord and assumes it will move onto a predictable record after 
deleting the current (new) record.

This has only happened by accident before.

However, as this is the second user site which has made this assumption, I 
decided to fix the problem. Accordingly, after deleting a new record,
start() is called. This seems reasonably safe (last time the advice to the
client was to include start() in their loop but the KIDMAP code has a lot 
of these instances).


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/07/25
HANDLING RELATIVE PATH CONVERSIONS

AIM Handle relative paths for other platforms regardless of current - user
code may want to generate other platform paths.

1. assumption if a path name begins with . it must then have a \ or / following 
this disallows Mac names beginning with ./ or .\ which is fairly safe as a 
leading "." on Mac is sem-forbidden anyway (used to mean a driver)

2. relative path names on dos or unix have .\ or ./ else they must be absolute or
something

Mac names do not have to have this: how can we handle this?
	will use flag mightBeMac
if we determine the string is not mac and we have mightBeMac return a  0 string  

3. We will append a : to beginning of mac paths this means a dos -> mac or unix
to mac will shorten string and mac->dos and mac->unix will lengthen string

4. if names in one format have destination path separators in it convert to 

5. all macpaths must begin with : no as we can detect a trailing or embedded one ?


990718

1. on 1, 2 and 5  above discusiions with andy indicated that all paths must begin
with :, ::, ./, ../, .\ or ..\ so those points are invalid. Mac paths starting with
character are absolute paths. [AD2000-02-15] However Unix paths are the other way 
around - absolute begin with / and relative with the name of a directory file in the 
current directory (ie: a sub-directory).

As a result wrote a function (isRelativePath()) that determines if the path is a
valid relative path. this this calls another function getPathType() which returns
a enumerated type for the type of path this second function also is used in the
logic of the parser functions

2. Added a method describePathType() which converts an enum path type to a
descriptive oofstring for debug and testing.

3. all conversion methods were based on the idiom used for the line ending interfaces 
in oofstring, where you can explicitly convert a path to a given platform or
to the current platform (current defined by #ifdefs)
however an extra parameter was added bool appendSeparator (default = true) to
allow the parsing of path names ending in filenames without appending a missing
path separator
ie :blah -> ./blah. appendSeparator = true
	:blag:oofts49.cpp -> ./blag/oofts49.cpp appendSeparator = false
	and not :blag:oofts49.cpp -> ./blag/oofts49.cpp/ appendSeparator = false

4. before calling a	relativePathXXX() a user should call isRelativePath to check
that path is valid since all relativePathXXX() calls have an
assert(relativePathXXX()) at the beginning for protection.

5. if relativePathXXX() is called on a path of the same type it will: 
- return 0 if appendSeparator = true and the original path has a terminating
separator 

- return  a new string if appendSeparator = true and the original path
has no terminating separator 

- return 0 if appendSeparator = false and the original path has a terminating 
separator
 
- return 0 if appendSeparator = false and the original path has no terminating
separator

2000/02/15
Revisiting these designs when changing openConnection et al to use oofFileRef
and use path conversion.

I'm not happy about these conversion functions being in oofString as a base
class - if you don't handle file paths then you don't want them.

As a trivial change to pull out these methods - put an oofPathString class
in the ooffiles.* source package.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/08/08
VISITING DIRECTORY TREES - HANDLING THE ROOT

A little more experience has shown a need to visit the root of the tree
because application code was often recording the root externally.

This should be added in a manner that doesn't break existing logic so
can be used only by subclassing. Thus the calling mechanism will be 
buried within visitFiles but the default is for a successful entry.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/08/15
FIX CRASH IN INVARIANT DUMPS

Problem reported with invariant dumping whilst linked with a debug library like DebugNew.

Stack trace:
 03972DD0    PPC  0B842068  dbTable::dbTable(const dbTable&)+00124
  03972D80    PPC  0B7F643C  OOF_dbaseBackend::clone(dbTable::selSharingT, dbTable*) const+00
050
  03972D30    PPC  0B7F2CA8  OOF_dbaseBackend::OOF_dbaseBackend(const OOF_dbaseBackend&, dbTa
ble::selSharingT, dbTable*)+00030
  03972CE0    PPC  0B815834  OOF_simpleRecordBackend::OOF_simpleRecordBackend(const OOF_simpl
eRecordBackend&, dbTable::selSharingT, dbTable*)+0017C
  03972C90    PPC  0B818F44  OOF_simpleRecordBackend::setSortOrder(dbSorter*)+00024
  03972C50    PPC  0B828BCC  OOF_simpleRecordBackend::Invariant(const char*) const+0000C
  03972C10    PPC  0B828D80  OOF_simpleRecordBackend::InvariantDump(const char*) const+0015C


We are clearly in the dbTable copy ctor and mBackend has not yet been set, so
dbTable::isDirty() fails. This is an awkward order-of-execution problem that
couldn't be fixed easily, except by locking out Invariants called during table
initialisation (check dbTable::sCurrentlyConstructing). I'd rather avoid this if
possible.

Happily, in this case, we can simplify the Invariant as mTable->isDirty() is a
simple forwarder that can be replaced by mDirty!


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/08/15
STORE ADDITIONAL FIELD LABELS IN XML SCHEMAE - FIX BUGS READING REPORTS

The basic problem is that our encoding into XML-legal element names is not wholly
reversible. This has caused some issues with report titles (other than 
where overridden) and particularly graph titles including % symbols, that come directly
from field names.

One alternative is to rewrite the name encoding so it is legal. 

Rather than disrupting the current encoding and database schema recreation logic,
it seems simplest to try adding an extra attribute to the schema, used to set
the field name.

However, test implementation shows that the point at which we read these names
is much earlier than other logic which needs to perform lookups (eg: "source" 
attributes on report bands).

A fairly grotty solution would be to create some deferred cleanup objects to
reset the names after all processing is complete. However, this violates a
separation in OOFILE - the schema writing/parsing and data parsing with XML
is core logic, ignorant of report-writer needs.

DECISION - go back to the simpler yet more fundamental approach of different
encoding for element names.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/09/05
FIX NON-INDEXED JOIN TRAVERSALS (WITH NEW RECORDS IN CACHE)

Initial setup causing middle table in MN relationship to contain new records
CStudentInformation::DoLinkStudentToStrand (application logic) 
	dbBrowseHelper::appendSelection
		dbTable::appendSelection
		- setSelection(...
		- broadcast(OOFmsg_AppendSelection...
			dbRelMaintainer::receiveMsg
				OOF_RelMN::appendSelection
				- creates new records in middle table 
				- copies join values via dbRelRefBase::updateRelValue
 
 
The refresh calling tree
CStudentInformation::DoClear (application logic)
- removes some records from selection then refreshes via 
	dbRelRefBase::relateRecord
		dbRelRefBase::relateFromRecord
			OOF_RelMN::RelateMN  ( on middle table)
				dbRelRefBase::relateFromRecord ( from LHS pointing to middle table)
					OOF_simpleRecordBackend::loadRelatedContextJoiningFromTo
						OOF_simpleRecordBackend::searchEqual
						- OOF_simpleRecordBackend::selectAll
								OOF_simpleRecordBackend::unloadRecord
								- unloads new records
							OOF_simpleRecordBackend::SearchEqualNonIndexed
								dbChar::asChars
									dbChar::operator const char*
										OOF_simpleRecordBackend::getFieldReadFrom
											dbTable::ensureRecordLoaded
											**** goes into endless loop triggering joins due to current state
											
											
More thoughts:

There is a specific client problem due to the way that MN relationship are
implemented - we "append" new records to the RHS of an MN relationship by
creating cached new records in the link table.

If a forced update of the RHS is performed by relateRecord, then we have a clash
of history.

The original intent of relationship traversal was that it only occurred when the
most LH record changed - invalidating the entire chain. Thus there are
assumptions all along about what is OK to do, including dumping cached records.

ALTERNATIVES
1) use 3 different caches - new, modified and deleted records. The current cache
clearing logic will thus only affect modified records. This minimises code change
flow-on. This will also fix the issue of deleteing related records being immediate.

2) add logic to protect a cache containing only new records around critical
relationship traversals.


ISSUES
Option 2) has been tested with a trial implementation in the lowest level of
relationship traversal:  OOF_simpleRecordBackend::loadRelatedContextJoiningFromTo

This avoids the crashes but unfortunately the cache is unloaded because dbRelRefBase::relateRecord
broadcasts an OOF_SelectionChanged which is picked up through dbRelRefBase::receiveMsg. On close 
examination, this is WRONGLY broadcast from the base table object - all we need to refresh is
the far RHS so is triggering an unload unnecessarily.

DECISION - as we have a quick working solution with option 2, use it for now on the basis
of minimal change to how the core caching logic works as it is much safer this way.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/10/13-14
MAKE INVARIANTS LOWER OVERHEAD WHEN ASSERTS STILL ACTIVE

Initial thinking was that some clients still want asserts in release builds but
may not want OOFILE Invariants to be evaluated, either because they are 
passing through invalid states or perceived overhead.

I thought it worth using OOF_Debug to define whether invariants are active,
rather than adding yet another macro.

An initial implementation had the Invariant disabled by having an inline version 
returning true. However Windows builds proved troublesome, with apparent definition of both
occurring.

Accordingly, conditional compilation is used only in the body of the Invariant.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/09/21
FIX INCORRECT BLOB HANDLING WHEN BUFFER FOREVER
Problem occurs in ooftest 47 when buffer forver is and dent record was set.
debugging showed need for ability to look at data base state from outer level, 
currently invariant dump onlu works within backends
so added 
 
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

99/09/23


RESET BLOBS CALL IN OOFRAM BACKEND

In CLASS	OOF_ramBackend Method	LoadRecordAtOffset against  ResetBlobs() 
Andy said, "why does RAM backend do this after load - others don't"
The answer appears to be that as the blobs are already in ram why cache another copy of them, especially as they are potentially large.
However with the cacheForever option now included we need the blobs in the cache,

DECISION - As the reset blob call interferes with the  cache forver functioning in the ram backend
remove it, this is at the expense of memory consumption
 
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

99/09/23

INVAR STATES 

if Tablevalid state is closed we do not care about the other states so 
at the beginning of invar do

IF tableValid = eclosed) THEN
	return	TRUE
END_IF

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/09/26 
FIXING INCORRECT BLOB HANDLING WHEN BUFFER FOREVER

Problem was in caching of blob storage, need to see state of mstorage so will add describeBlobs and
blobState methods, used 	describeBlobs instead of desribe  as the call is fld->blobState() and desribe
method already exits and dumps the field state this name is not ambigious
		
 debugging showed need for ability to look at data base state from outer level, 
currently invariant dump onlu works within backends
so added 

Problem tracked down to CLASS 	
	OOF_simpleRecordBackend
	METHOD	CachingContextChange
		- in this method if the roecord is dirty the blob is cached, however the blob storage is not reset.
		if storgae is then still marked as loaded and dirty and so the next record sees this old blob instead o
		loading the correct blob.
	 Added a ResetBlobs call inside the  isDirty() if statement
 
	DECISION - Add a BlobReset call to clear storage

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/10/03 
FIXING ASSERT BUG in dbase baqckend 

Problem was found in ooftest 47, loadBlob was called which only had an assert(0) call, other backends
had actuall load blob code. dBase backend assumed that blobs were loaded by load record at offset and
so loadblob was unimplemeted. 
However with buffer forever mode blobs were not cached if not changed and when a record was loaded from cache the 
blob was reloaded from disk, so loadblob was required and called


DECISION - implement loadBlob in dbase

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/10/04 
DBASE  DB IV BLOB BUG 

While reading code in preperation for addition of DB III blobs it was found that db IV code did not actually
save chages to blobs where block size had not changed. Tests confirmed this.
DB IV memo format allows for file space reuse if blob is deleted or grown. Current logic does not do this
 
DECSION - Fix save change bug
				- Defer full reuse handling for now

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/10/04 
DBASE  DB III BLOB HANDLING 

When a new blob is added, it is apphended to the end of the file, 
thte following algorithm will handle existing blobs

load record at offset measure the length of the existing blob
	so in loadRecord at offset
	when  blob len is found set mBLOBBlocks[BLOBnum] to number of 512 byte blocks used by blob


Note file format book states that db II memo are terminated by two bytes of 0x1a foxbase only apphends two 

so we will write out two but only read one 


Note the fileformats book states that memo terminator is two bytes of 0x1a, tests show that foxbase only
generates one terminator byte, we will generate two but on read back will only look for first terminator
to maintain comaptability with fox base 

when calcualtinf memo foeld size add two to take into account the terminator bytes 	

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/10/21
DBASE LAST WRITE ACCESS 

The file formats book states that the dbf file header has the last date write access stored in bytes
01H 02H 03H in binary form and YYMMDD.

This was not implemented so added code to savecontext method.
origonally  i was going to add this to writeBuffer(). However on writeBuffer entry the file postion 
is set to to  the current record to add date code there would require that file position be read
and restored.

A check of code showed that SaveContext after calling writeBuffer seeks to file header and 
writes number of records, so this is the natural place to place the date code.

We could flag a write has occured and write the header when the table closes, but this is 
less robust and if the dtabase is open over several days it would only record the day of shutdown 
and not last write 

Initially decided to add a have Modified member to the dbase backen that is iniliased to false and 
allows one of headerwrite of the modified date. this model will fail if the database is left running continuaslly over a day 

NOTE 
 -As the header wrtie modication date is strored in one byte only we will have to handle the 1999 to 2000
 rollover will use dbDate::adjustShortYear to handle this and windo correctly 

DECSISION


we must read header date on connect and store i, and on each write check if current date is 
is greater than mlastmodified, if so update header
 
 -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/11/4
ILLEGAL INVARIABLE STATES

unloaded valid dirty is an illegal combination of states.
BUT Dirty becomes a "don't care" for unloaded records.  we have seen unloaded valid dirty all
However we should keep this illegal and correct OOFILE to clear Dirty when unloading.

 DECSISION
 set mdirty to false whenever mRecordState is set to eUnloaded. Note this is already done in
 OOF_simpleRecordBackend::unloadRecord
 
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/11/7
VALIDATING DEPTH

This code is needed for the invariant calls as we have had a serious bug on relationships that
involved a recursive validation death spiral. the instrumenting code was inserted then removed as ealier 
invariant code did not monitor depth.
 
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/11/8
INVARIANT STATE LINE 

LINE loaded invalid not dirty empty is an illegal state as we cannot have a loaded record from an
empty selection, howver this state is observed in oofile. Investigations showd it to be a transient state
his may be a weakness in the model!!!!
This appears to be anothertransient state

OOF_ctreeBackend::clone(dbTable::selSharingT,dbTable*) const
	OOF_ctreeBackend::OOF_ctreeBackend(const OOF_ctreeBackend&,dbTable::selSharingT,dbTable*)
		OOF_simpleRecordBackend::OOF_simpleRecordBackend(const OOF_simpleRecordBackend&,dbTable::selSharingT,dbTable*)
			OOF_simpleRecordBackend::setSortOrder(dbSorter*)
when SetSortOrder is called the table is in state LINE and the invariant call sees LINE
in the contructor the following code exists

if (rhs.sorter())
		setSortOrder( rhs.sorter()->clone());		
	if (selSharing == dbTable::selNotShared) { // explicitly called from cloneTableWithoutSelection
		mRecordState = eUnloaded;
so afterthe SetSortOrder the table will be in the legal UINE

DESCSION move  mRecordState = eUnloaded 
to before the SetSortOrder call

ser sort order dose not needwhat to do o n tramsients

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/11/8

INVARIANT STATE LVNE

LVNE is an illegal state this was found to be due to thefollowing call stack
dbTable::deleteRecord()
	OOF_dbaseBackend::deleteRecord()
		OOF_simpleRecordBackend::selectNone() if selection has one record set to none as the one is being deleted 
			OOF_simpleRecordBackend::unloadRecord()
				OOF_simpleRecordBackend::Invariant(const char*) const
		it should then call mSelection.selectNone();
	
The invariant is caled on entry to unload so we end with a transient LVNE

Now uload record effects the following flags: 	mCurrLoadedRecOffset;  mRecordState; mDirty then restes blobs
ResetBlobs() & does not broadcast changes.

selectNone() calls drop selection and sets state to empty then calls
	OOF_recordSelectionRep::DropSelection() which sets mstate to empty, decrements mRecsRep->mReferences
	and contidtionally deletes mRecsRep & does not broadcast changes.
		~mRecsRep does not broadcast changes
As a result of no changes being braodcast we can just swap the order of methods:
		selectNone
		unloadRecord so that the record will be unloaded before invoking selectNone so that LVNE will not be seen
 
LVNE also occurs in 	stSaveSelection::~stSaveSelection()
	
looking at the following stack traces we can see why

dbTable::setSelection(const dbSelection&)
				dbTable::setSelection(const OOF_Selection*)
					OOF_simpleRecordBackend::setSelection(const OOF_Selection*)
						-- sets selection back to previous state (empty) now have LVNE
						
next 
		dbTable::setSelection(const dbSelection&)
			dbTable::setSelection(const OOF_Selection*)
				OOF_simpleRecordBackend::setSelection(const OOF_Selection*)
					dbTable::start() 
						dbTable::ContextChange()
							--unloads record and state becomes the legal UVNE


What we need is some means of signaling a transient illegal state, (orr else we have to radically rewrite
oofile, which might not work any way)

PROPOSAL
1. Assumes only one transition at a time 
dbTables have a newmeber mTransitState of type enum, the enum types will indicate the source of the transition
eg	eDelRecordTrans, eStSaveSelectionTrans

When entering a transition phase, mTransitState is set 
the invar code then checks the transition state, if it detects an LVNE state it acts approrialty


When leaving a transition phase mTransitState is set to eNoTransition




2. if we are in two transition states, big trouble is possible but we could handle this if 
used an unsigned int for mTransitState and use each bit position to indicate a transition 


DECISION
	FOR NOW COMMENT OUT THE LVNE LVDE INVAR CODE
	for now comment ouy the LVNE invar handling code and investigate transient problem further  
			
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/11/11

IMPROVE SORTER STATE CODE

The sorter state code build the state variable by using a union of a string and
unsigned long to build the state, this is prone to endian problems and is
somewhat clumsy, better to build the state variable with bitwise or.
 
 
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/11/11
IMPROVE IMPROVE BACKEND STATE CODE

The simplerecordBackend state code builds the state variable by using a union of
a string and unsigned long to build the state, this is prone to endian problems
and is somewhat clumsy, better to build the state varraiable with bitwise or.
  
  
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/12/15
ADD VIEW CLONING FOR INDEPENDENT CLONES OF RELATED VIEWS

There are two distinct reasons why you might want to clone a dbView.

1) (as used in the GUI) you want to iterate a list of records without 
propagating changes in the current record. For example, just refreshing a 
list as scrolled will require loading sequential records.

2) as used in the report editing - we want to save a dbView as a subset of
both records and fields. Thus the selection is NOT shared with the original.


We don't want to change the current behaviour (although there are highly
suspect incongruities with related data) because that may break GUI 
logic depending on it.

However, we do want the ability to clone a view as in 2).

Considering just the cloning side, use of default params allows us to extend
the current dbView::clone behaviour to clone an independent selection.

Being an explicit operation we are making it clear that this breaks the 
automatic update of the selection via the relationship - it's a snapshot!

However, testing reveals a further problem, akin to a race condition. Our lazy
instantiation of selections means that the table being cloned (if a related table)
fails to have the correct selection before we clone it.

We need a way to clone these tables and force their selections to be valid.

Inspecting KIDMAP user code reveals a couple of places where they would also
be vulnerable to this issue.

This instantiation needs to be forced without changing the constness of 
cloneTableSharingSelection and cloneTableWithoutSelection which are used
heavily on const tables.

Alternatives
1) provide a second const method and so the public cloneTable can be non-const
and update the selection

2) force user code (and our dbView code as a client) to explicitly update the 
selection on the table before cloning

DECISION 1) as it is safer - 2) allows user mistakes to have a serious effect
on database selection validity.
 

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
99/12/16 
BUFFERING DIRTY RECORDS AND unloadRecord CLASH

The model for the dirty record cache is that the cache is an alternate data
source. It contains modified records which have not been saved to disk, for the
current dbTable. When we load a record, we look to see if there's a modified
version in the cache.

It is possible for a selection to skip some records in the cache.

There are two common cases when the cache is used:
1) with related tables, they have a saveOption of requireExplicitAndBuffer and so
   modified records are cached until a cascading save from the parent saves them.
   The cache may also be flushed by a cascading unloadRecord, without saving.
   
2) with the bufferForever mode (as used in the report editor) we keep records in 
   the cache until the save option changes and the cache is flushed

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2000/01/19 
RAMP SAVING FILES

Previous fixes to oof_rampBackend::close seem to have broken saving to file at times other
than closure. 

dbConnect_ramp::save is calling ooframpbackend::close  Which saves table to file.
So will add missing OOF_rampBackend::save method, in dbConnect_ramp we must cast
theTable->mBackend to an OOF_rampBackend* this is not nice, but is reasonably safe
as the cast is within a dbConnect_ramp so the backend must be an OOF_rampBackend.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2000/02/15
USE FILE REFS IN OPENING DATABASES

Currently we pass in a name to openConnection or newConnection or, on
the Mac, also allow the FSSpec structure (returned by file selection dialog)
to be passed in.

Why change?

There are two problems.
1) current approach is unable to indicate that an FSSpec points to 
   a directory rather than a file. If a user implements a Choose Directory
   dialog, returning an FSSpec, to use with dBase files, then we have a problem.

2) It's annoying having to maintain a platform-specific pair of methods
   every time someone adds a database backend (admittedly an infrequent occurrence).

Other issues:
a) we want our relative path handling code incorporated at some level so 
   a single path name can be passed in and work across platforms.

b) it seems a little ridiculous for openConnection and newConnection to have
   this much duplication of code when the essential difference between types
   of connection (or c-tree Plus modes) is whether there's a single database 
   file or directory of database files.
   
c) if trying to iterate a directory structure and thus getting oofFileRef's
   it's a minor nuisance having to convert them to paths and then (on the
   Mac) having costly path-parsing code executed when the oofFileRef by
   itself is unambiguous.
   
Because of all these problems it is proposed to change to using oofFileRef
objects as the sole way of specifying a database or database directory to
open/newConnection. Having a form that takes a const oofFileRef& will allow
a temporary oofFileRef to be constructed from existing code that passes in
path names or FSSpec structures so there will be no need to change user code.

On further consideration there may be times when this is not appropriate.
In particular, these changes need to be deferred until the issue of how 
Faircom Server support is integrated. There is also the case that the typical
use of dbConnect_ram requires a default ctor which has been satisfied by being
able to pass in an empty name.

For now, we'll just settle for being able to cope with paths specified using
a different platform.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2000/02/17,25
USE XML STANDALONE READING AND WRITING WITHOUT OOFILE DATABASE 

The goal is to be able to use OOFILE's utilities for writing XML without
involving databases classes at all.

Ideally you should be able to link a minimal oofStr, oofArray and oofXML.cpp
and little else.

When using in a minimal manner paired with expatpp one immediate issue is
whether iostreams need to be linked.

For a minimal build this is appealing but for now, pulling iostreams out
of oofString is difficult and oofXMLwriter has them as the inherent output
format.

DECISION - leave streams in!

Looking at the classes in oofxml.cpp they are already well-factored in their
database dependencies. The only static utilities of more general use might be
oofXMLwriter::encodeName and decodeName which are used for writing encoded names
as tags and attributes.

These methods could be pulled into oofTagMaker which has a similar domain of
application.

DECISION - pull useful encoding utilities into oofTagMaker

oofTagMaker::makeSerializableElement presents a minor problem of dependencies. 
It is similar to other oofTagMaker methods but pulling it in implies we must
also define oofSerializable. As oofSerializable is a lightweight interface class
this doesn't present much problem. The problem is with balancing the input with 
the output side - I believe our standalone XML package shouldn't write something
it is incapable of reading.

To parse serializable elements we have a oofXMLserializableParser which knows
how to read their details. This parser dtor invokes a factory function via
oofSerializableFactory and this is where the dependencies come in.

oofSerializableFactory currently uses an OOF_Dictionary to provide ownership
and cleanup of the registered factories. Pulling OOF_Dictionary into a source
base invokes the OOFILE_PublicBase base class for the persistent items.

ALTERNATIVES
1) factor OOF_Dictionary out of the existing code base

2) as we are only using the member-deleting aspect of OOF_Dictionary in our
   sFactories dictionary, write our own array class used by oofXMLserializableParser
   so it can be included in the base XML toolkit.

3) Ignore the serializable stuff altogether in the XML toolkit and take it out
   of oofTagMaker etc.
      
Option 1) involves a lot of changes to base OOFILE code and fiddling around.

Option 2) is a cleaner separation writing new code to isolate the function.
However it is complicated by a race condition that saw oofSerializableFactory::sFactories 
put in oofrep3.cpp. Also, anything relying on statics is a bit suspect if we want to
use the oofXML kit in embedded code.

Option 3) involves the least code change but loses a valuable function from the
XML toolkit for a minimal saving in time. It offers no significant code safety benefit
over 2).

DECISION - 3) is simplest.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2000/03/23
EXPORTING ASCII CHARS GREATER THAN 0X80 IN XML

Note technically ASCII is not defined above 0x7f - there will be cross-platform issues
with fonts if users store reports or other database data in high ASCII.

brunot@mercator.com.au found a bug in which 0xbf () was being imported and displayed as 
(0xc2 0xbf).

Initial investigation showed that HTML encodes a char in number form as
&#XXX; where XXX is a decimal number, howver XML should use the form
&#xZZ; where ZZ is a hex number.

We were using the html number coding scheme. 

oofstring conversion routines tested and worked but same error occured in xml decoding

investigation revealed this is because 
xpat converts parsed text to utf8format.
 From James Clarks'  notes:
	"By default, expat assumes that documents are encoded in UTF-8. In UTF-8, ASCII 
	characters are represented by a single byte as they would be in ASCII, but 
	non-ASCII characters are represented by a sequence of two or more bytes all 
	with the 8th bit set. The encoding most widely used for European languages 
	is ISO 8859-1 which is not compatible with UTF-8. To use this encoding, expat 
	must be told either by supplying an argument of "iso-8859-1" to XML_ParserCreate, 
	or by starting the document with <?xml version="1.0" encoding="iso-8859-1"?>. "

Testing has shown that if XML_ParserCreate("iso-8859-1") is invoked
then xpat converts latin1 to utf-8. We cannot seem to make xpat return
plain 8bit ASCII data so will have to add a method that converts a
string with utf-8 encoded bytes to 8 bit Mac Windows ASCII So we must
decode the utf-8 back to Mac or Windows (latin1) world.

utf8 8 codes chars as follows
char < 0x80  output char
char > 0x80 output two chars as follows
eg
	given char = stuvwyz output = 10000st 10uvwxyz 

so will make code convert from uft by adding a method
utf8To8BitAscii to class oofString


To make this work in xml inport export will add  calls to this new
method in oofXMLdataParser charData method AND OOF_XMLrepLayoutParser
charDat.

2000/04/13
IMPORTING ASCII CHARS GREATER THAN 0X80

Oops origonally wrote  utf8To8BitAscii with same pattern as oofString::decodeEntity where input parameter is an oofstring.  
Then  added a version with a char* parameter as initially thats what xpat appeared to return. 

Testing found that xpat returns a char * with no null termination, rather it returns a count of chars
so wrote a third version that takes a count and char*, left the others for future general use.

The char* ,count  version's algorithm is correct. the origonal two versions  the others did not work properly as they were bases on the niave assumption that 
the encoding was X whwre x is origonal unecoded char (thats what was seen in origonal samples).
Unfortunalty forgot to change the other genral purpose converters, as did not have atest case for it wasusinf sample reports

it is betterto have the speicialised char * count version to interface to xml rather than convert copy the token string to an oofstring then
call the conversion as this would result many unessary copies occuring.
DECSIONS

Remove char * version 
Fix ooftsring version fix ooftest61.cpp so it checks all possibilities  also fixed a bug in ooftest 61 had &quote; instead of &quot; 

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2000/04/13
IMPORTING ENCODED CHARS

while checking code realised that while xml uses hex notation html use decimal notation, as we will be working on html forms soon.
So added code to handle both forms eg &#x24; or &#36;

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
00/05/11
CARBONISING CODE
ConditionalMacros.h has a define
 #define TARGET_API_MAC_CARBON 1
 so base conversion will involve wrapping code in #ifdef (or #ifndef)
 In particular will  wrap all working directory code into
 #ifdef  TARGET_API_MAC_CARBON, so as to maintain backward compatability




-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2000/05/15
OOFDIR BUG

in the following calls stack(
oofDirectory((":fee:fi:fo:fum:","",true)
	setDirFromPath
		CreateSubdirsAndWorkingDirID


Testing found that CreateSubdirsAndWorkingDirID (carbn version) was failing when with an error -35 no such volume  
::FSMakeFSSpec(mVRefNum, mDirID, nil, &fileSpec) as the3 mDirID was 
this is because mDir has not been set so set it


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2000/05/15
CARBONISING CODE

Will replace calls to CreateWorkingDirID with a generic createDirID
this will call CreateWorkingDirID for noncarbon and
	CreateHardDirID for carbon. This means code will make more sense.

have left mWorkingdir member and associated initialisation code in oofDirectory code for now
as ti ifdef them out would add noise to the code and just make itharder to read and comprehend.




-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2000/05/22
BUG IN CARBON CreateSubdirsAndWorkingDirID

theVRefum variable is used to hold iether a volume ref number or dir id number this makes code hard to follow (especilayy as wd id can
be passed to  vref in non carbon code) so make a new variable thDirID for clarity.

When we create a new directory (or nested set of directories we must set mDirID to the dirid of the created directory. to 

the AndWorkingDirID  is a misnomer  because this really only creates subdirectories, we would
be better to call the function CreateSubdirs and for the noncarbon which uses wworking directoies kust make CreateSubdirs
a wrapper around CreateSubdirsAndWorkingDirID  

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
SYSTEM TASK IN FAIRCOM CTCLIB.C (Not actually an OOFILE fix)
00/05/22

This function is not supported in carbon so wrap it in ifdefs

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
CARBONISING COPYTODIRECTORY
00/06/05
Jeff Barbose found that copyToDirectory uses newptrSys calls which are not supported in carbon looked at 
reason they wre missed, only thing i could find is that Jeff is using later version of catbon and maybe cw.
could not find any reasons in diary why newsyspoite  (which allocattes memory from sys heap)  was used.
Instead will convert to newPtr that allocates o will convert to newptr whichallocates from application heap.

Also as per Jeffs email scanned oofile and found a few old pbcalls that slipped past my cw pro6 beta and carbon sdk which i think are ealier than his
will hand check code with seraches in omu.

This found the following need to be changed
	COPYTODIRECTORY
	OOF_CheckPath
	OOF_SeTPath

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
OOFMASK TOKENISING ading fmp compatablity
00/06/27

Looking at append target it appears that it finds chunks deleinited by oofstring inTarget,
this means it is effictivly a tokeniser.
we want to detect cdml commands wrapped into "[fmp" & "]" clearly we need a seconf delemiter 

it assumes chunk|replacement point|chunk... was more flexible.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
DBASE BACKEND OPEN OR CREATE
00/08/15

if fopen return 0; file did not open will assume it was not there and i
openorcretae was used create it

bool 
OOF_ramBackend::openTableInConnection(const dbConnect*,const bool createIfMissing) 
{
	return createIfMissing;
	// null method - no physical table to open
}

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
DBASE BACKEND OPEN OR CREATE
00/08/16

will move the state code that kkeps track of open and cretae state from the dbConnect_Ctree to dbCoonect so that dbase can use it

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
RAM RAMP BACKEND OPEN OR CREATE
00/08/16
for symmetry with other backends will implement openTableInConnection for ram & ramp
will just return the value of the createIfMissing argument to stop warnings will not actully do anything
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
CARBONISING DBASE
00/09/00
dbase code used the mac call create() to create files this is unsuportted in CARBON instead must use FSpCreate
so add carbonised functionality. For now will put this call inside #ifndef TARGET_API_MAC_CARBON
will check later to see if we can dump support for create.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
OOFSCHEMA EXPORT
001006

the method
operator<<(ostream& os, OOF_fieldTypes ft)
emits  "compound" for types fixedBinaryField & boolField
this means the dump text is ambigious and we cannot rebuild scema from the describe method output
 text, so will change strings to "fixedBinary" and "bool"

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
OOFSCHEMA EXPORT
001018

CLASS 	dbConnect_ramp
Method	writeOOFSchema
	-renamed to writeRampSchema
	
CLASS 	dbConnect_ramp
Method	writeOOFSchema
	made new functionlity, sets mWritesOOFSchema true

CLASS		dbConnect_ramp
Method	dbConnect_ramp
	added initaliastion for mWritesOOFSchema
	
CLASS dbConnect_ramp
Member	mWritesOOFSchema
	- added it


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2000/09/14
ERRORS DELETING TEMPORARY DATABASES

Two problems have been identified with temporary databases.

In at least one circumstance with a ctree backend, a file didn't appear
to have been created. It appears if you never write data to a database 
that the files are not guaranteed to be created, with the useSeparateFiles
model.

Thus it seems it is not an error to try to delete a file that is absent
and we need to rev oofFileRef to cope.

Options:
1) rely on throwing a different exception
has problems if no exceptions enabled (eg: early GCC)

2) always allow missing files

3) add an optional parameter that indicates if file missing is an error,
   defaulting to no error, so users can choose to catch the error
   
DECISION - 3   


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2000/09/14
USE OF DATABASE DIRECTORY WITH RELATIVE PATHS
On Mac all paths are effectively absolute as they are converted into an FSSpec.

On Windows and Unix we store the original path string.

This causes problems with long term directories that are used after the 
program has changed current directory.

For example, with the c-tree Plus backend, we change directory to the 
c-tree file directory to open a database. Deleting a temporary database
then fails as the directoryDatabase() returns a relative directory.

Options:
1) blindly convert all incoming paths to an absolute path

2) allow user override of 1) with an optional parameter

3) add a method to convert path to absolute so we can call it when 
   needed.
   
This problem has been exacerbated by main development on Mac as it's not
been noted and there are no regression tests which fail on Mac (test 50
fails on Unix).

It is a subtle and dangerous bug to find. Whilst some people may have code
that is damaged by changing the default behaviour of oofDirectory, I think it
is safer than relying on explicit use of the conversion in future.

DECISION - 2)
   

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2000/11/09
ENHANCE WORD SEARCHING TO MAKE EASIER TO INTERFACE TO GUI

Work on a local project with flexible querying has made it attractive
to be able to couple an oofWordParser directly to a query.

We already have table and field-level word searches taking either
a delimited string or array of c strings. The parsing of these has
a start/more/next/current() interface which maps well to oofWordParser.

Alternatives:
1) using the current searches have the oofWordParser write out
   a compatible data format
   
2) implement a query that wraps the oofWordParser, forwarding calls

3) unify the interfaces of oofWordParser and the query so we can use
   the parser object directly.
   
   
Option 3) is possibly more elegant but introduces some coupling between
what are currently separate packages.

Option 2) has coupling in that a wrapper class (a Facade pattern) is 
aware of the oofWordParser interface but seems easier to develop and allows
for oofWordParser to evolve independently. However, attempts to develop
revealed searchSelContainsAnyOf and other methods where reimplementation
would be required.

Option 1) has total decoupling but intially appeared overhead.
However, it avoids repeated calculation of the words. This is not a problem
in the indexed searches but would be very inefficient in the searchSel
methods.

DECISION - 1)     


SUB-ISSUE - MANAGING A GENERATED ARRAY
To implement this decision above we need a way to translate an oofWordParser
into an array of strings.

It seems possible that a given parser may be reused quite often if for example
search interface allowed choice of word vs fragment matching, search in sel
etc.

Thus it seems useful for the generated list to be persistent.

There is also a minor exception handling issue which if we use local arrays
makes a lot of code more expensive.

OPTIONS
1) generate local arrays in each search code, including exception safety

2) put array generation into oofWordParser and retain state so can be
effectively cached.

DECISION - 2)


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2000/11/13
SETTING RELATED FIELDS FOR POINTER RELATIONSHIPS

We need a way to identify that a given record (ie: OID) is to be 
the related instance for a dbRelRef (is this too narrow a case to 
consider right now?)

A common scenario is in ooftst53, specifying the M:N relationship record
between two existing records. (Note: specifying related records
successively from the LHS is already catered for.)

Because we are adding an instance related to the LHS dbPerson we
get OID propagation from that record. We need some way to retrieve
the value from the far RHS table instance without triggering cascading
updates (ie: trying to make the current table valid when it has not
had enough data saved to become valid!).

There is a possibility we have robustly obtained the OID of the record
we want, so simply calling setOID() on the dbRelRef to the RHS will be
sufficient.

In many other cases, a search expression could be used to change the
RHS set to a desired record. We want to make sure this is robustly called
rather than working just by happy accident of current record state.

It would be nice to have a more robust way to get just the oid back from 
a search expression than the overhead of cloning a table but the main
priority is avoiding side effect. Occasional triggering of the assert
in OOF_recordCache::~OOF_recordCache is worrying.

Options:
1) temporarily change table status so it doesn't think it's a dependent
LHS clone, to suppress side effects of searching.

2) clone the table, search the clone and stash the resulting oid

3) add a suite of new search clauses that return a single oid

4) force user code to do 2) or some equivalent, only calling setOID

5) same as 2 but dbRelRefBase can retain a dbTable clone for these 
   operations. That cuts the overhead out dramatically so we have 
   cloning just once and then if the same field is hit repeatedly 
   there's no overhead.
   
The main danger is that naive user code will do something like a 
search on a related table, not realising that under some circumstances
that may trigger attemps to update from the LHS.

Option 2) is annoying in its overhead but most likely to be rapidly
achieved. Providing a call to wrap the mechanism of search parsing means
we can choose to avoid cloning in future.

DECISION - 5) at dbRelRef level as don't need it for Set.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2000/11/13
COPING WITH SEARCH SPECS ON OTHER TABLES
The above solution for related fields raises an issue which has
happened before.

We have an uneasy situation where sometimes a search clause
can be regarded as purely declarative, and we would not be bothered
by the actual table used in the spec, and times when it is important.

For example, People->search( People2->name=="Andy");
is a simple declarative search. It seems intuitive that the People2
database is just indicating the field and we would really use the
field instance People->name for the search.

However, take a family tree example, where People->Father is the same
class, in a recursive relationship:

People->search ( People->Father->Father->name=="andy");

In this case the immediate table in the search clause is of class dbPeople
but not the correct instance.

OPTIONS
1) give up on allowing related specs as in our earlier fix

2) require more specific search criteria and have error trapping 
   for the current situation (which spirals into an endless loop)
   
3) add more intelligent parsing for search in the backend to cope
   if tables are not that table being searched. To cope with recursive
   relationships we need to check the endpoints of the search clause,
   the People at the left and rightmost Father in the example above.
   

DECISION - 3) fixes a bug which users may also encounter.

LATER
Attempting to do this doesn't fix the bug we encounter in our specific
case because we have a related table situation.

An easier specific is under our control without the generic fix. We can
change the search clause to be on our relRef's cached search table.


-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2001/2/11
BLOB POINTER API'S AND MULTIPLE PLATFORM COMPILING

In the past we changed most of the BLOB pointers to char* from void*

Under Unix we get the following errors with void*
./source/core/oof3.cpp: In method `dbBLOBstorage::~dbBLOBstorage()':
./source/core/oof3.cpp:1850: warning: `void *' is not a pointer-to-object type
./source/core/oof3.cpp: In method `void dbBLOBstorage::adoptBody(void *, long unsigned int)':
./source/core/oof3.cpp:1922: warning: `void *' is not a pointer-to-object type
./source/core/oof3.cpp: In method `void * dbBLOBstorage::allocRoomFor(long unsigned int)':
./source/core/oof3.cpp:1938: warning: `void *' is not a pointer-to-object type
./source/core/oof3.cpp:1943: warning: `void *' is not a pointer-to-object type
./source/core/oof3.cpp: In method `void dbBLOBstorage::convertLineEndings()':
./source/core/oof3.cpp:2006: warning: `void *' is not a pointer-to-object type
...
g++ -o OOF_Test30 ooftst30.o oofile.a -L../faircom/lib.fpg -lctreestd 
Undefined                       first referenced
 symbol                             in file
dbText::append(char const *, unsigned long, unsigned long)oofile.a(oof4.o)
dbBLOB::adoptBody(char *, unsigned long) constoofile.a(oofrec1.o)
dbBLOB::append(char const *, unsigned long, unsigned long)oofile.a(oof4.o)
ld: fatal: Symbol referencing errors. No output written to OOF_Test30
collect2: ld returned 1 exit status
*** Error code 1
make: Fatal error: Command failed for target `OOF_Test30'


with char* pointers we get problems actually calling them with an array
of longs, but OOFILE compiles OK.

g++ -c samples/ooftst30.cpp  -I./source/core/ -I./source/utils/ -I./source/ctree/ -I./source/dbase/ -I./samples -ggdb -DOOF_GCC -DSOLARIS
samples/ooftst30.cpp: In function `int main()':
samples/ooftst30.cpp:272: no matching function for call to `oofBLOB::append (long int[7], long int &)'
source/core/oof3.h:402: candidates are: void dbBLOB::append(const dbBLOB &)
source/core/oof3.h:403:                 void dbBLOB::append(const char *, long unsigned int, long unsigned int = 0ffffffff)
*** Error code 1
make: Fatal error: Command failed for target `ooftst30.o'

Stroustrup section C.6.2.3 "Pointer and Reference Conversions" says that any pointer type can be 
implicitly converted to a void*(see Stroustrup 5.6).

Given that the incoming params here are const pointers I think it is a reasonably safe trade-off to
have these two API's take const void* and perform internal conversion to const char*.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2001/09/07
FIXING TRANSACTIONS
Originally OOF_FServBackend had the method commitRecord and this was copied across 
to the OOF_ctreeBackend. On review this name is incorrect it should be commitTransaction
as it actually commits a transaction which can involve many records. So
change the name to commitTransaction.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2001/09/08
TRANSACTION MODES HANDLING

The ctree call begin can have several modes passed in as parameters. The OOF_ctreeBackend and 
OOF_FServBackend backends, had the mode hard coded to TRNLOG|ENABLE  in thier beginTransaction, method.
This is not flexible enough so add a static sTransMode to the backend with the default stateTRNLOG|ENABLE.
this is required only on server builds and single user TRANPROC builds


The ctree calls commit and abort can also have several modes passed in as parameters. 
The OOF_FServBackend backend, had the mode hard coded to ctKEEP_OUT.  
As with the tFileMode this is not flexible enough so, add a static sTransReleaseMode to the backend with the default 
state ctKEEP_OUT.

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2001 /09/19
MAKE TRANSACTION HANDLING MORE FLEXIBLE

Decided that the use of the statics to handle transaction modes, although useful for simple applications. Is some
what inflexible as if a user wants to use a richer range of modes (ie > 1 mode per app). 
the static only model leads to clumsy code like

Save static 
Set static
Call transaction function
Restore static

so change api of transaction methods to be of form
	beginTran(short tMode =0){
		if(tMode)
			use tmode
		else
			use static Varable
		}
	this will not break existing code, but allows the user to change modes as desired.
	
	
	
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2001/10/15
PROPAGATING DELETES AND DIFFERENT RELATIONSHIP CONNECTIVITY

OOFILE supports relationships with dbRelRef (0..1 related) and
dbRelSet(0..N related) on either side of a relationship, ie: 1:1 1:N,
N:1  or N:M.

The order of declaring a relationship generally indicates ownership -
when we have dbRelationship(Patients.Visits, Visits.Patient) then
Patients owns Visits records by default.

When we have an N:1 relationship, it is typically used for some lookup
reference and there is no ownership and hence no propagation of deletes.

An N:M relationship is implemented by a linking table in the middle
(currently, at least) and a pair of 1:N, N:1 relationships into and out
of that table. Both of these, considered as 1:N relationships, imply
ownership. In other words, deleting one end of the N:M relationship
will take out the intermediate records and unlink the other end.

Similarly, if we delete the single lookup record at the far side of an N:1
relationship, we should unlink the relationship.

Note: for referential integrity purposes, we may also bar this delete.

There are currently bugs in OOFILE with regard to pointer relationships
and deleting in N:1 situations.

FORCES:
F1) Current bugs have caused customer crashes with deleteAll. This implies accidentally
declaring an N:1 lookup relationship as 1:N is too easy a conceptual mistake.

F2) In most lookup scenarios, would want the default be just unlinking N:1 and
still want the related record left, eg: on a GUI popup.

F3) Special case exists of deleteAll that maybe justifies temporarily disabling
referential integrity checks. Totally clearing out a database could be regarded as
an atomic operation (largely just carried out in OOFILE test programs). 


DELETION ALTERNATIVES
1) Unlink records going from N:1 by default. Use the existing propagatesRelatedDeletes() 
method to indicate delete should occur instead of unlinking. When we delete the 1 record,
it fixes referential integrity by unlinking back to the remaining N records.

2) Do 1) but as if reference counted - when the reference count falls to zero
actually delete the 1 record.

3) Disallow propagation for N:1

DECISION 3
For now, just disallow it - there was a confusing bug in OOFILE due to not
understanding that a 1:N has a set pointing from the LHS to the ref on the RHS
so the default was setting propagation when it shouldn't have been.

Leave a TODO to handle this case properly later after user consultation.

DEFINITION ALTERNATIVES
a) Add a different state or two, eg: eDeleteOrUnlink, eDeleteLastRef, eUnlinkAlways

b) Add a new state flag to complement deletion.

DECISION - deferred for user consultation.


-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2001/10/17
ATTEMPT TO FIX STEVE'S BUG WITH CACHING UNTOUCHED NEW RECORDS AND COUNT

A user reported that count() fails to return a value including the current new record
but countAll() does include new records.

Agreed this sounds like a bug.

On initial examination, there are a few issues:
1) records are only cached if they are dirty, not if pristine new ones. This has 
   probably not been spotted before as the only users of related caches (in GUI)
   have databases which initialise some fields in an overriden newRecord() and
   hence always have dirty records.
   

-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2001/10/30
MULTIPLE USE OF LOOKUP TABLES

Say we have a simple lookup table, which is pointed to by a dbRelRef and
we want to have many separate tables use this lookup (eg: an Issue Type).

If we have two tables pointing at a third, this is not allowed and is caught 
as you try to create the database, by the assertion in:
dbRelRefBase::SetCommonRelationshipFields(dbRelRefBase& rhs)
{
	assert(!mInverseField); 

In application development, this will be quite a common scenario.

The reason it presents such a problem is because the evaluation of relationships
in OOFILE always assumes there is a unique party at each end. This allows
transition of the relationship in either direction and helps with referential integrity.

ALTERNATIVES
1) Allow unidirectional relationships, which matches object modelling practice
   and would solve this problem nicely, except for being vulnerable to deleting
   lookup records without knowing if they are in use.

2) Add extra relationship members in the target table for each user of the table.
   This is robust with the current OOFILE technology but somewhat of a nuisance for
   application authors.

3) Cloning the prototypical table sounds attractive but doesn't work - you can't
   clone tables before their schemae are fully established so you can't do it before
   setting up the relationships.
   
   
DECISION - use 2) to solve immediate application needs, scheduling 1) as a To Do



-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
2001/11/20
SAVING BLOBS AND BINARY ACCESS

In a typical scenario we may have a buffer which we want to save in a dbBLOB
without copying.

dbBLOB::adoptBody neatly takes care of that.

What if we want to write directly into the dbBLOB?

We can call allocRoomFor to specify a buffer of adequate size and gain
access to that buffer via bodyAddress.

However, OOFILE doesn't currently give us a way to  finish the "write" action
by updating the various fields, as if we had called setBytes.

We can explicitly call markDirty to ensure a write is attempted but unless
the blob length has been set, nothing will be written.

ALTERNATIVES
1) bodyAddress can assume that as we are giving write access to the body
   that the contents may be updated and therefore the database should
   be dirtied and length set.
   
2) we make the SetLength method public and have it call markDirty

3) we make markDirty update the length

4) we rely on manual calls to markDirty but update the length in allocRoomFor


The goal is to keep existing code working safely but allow additional access.
Therefore changing bodyAddress so it dirties the field is too dangerous and 
may affect users who are using it for read-only access.

The most directly intentional "setup" call if you are writing directly to
the BLOB is allocRoomFor.

DECISION - 4
